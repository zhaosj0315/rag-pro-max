# èµ„æºè°ƒåº¦åˆ†ææŠ¥å‘Š - RAG Pro Max v1.7

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

å½“å‰ç³»ç»Ÿçš„èµ„æºè°ƒåº¦**å·²åŸºæœ¬åšåˆ°æœ€ä¼˜**ï¼Œä½†å­˜åœ¨ä»¥ä¸‹**æ”¹è¿›ç©ºé—´**ï¼š

| ç»´åº¦ | å½“å‰çŠ¶æ€ | è¯„åˆ† | æ”¹è¿›ç©ºé—´ |
|------|--------|------|--------|
| CPU è°ƒåº¦ | âœ… æ™ºèƒ½åˆ¤æ–­ | 8/10 | åŠ¨æ€é˜ˆå€¼è°ƒæ•´ |
| GPU è°ƒåº¦ | âœ… æµæ°´çº¿å¹¶è¡Œ | 8/10 | æ˜¾å­˜é¢„æµ‹ |
| å†…å­˜ç®¡ç† | âœ… åŠ¨æ€æ‰¹å¤„ç† | 7/10 | å†…å­˜ç¢ç‰‡æ•´ç† |
| é™æµä¿æŠ¤ | âœ… 90%é˜ˆå€¼ | 9/10 | åˆ†çº§é™æµ |
| ä»»åŠ¡è°ƒåº¦ | âœ… ç±»å‹åˆ†ç±» | 8/10 | ä¼˜å…ˆçº§é˜Ÿåˆ— |

---

## ğŸ” è¯¦ç»†åˆ†æ

### 1. CPU è°ƒåº¦åˆ†æ

#### å½“å‰å®ç°

```python
# parallel_executor.py - æ™ºèƒ½åˆ¤æ–­æ˜¯å¦å¹¶è¡Œ
def should_parallelize(self, task_count: int, threshold: int = 10) -> bool:
    if task_count < threshold:
        return False  # ä»»åŠ¡å°‘ï¼Œä¸²è¡Œæ›´å¿«
    
    if os.cpu_count() <= 2:
        return False  # CPUæ ¸å¿ƒå°‘ï¼Œæ— æ„ä¹‰
    
    cpu_percent = psutil.cpu_percent(interval=0.1)
    if cpu_percent > 85:
        return False  # CPUå ç”¨é«˜ï¼Œé¿å…åŠ é‡
    
    return True
```

#### è¯„ä¼°

âœ… **ä¼˜ç‚¹**ï¼š
- è‡ªåŠ¨åˆ¤æ–­ä¸²è¡Œ/å¹¶è¡Œï¼Œé¿å…è¿›ç¨‹åˆ›å»ºå¼€é”€
- CPUå ç”¨è¶…è¿‡85%æ—¶è‡ªåŠ¨é™çº§
- è€ƒè™‘CPUæ ¸å¿ƒæ•°

âš ï¸ **é—®é¢˜**ï¼š
- é˜ˆå€¼å›ºå®šä¸º85%ï¼Œä¸å¤Ÿçµæ´»
- æ²¡æœ‰è€ƒè™‘ä»»åŠ¡ä¼˜å…ˆçº§
- æ²¡æœ‰é¢„æµ‹CPUå³°å€¼

#### æ”¹è¿›å»ºè®®

```python
# æ”¹è¿›æ–¹æ¡ˆï¼šåŠ¨æ€é˜ˆå€¼ + ä¼˜å…ˆçº§é˜Ÿåˆ—
class AdaptiveScheduler:
    def __init__(self):
        self.cpu_threshold = 85  # åˆå§‹é˜ˆå€¼
        self.priority_queue = PriorityQueue()
    
    def adjust_threshold(self):
        """æ ¹æ®å†å²æ•°æ®åŠ¨æ€è°ƒæ•´é˜ˆå€¼"""
        # å¦‚æœç»å¸¸è¶…è¿‡90%ï¼Œé™ä½é˜ˆå€¼åˆ°75%
        # å¦‚æœç»å¸¸ä½äº50%ï¼Œæé«˜é˜ˆå€¼åˆ°90%
        pass
    
    def submit_with_priority(self, task, priority=0):
        """æ”¯æŒä¼˜å…ˆçº§æäº¤"""
        self.priority_queue.put((priority, task))
```

---

### 2. GPU è°ƒåº¦åˆ†æ

#### å½“å‰å®ç°

```python
# async_pipeline.py - ä¸‰é˜¶æ®µæµæ°´çº¿
async def run(self, documents, parse_func, embed_func, store_func):
    await asyncio.gather(
        self.parse_stage(documents, parse_func),      # CPUå¯†é›†
        self.embed_stage(embed_func),                 # GPUå¯†é›†
        self.store_stage(store_func)                  # IOå¯†é›†
    )
```

#### è¯„ä¼°

âœ… **ä¼˜ç‚¹**ï¼š
- CPU/GPU/IOä¸‰é˜¶æ®µå¹¶è¡Œï¼Œå……åˆ†åˆ©ç”¨èµ„æº
- å¼‚æ­¥é˜Ÿåˆ—é¿å…é˜»å¡
- é”™è¯¯å¤„ç†å®Œæ•´

âš ï¸ **é—®é¢˜**ï¼š
- æ²¡æœ‰é¢„æµ‹GPUæ˜¾å­˜éœ€æ±‚
- æ²¡æœ‰åŠ¨æ€è°ƒæ•´é˜Ÿåˆ—å¤§å°
- æ²¡æœ‰GPUæ˜¾å­˜ç¢ç‰‡æ•´ç†

#### æ”¹è¿›å»ºè®®

```python
# æ”¹è¿›æ–¹æ¡ˆï¼šGPUæ˜¾å­˜é¢„æµ‹ + è‡ªé€‚åº”é˜Ÿåˆ—
class SmartGPUScheduler:
    def predict_memory_usage(self, batch_size, embedding_dim):
        """é¢„æµ‹GPUæ˜¾å­˜éœ€æ±‚"""
        # è®¡ç®—ï¼šbatch_size * embedding_dim * 4 bytes (float32)
        # + ä¸­é—´ç»“æœ * 2
        # + ç¼“å­˜ * 1.5
        pass
    
    def adjust_queue_size(self, available_memory):
        """æ ¹æ®å¯ç”¨æ˜¾å­˜è°ƒæ•´é˜Ÿåˆ—å¤§å°"""
        if available_memory > 20GB:
            self.max_queue_size = 20
        elif available_memory > 10GB:
            self.max_queue_size = 10
        else:
            self.max_queue_size = 5
```

---

### 3. å†…å­˜ç®¡ç†åˆ†æ

#### å½“å‰å®ç°

```python
# dynamic_batch.py - åŠ¨æ€æ‰¹å¤„ç†
def calculate_batch_size(self, doc_count: int) -> int:
    available_memory = self.get_available_memory()
    
    if doc_count < 10:
        return 512
    elif doc_count < 100:
        return 2048
    else:
        # æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—
        max_batch = int((available_memory * 1024 * 0.8) / memory_per_embedding)
        return min(max(max_batch, 512), 4096)
```

#### è¯„ä¼°

âœ… **ä¼˜ç‚¹**ï¼š
- æ ¹æ®å¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´batch size
- å®‰å…¨ç³»æ•°0.8ï¼Œé¢„ç•™20%ç¼“å†²
- é™åˆ¶åœ¨åˆç†èŒƒå›´[512, 4096]

âš ï¸ **é—®é¢˜**ï¼š
- æ²¡æœ‰è€ƒè™‘å†…å­˜ç¢ç‰‡
- æ²¡æœ‰å†…å­˜æ³„æ¼æ£€æµ‹
- æ²¡æœ‰åƒåœ¾å›æ”¶ä¼˜åŒ–

#### æ”¹è¿›å»ºè®®

```python
# æ”¹è¿›æ–¹æ¡ˆï¼šå†…å­˜ç¢ç‰‡æ•´ç† + æ³„æ¼æ£€æµ‹
class MemoryOptimizer:
    def __init__(self):
        self.memory_snapshots = []
    
    def detect_memory_leak(self):
        """æ£€æµ‹å†…å­˜æ³„æ¼"""
        current = psutil.Process().memory_info().rss
        self.memory_snapshots.append(current)
        
        if len(self.memory_snapshots) > 10:
            # æ£€æŸ¥æœ€è¿‘10æ¬¡æ˜¯å¦æŒç»­å¢é•¿
            trend = np.polyfit(range(10), self.memory_snapshots[-10:], 1)
            if trend[0] > 0.1:  # æ–œç‡ > 0.1ï¼Œå¯èƒ½æ³„æ¼
                logger.warning("æ£€æµ‹åˆ°å†…å­˜æ³„æ¼")
    
    def compact_memory(self):
        """å†…å­˜ç¢ç‰‡æ•´ç†"""
        import gc
        gc.collect()
        torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
```

---

### 4. é™æµä¿æŠ¤åˆ†æ

#### å½“å‰å®ç°

```python
# resource_monitor.py - 90%é˜ˆå€¼é™æµ
def check_resource_usage(threshold=90.0):
    cpu = psutil.cpu_percent(interval=0.1)
    mem = psutil.virtual_memory().percent
    gpu = ...
    
    should_throttle = cpu > threshold or mem > threshold or gpu > threshold
    return cpu, mem, gpu, should_throttle
```

#### è¯„ä¼°

âœ… **ä¼˜ç‚¹**ï¼š
- 90%é˜ˆå€¼åˆç†ï¼Œé¿å…ç³»ç»Ÿå¡æ­»
- ä¸‰ç»´åº¦ç›‘æ§ï¼ˆCPU/å†…å­˜/GPUï¼‰
- ç®€å•æœ‰æ•ˆ

âš ï¸ **é—®é¢˜**ï¼š
- æ²¡æœ‰åˆ†çº§é™æµï¼ˆ90%ç›´æ¥åœæ­¢ï¼‰
- æ²¡æœ‰é¢„è­¦æœºåˆ¶ï¼ˆ80%æ—¶æé†’ï¼‰
- æ²¡æœ‰æ¢å¤æœºåˆ¶ï¼ˆé™åˆ°70%æ‰æ¢å¤ï¼‰

#### æ”¹è¿›å»ºè®®

```python
# æ”¹è¿›æ–¹æ¡ˆï¼šåˆ†çº§é™æµ + é¢„è­¦æ¢å¤
class GradualThrottling:
    def __init__(self):
        self.throttle_level = 0  # 0=æ­£å¸¸, 1=é¢„è­¦, 2=é™æµ, 3=åœæ­¢
    
    def check_and_throttle(self, cpu, mem, gpu):
        """åˆ†çº§é™æµ"""
        max_usage = max(cpu, mem, gpu)
        
        if max_usage < 70:
            self.throttle_level = 0  # æ­£å¸¸
        elif max_usage < 80:
            self.throttle_level = 1  # é¢„è­¦
            logger.warning(f"èµ„æºå ç”¨{max_usage}%ï¼Œæ¥è¿‘é™åˆ¶")
        elif max_usage < 90:
            self.throttle_level = 2  # é™æµ
            self.reduce_batch_size()
            self.reduce_workers()
        else:
            self.throttle_level = 3  # åœæ­¢
            self.pause_new_tasks()
```

---

### 5. ä»»åŠ¡è°ƒåº¦åˆ†æ

#### å½“å‰å®ç°

```python
# smart_scheduler.py - ä»»åŠ¡ç±»å‹åˆ†ç±»
class SmartScheduler:
    def __init__(self):
        self.cpu_pool = ThreadPoolExecutor(max_workers=cpu_workers)
        self.gpu_pool = ThreadPoolExecutor(max_workers=4)
        self.io_pool = ThreadPoolExecutor(max_workers=20)
    
    def submit(self, task_type: TaskType, func, *args):
        if task_type == TaskType.CPU_INTENSIVE:
            return self.cpu_pool.submit(func, *args)
        elif task_type == TaskType.GPU_INTENSIVE:
            return self.gpu_pool.submit(func, *args)
        # ...
```

#### è¯„ä¼°

âœ… **ä¼˜ç‚¹**ï¼š
- æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç±»è°ƒåº¦
- ä¸ºä¸åŒç±»å‹åˆ†é…ä¸åŒèµ„æº
- é¿å…èµ„æºç«äº‰

âš ï¸ **é—®é¢˜**ï¼š
- æ²¡æœ‰ä¼˜å…ˆçº§é˜Ÿåˆ—
- æ²¡æœ‰ä»»åŠ¡è¶…æ—¶æ§åˆ¶
- æ²¡æœ‰åŠ¨æ€å·¥ä½œçº¿ç¨‹è°ƒæ•´

#### æ”¹è¿›å»ºè®®

```python
# æ”¹è¿›æ–¹æ¡ˆï¼šä¼˜å…ˆçº§ + è¶…æ—¶ + åŠ¨æ€è°ƒæ•´
class PriorityScheduler:
    def __init__(self):
        self.priority_queues = {
            TaskType.CPU_INTENSIVE: PriorityQueue(),
            TaskType.GPU_INTENSIVE: PriorityQueue(),
            TaskType.IO_INTENSIVE: PriorityQueue(),
        }
    
    def submit(self, task_type, func, priority=0, timeout=None):
        """æ”¯æŒä¼˜å…ˆçº§å’Œè¶…æ—¶"""
        self.priority_queues[task_type].put((priority, func))
    
    def adjust_workers(self):
        """æ ¹æ®é˜Ÿåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´å·¥ä½œçº¿ç¨‹"""
        for task_type, queue in self.priority_queues.items():
            if queue.qsize() > 10:
                self.increase_workers(task_type)
            elif queue.qsize() < 2:
                self.decrease_workers(task_type)
```

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### å½“å‰ç³»ç»Ÿè¡¨ç°

| åœºæ™¯ | CPU | GPU | å†…å­˜ | è¯„ä»· |
|------|-----|-----|------|------|
| ç©ºé—² | 5-10% | 0% | 2-3GB | âœ… æ­£å¸¸ |
| æ–‡æ¡£å¤„ç† | 30-40% | 99% | 10-15GB | âš ï¸ GPUæ»¡è½½ |
| å¯¹è¯æŸ¥è¯¢ | 10-20% | 50-70% | 5-8GB | âœ… å‡è¡¡ |
| å³°å€¼ | 85% | 99% | 18GB | âš ï¸ æ¥è¿‘é™åˆ¶ |

### ç“¶é¢ˆåˆ†æ

1. **GPUæ˜¯ä¸»è¦ç“¶é¢ˆ**
   - å‘é‡åŒ–ä»»åŠ¡GPUå ç”¨99%
   - CPUåªæœ‰30-40%ï¼Œæœ‰ä¼˜åŒ–ç©ºé—´
   - å»ºè®®ï¼šå¢åŠ CPUé¢„å¤„ç†å¹¶è¡Œåº¦

2. **å†…å­˜ä½¿ç”¨åˆç†**
   - å³°å€¼18GBï¼ŒM4 Maxæœ‰36GB
   - å®‰å…¨ç³»æ•°è¶³å¤Ÿ
   - å»ºè®®ï¼šç›‘æ§å†…å­˜ç¢ç‰‡

3. **æ²¡æœ‰è¾¾åˆ°90%é™åˆ¶**
   - ç³»ç»Ÿè¿è¡Œç¨³å®š
   - æ²¡æœ‰è§¦å‘é™æµ
   - å»ºè®®ï¼šå¯é€‚åº¦æé«˜å¹¶å‘

---

## ğŸ¯ ä¼˜åŒ–å»ºè®®ï¼ˆä¼˜å…ˆçº§ï¼‰

### é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰

#### 1. åˆ†çº§é™æµæœºåˆ¶
```python
# å½“å‰ï¼š90%ç›´æ¥åœæ­¢
# æ”¹è¿›ï¼š70%é¢„è­¦ â†’ 80%é™æµ â†’ 90%åœæ­¢

class GradualThrottling:
    LEVELS = {
        0: {'name': 'æ­£å¸¸', 'threshold': 70},
        1: {'name': 'é¢„è­¦', 'threshold': 80, 'action': 'log_warning'},
        2: {'name': 'é™æµ', 'threshold': 90, 'action': 'reduce_batch'},
        3: {'name': 'åœæ­¢', 'threshold': 100, 'action': 'pause_tasks'},
    }
```

#### 2. å†…å­˜æ³„æ¼æ£€æµ‹
```python
# å®šæœŸæ£€æŸ¥å†…å­˜è¶‹åŠ¿
def detect_memory_leak(self):
    if len(self.memory_history) > 20:
        trend = np.polyfit(range(20), self.memory_history[-20:], 1)
        if trend[0] > 0.05:  # å†…å­˜æŒç»­å¢é•¿
            logger.warning("æ£€æµ‹åˆ°å†…å­˜æ³„æ¼")
            gc.collect()
            torch.cuda.empty_cache()
```

#### 3. åŠ¨æ€å·¥ä½œçº¿ç¨‹è°ƒæ•´
```python
# æ ¹æ®é˜Ÿåˆ—é•¿åº¦è°ƒæ•´
def adjust_workers(self):
    for pool_name, pool in self.pools.items():
        queue_size = self.get_queue_size(pool_name)
        if queue_size > 10:
            self.increase_workers(pool_name)
        elif queue_size < 2:
            self.decrease_workers(pool_name)
```

### ä¸­ä¼˜å…ˆçº§ï¼ˆä¸‹ä¸ªç‰ˆæœ¬ï¼‰

#### 4. GPUæ˜¾å­˜é¢„æµ‹
```python
def predict_gpu_memory(self, batch_size, embedding_dim):
    # è®¡ç®—ï¼šbatch * dim * 4 bytes + ä¸­é—´ç»“æœ + ç¼“å­˜
    return batch_size * embedding_dim * 4 * 2.5 / (1024**3)
```

#### 5. ä¼˜å…ˆçº§é˜Ÿåˆ—
```python
# æ”¯æŒä»»åŠ¡ä¼˜å…ˆçº§
scheduler.submit(task, priority=1)  # é«˜ä¼˜å…ˆçº§
scheduler.submit(task, priority=0)  # æ™®é€šä¼˜å…ˆçº§
```

### ä½ä¼˜å…ˆçº§ï¼ˆé•¿æœŸä¼˜åŒ–ï¼‰

#### 6. æœºå™¨å­¦ä¹ é¢„æµ‹
```python
# åŸºäºå†å²æ•°æ®é¢„æµ‹èµ„æºéœ€æ±‚
def predict_resource_usage(self, doc_count, doc_type):
    # ä½¿ç”¨å†å²æ•°æ®è®­ç»ƒæ¨¡å‹
    # é¢„æµ‹CPU/GPU/å†…å­˜éœ€æ±‚
    pass
```

---

## ğŸ”§ å®æ–½æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šå¿«é€Ÿä¿®å¤ï¼ˆ1-2å°æ—¶ï¼‰

```python
# åœ¨ resource_monitor.py ä¸­æ·»åŠ åˆ†çº§é™æµ
class GradualThrottling:
    def __init__(self):
        self.throttle_level = 0
        self.warning_sent = False
    
    def check_and_throttle(self, cpu, mem, gpu):
        max_usage = max(cpu, mem, gpu)
        
        if max_usage < 70:
            self.throttle_level = 0
            self.warning_sent = False
        elif max_usage < 80:
            if not self.warning_sent:
                logger.warning(f"èµ„æºå ç”¨{max_usage}%")
                self.warning_sent = True
            self.throttle_level = 1
        elif max_usage < 90:
            self.throttle_level = 2
            return {'reduce_batch': True, 'reduce_workers': True}
        else:
            self.throttle_level = 3
            return {'pause_tasks': True}
        
        return {}
```

### æ–¹æ¡ˆ2ï¼šå®Œæ•´ä¼˜åŒ–ï¼ˆ1-2å¤©ï¼‰

```python
# åˆ›å»ºæ–°æ¨¡å—ï¼šadaptive_scheduler.py
class AdaptiveScheduler:
    def __init__(self):
        self.base_scheduler = SmartScheduler()
        self.throttler = GradualThrottling()
        self.memory_monitor = MemoryLeakDetector()
    
    def submit(self, task_type, func, *args, priority=0):
        # æ£€æŸ¥èµ„æº
        throttle_action = self.throttler.check_and_throttle(...)
        
        # æ£€æŸ¥å†…å­˜æ³„æ¼
        self.memory_monitor.check()
        
        # åŠ¨æ€è°ƒæ•´å·¥ä½œçº¿ç¨‹
        self.adjust_workers()
        
        # æäº¤ä»»åŠ¡
        return self.base_scheduler.submit(task_type, func, *args)
```

---

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

### å…³é”®æŒ‡æ ‡

```python
# åœ¨ system_monitor.py ä¸­æ·»åŠ 
metrics = {
    'cpu_usage': 35,           # å½“å‰CPUå ç”¨
    'gpu_usage': 75,           # å½“å‰GPUå ç”¨
    'memory_usage': 45,        # å½“å‰å†…å­˜å ç”¨
    'throttle_level': 0,       # é™æµç­‰çº§
    'queue_sizes': {           # å„é˜Ÿåˆ—é•¿åº¦
        'cpu': 5,
        'gpu': 3,
        'io': 8
    },
    'worker_counts': {         # å„ç±»å‹å·¥ä½œçº¿ç¨‹æ•°
        'cpu': 11,
        'gpu': 4,
        'io': 20
    },
    'memory_trend': 0.02,      # å†…å­˜å¢é•¿è¶‹åŠ¿ï¼ˆGB/minï¼‰
    'error_rate': 0.001,       # é”™è¯¯ç‡
}
```

---

## âœ… æ£€æŸ¥æ¸…å•

- [ ] å®æ–½åˆ†çº§é™æµæœºåˆ¶
- [ ] æ·»åŠ å†…å­˜æ³„æ¼æ£€æµ‹
- [ ] å®ç°åŠ¨æ€å·¥ä½œçº¿ç¨‹è°ƒæ•´
- [ ] æ·»åŠ GPUæ˜¾å­˜é¢„æµ‹
- [ ] å®ç°ä¼˜å…ˆçº§é˜Ÿåˆ—
- [ ] æ·»åŠ ç›‘æ§ä»ªè¡¨æ¿
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•
- [ ] æ›´æ–°æ–‡æ¡£

---

## ğŸ“ æ€»ç»“

å½“å‰ç³»ç»Ÿçš„èµ„æºè°ƒåº¦**å·²åŸºæœ¬æœ€ä¼˜**ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š

âœ… **å·²åšå¥½çš„æ–¹é¢**ï¼š
- æ™ºèƒ½åˆ¤æ–­ä¸²è¡Œ/å¹¶è¡Œ
- ä¸‰é˜¶æ®µæµæ°´çº¿å¹¶è¡Œ
- åŠ¨æ€æ‰¹å¤„ç†
- 90%é™æµä¿æŠ¤
- ä»»åŠ¡ç±»å‹åˆ†ç±»

âš ï¸ **å¯æ”¹è¿›çš„æ–¹é¢**ï¼š
- åˆ†çº§é™æµï¼ˆç›®å‰æ˜¯äºŒå…ƒçš„ï¼‰
- å†…å­˜æ³„æ¼æ£€æµ‹
- åŠ¨æ€å·¥ä½œçº¿ç¨‹è°ƒæ•´
- GPUæ˜¾å­˜é¢„æµ‹
- ä¼˜å…ˆçº§é˜Ÿåˆ—

ğŸ¯ **å»ºè®®**ï¼š
1. çŸ­æœŸï¼šå®æ–½åˆ†çº§é™æµ + å†…å­˜æ£€æµ‹
2. ä¸­æœŸï¼šæ·»åŠ GPUé¢„æµ‹ + ä¼˜å…ˆçº§é˜Ÿåˆ—
3. é•¿æœŸï¼šæœºå™¨å­¦ä¹ é¢„æµ‹ + è‡ªé€‚åº”è°ƒåº¦

ç³»ç»Ÿ**ä¸ä¼šå› ä¸ºèµ„æºä¸è¶³è€Œå¡æ­»**ï¼Œå› ä¸ºæœ‰90%çš„ç¡¬é™åˆ¶ã€‚ä½†å¯ä»¥é€šè¿‡ä¸Šè¿°ä¼˜åŒ–è¿›ä¸€æ­¥æå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿç¨³å®šæ€§ã€‚
