# ğŸ”¬ v2.3.1 æŠ€æœ¯è°ƒç ”æŠ¥å‘Š

## ğŸ¯ è°ƒç ”ç›®æ ‡

ä¸ºv2.3.1æ™ºèƒ½åŒ–å¢å¼ºç‰ˆæä¾›æŠ€æœ¯å¯è¡Œæ€§åˆ†æå’Œå®ç°æ–¹æ¡ˆã€‚

## ğŸ§  å¤šæ¨¡å‹é›†æˆè°ƒç ”

### ä¸»æµLLMæä¾›å•†å¯¹æ¯”

| æä¾›å•† | æ¨¡å‹ | ä¸Šä¸‹æ–‡é•¿åº¦ | æˆæœ¬($/1M tokens) | ç‰¹ç‚¹ |
|--------|------|------------|-------------------|------|
| OpenAI | GPT-4 | 128K | $30/$60 | é€šç”¨èƒ½åŠ›å¼º |
| OpenAI | GPT-3.5-turbo | 16K | $1/$2 | æ€§ä»·æ¯”é«˜ |
| Anthropic | Claude-3-Sonnet | 200K | $15/$75 | ä»£ç èƒ½åŠ›å¼º |
| Anthropic | Claude-3-Haiku | 200K | $0.25/$1.25 | é€Ÿåº¦å¿« |
| Google | Gemini-Pro | 1M | $7/$21 | é•¿ä¸Šä¸‹æ–‡ |
| æ™ºè°±AI | GLM-4 | 128K | Â¥0.1/Â¥0.1 | ä¸­æ–‡ä¼˜åŒ– |

### æŠ€æœ¯å®ç°æ–¹æ¡ˆ

#### 1. ç»Ÿä¸€æ¥å£è®¾è®¡
```python
from abc import ABC, abstractmethod

class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, messages: List[Dict], **kwargs) -> str:
        pass
    
    @abstractmethod
    def get_token_count(self, text: str) -> int:
        pass

class OpenAIProvider(LLMProvider):
    def __init__(self, api_key: str, model: str):
        self.client = OpenAI(api_key=api_key)
        self.model = model
    
    async def generate(self, messages: List[Dict], **kwargs) -> str:
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            **kwargs
        )
        return response.choices[0].message.content
```

#### 2. æ™ºèƒ½è·¯ç”±ç®—æ³•
```python
class QueryClassifier:
    def __init__(self):
        # ä½¿ç”¨è½»é‡çº§BERTæ¨¡å‹è¿›è¡Œåˆ†ç±»
        self.model = AutoModel.from_pretrained('distilbert-base-uncased')
        self.categories = {
            'code': ['ç¼–ç¨‹', 'ä»£ç ', 'bug', 'å‡½æ•°', 'ç®—æ³•'],
            'creative': ['å†™ä½œ', 'åˆ›æ„', 'æ•…äº‹', 'è¯—æ­Œ', 'æ–‡æ¡ˆ'],
            'factual': ['ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'è§£é‡Š', 'äº‹å®', 'æ•°æ®'],
            'analysis': ['åˆ†æ', 'æ¯”è¾ƒ', 'æ€»ç»“', 'è¯„ä¼°', 'ç ”ç©¶']
        }
    
    def classify(self, query: str) -> str:
        # å…³é”®è¯åŒ¹é… + è¯­ä¹‰åˆ†ç±»
        for category, keywords in self.categories.items():
            if any(kw in query for kw in keywords):
                return category
        
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†ç±»
        return self.semantic_classify(query)
```

## ğŸ” æ··åˆæ£€ç´¢æŠ€æœ¯è°ƒç ”

### BM25ç®—æ³•é›†æˆ

#### ä¼˜åŠ¿
- å…³é”®è¯ç²¾ç¡®åŒ¹é…
- è®¡ç®—é€Ÿåº¦å¿«
- æ— éœ€è®­ç»ƒ

#### å®ç°æ–¹æ¡ˆ
```python
from rank_bm25 import BM25Okapi
import jieba

class BM25Retriever:
    def __init__(self, documents: List[str]):
        # ä¸­æ–‡åˆ†è¯
        tokenized_docs = [list(jieba.cut(doc)) for doc in documents]
        self.bm25 = BM25Okapi(tokenized_docs)
        self.documents = documents
    
    def search(self, query: str, top_k: int = 10):
        tokenized_query = list(jieba.cut(query))
        scores = self.bm25.get_scores(tokenized_query)
        
        # è·å–top_kç»“æœ
        top_indices = scores.argsort()[-top_k:][::-1]
        return [(self.documents[i], scores[i]) for i in top_indices]
```

### é‡æ’åºæ¨¡å‹è°ƒç ”

#### Cross-Encoder vs Bi-Encoder

| æ¨¡å‹ç±»å‹ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|----------|------|------|----------|
| Bi-Encoder | é€Ÿåº¦å¿«ï¼Œå¯é¢„è®¡ç®— | ç²¾åº¦ç›¸å¯¹è¾ƒä½ | åˆæ­¥æ£€ç´¢ |
| Cross-Encoder | ç²¾åº¦é«˜ï¼Œäº¤äº’å»ºæ¨¡ | é€Ÿåº¦æ…¢ï¼Œå®æ—¶è®¡ç®— | é‡æ’åº |

#### æ¨èæ¨¡å‹
- **ä¸­æ–‡**: `BAAI/bge-reranker-large`
- **è‹±æ–‡**: `cross-encoder/ms-marco-MiniLM-L-12-v2`
- **å¤šè¯­è¨€**: `sentence-transformers/all-MiniLM-L6-v2`

## ğŸ§® ä¸Šä¸‹æ–‡ç®¡ç†æŠ€æœ¯

### åŠ¨æ€ä¸Šä¸‹æ–‡ç­–ç•¥

#### 1. æ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†
```python
def calculate_relevance_score(query: str, document: str) -> float:
    # 1. è¯­ä¹‰ç›¸ä¼¼åº¦ (40%)
    semantic_score = cosine_similarity(
        embed_model.encode(query),
        embed_model.encode(document)
    )
    
    # 2. å…³é”®è¯åŒ¹é… (30%)
    keyword_score = bm25_score(query, document)
    
    # 3. æ–‡æ¡£è´¨é‡ (20%)
    quality_score = assess_document_quality(document)
    
    # 4. ç”¨æˆ·å†å²åå¥½ (10%)
    preference_score = get_user_preference_score(document)
    
    return 0.4 * semantic_score + 0.3 * keyword_score + \
           0.2 * quality_score + 0.1 * preference_score
```

#### 2. ä¸Šä¸‹æ–‡å‹ç¼©ç®—æ³•
```python
class ContextCompressor:
    def __init__(self, compression_model: str = "microsoft/DialoGPT-medium"):
        self.model = AutoModel.from_pretrained(compression_model)
    
    def compress_context(self, context: str, target_length: int) -> str:
        # 1. æå–å…³é”®å¥å­
        key_sentences = self.extract_key_sentences(context)
        
        # 2. æŠ½è±¡å¼æ‘˜è¦
        summary = self.generate_summary(key_sentences, target_length)
        
        return summary
```

## ğŸ¨ å¯è§†åŒ–æŠ€æœ¯è°ƒç ”

### å‰ç«¯æ¡†æ¶é€‰æ‹©

#### Streamlit vs React

| ç‰¹æ€§ | Streamlit | React |
|------|-----------|-------|
| å¼€å‘é€Ÿåº¦ | å¿« | ä¸­ç­‰ |
| è‡ªå®šä¹‰ç¨‹åº¦ | ä¸­ç­‰ | é«˜ |
| æ€§èƒ½ | ä¸­ç­‰ | é«˜ |
| å­¦ä¹ æˆæœ¬ | ä½ | é«˜ |

**ç»“è®º**: v2.3.1ç»§ç»­ä½¿ç”¨Streamlitï¼Œv2.3.1è€ƒè™‘Reacté‡æ„

### å¯è§†åŒ–åº“é€‰æ‹©

#### æ¨èæ–¹æ¡ˆ
- **å›¾è¡¨**: Plotly (äº¤äº’æ€§å¼º)
- **ç½‘ç»œå›¾**: NetworkX + Plotly
- **æµç¨‹å›¾**: Graphviz + Streamlit

```python
import plotly.graph_objects as go
import plotly.express as px

def visualize_retrieval_process(query: str, results: List[Dict]):
    # åˆ›å»ºæ£€ç´¢è¿‡ç¨‹å¯è§†åŒ–
    fig = go.Figure()
    
    # æ·»åŠ æŸ¥è¯¢èŠ‚ç‚¹
    fig.add_trace(go.Scatter(
        x=[0], y=[0],
        mode='markers+text',
        text=[query],
        name='Query'
    ))
    
    # æ·»åŠ æ–‡æ¡£èŠ‚ç‚¹
    for i, result in enumerate(results):
        fig.add_trace(go.Scatter(
            x=[i+1], y=[result['score']],
            mode='markers+text',
            text=[result['title'][:20]],
            name=f'Doc {i+1}'
        ))
    
    return fig
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

### ç¼“å­˜ç­–ç•¥

#### å¤šå±‚ç¼“å­˜æ¶æ„
```python
import redis
from functools import lru_cache

class CacheManager:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379)
        self.memory_cache = {}
    
    @lru_cache(maxsize=1000)
    def get_embedding(self, text: str):
        # L1: å†…å­˜ç¼“å­˜
        if text in self.memory_cache:
            return self.memory_cache[text]
        
        # L2: Redisç¼“å­˜
        cached = self.redis_client.get(f"embed:{hash(text)}")
        if cached:
            return pickle.loads(cached)
        
        # L3: è®¡ç®—å¹¶ç¼“å­˜
        embedding = self.compute_embedding(text)
        self.redis_client.setex(
            f"embed:{hash(text)}", 
            3600,  # 1å°æ—¶è¿‡æœŸ
            pickle.dumps(embedding)
        )
        self.memory_cache[text] = embedding
        return embedding
```

### å¼‚æ­¥å¤„ç†

#### ä»»åŠ¡é˜Ÿåˆ—è®¾è®¡
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncTaskManager:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.task_queue = asyncio.Queue()
    
    async def process_batch_queries(self, queries: List[str]):
        tasks = []
        for query in queries:
            task = asyncio.create_task(self.process_single_query(query))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
    
    async def process_single_query(self, query: str):
        # å¼‚æ­¥å¤„ç†å•ä¸ªæŸ¥è¯¢
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor, 
            self.sync_process_query, 
            query
        )
```

## ğŸ“Š æŠ€æœ¯é£é™©è¯„ä¼°

### é«˜é£é™©é¡¹
1. **å¤šæ¨¡å‹APIç¨³å®šæ€§** - ä¾èµ–ç¬¬ä¸‰æ–¹æœåŠ¡
2. **æˆæœ¬æ§åˆ¶** - å¤šæ¨¡å‹è°ƒç”¨æˆæœ¬è¾ƒé«˜
3. **å“åº”å»¶è¿Ÿ** - é‡æ’åºå¢åŠ å¤„ç†æ—¶é—´

### ä¸­é£é™©é¡¹
1. **æ¨¡å‹å…¼å®¹æ€§** - ä¸åŒæ¨¡å‹è¾“å‡ºæ ¼å¼å·®å¼‚
2. **ç¼“å­˜ä¸€è‡´æ€§** - åˆ†å¸ƒå¼ç¼“å­˜åŒæ­¥é—®é¢˜
3. **å¹¶å‘å¤„ç†** - é«˜å¹¶å‘ä¸‹çš„èµ„æºç«äº‰

### ä½é£é™©é¡¹
1. **BM25é›†æˆ** - æˆç†Ÿç®—æ³•ï¼Œé£é™©è¾ƒä½
2. **å¯è§†åŒ–å®ç°** - Plotlyç”Ÿæ€æˆç†Ÿ
3. **é…ç½®ç®¡ç†** - åŸºäºç°æœ‰æ¶æ„æ‰©å±•

## ğŸ› ï¸ æŠ€æœ¯é€‰å‹å»ºè®®

### æ ¸å¿ƒæŠ€æœ¯æ ˆ
- **å¤šæ¨¡å‹ç®¡ç†**: LiteLLM (ç»Ÿä¸€æ¥å£)
- **æ£€ç´¢å¼•æ“**: Elasticsearch + è‡ªç ”æ··åˆæ£€ç´¢
- **é‡æ’åº**: sentence-transformers
- **ç¼“å­˜**: Redis + å†…å­˜ç¼“å­˜
- **å¼‚æ­¥**: asyncio + aiohttp
- **å¯è§†åŒ–**: Plotly + Streamlit

### å¼€å‘å·¥å…·
- **ä»£ç è´¨é‡**: black, flake8, mypy
- **æµ‹è¯•æ¡†æ¶**: pytest, pytest-asyncio
- **æ€§èƒ½ç›‘æ§**: prometheus + grafana
- **æ—¥å¿—ç®¡ç†**: structlog + ELK

## ğŸ“ˆ å®æ–½å»ºè®®

### Phase 1 ä¼˜å…ˆçº§
1. **å¤šæ¨¡å‹ç®¡ç†å™¨** - æ ¸å¿ƒåŸºç¡€è®¾æ–½
2. **æ™ºèƒ½è·¯ç”±** - å·®å¼‚åŒ–ä»·å€¼
3. **æ··åˆæ£€ç´¢** - æ€§èƒ½æå‡å…³é”®

### æŠ€æœ¯å€ºåŠ¡
- é‡æ„ç°æœ‰æ£€ç´¢æ¨¡å—
- ç»Ÿä¸€é…ç½®ç®¡ç†
- å®Œå–„é”™è¯¯å¤„ç†

### ç›‘æ§æŒ‡æ ‡
- APIè°ƒç”¨æˆåŠŸç‡
- å¹³å‡å“åº”æ—¶é—´
- ç¼“å­˜å‘½ä¸­ç‡
- ç”¨æˆ·æ»¡æ„åº¦

---

**è°ƒç ”å®Œæˆæ—¶é—´**: 2025-12-15  
**è°ƒç ”è´Ÿè´£äºº**: æŠ€æœ¯å›¢é˜Ÿ  
**ä¸‹ä¸€æ­¥**: å¼€å§‹Phase 1å¼€å‘
