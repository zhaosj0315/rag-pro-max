#!/usr/bin/env python3
"""å¿«é€Ÿæµ‹è¯•æ¨èé—®é¢˜ç”Ÿæˆ"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.chat_utils_improved import generate_follow_up_questions_safe
from src.logging.log_manager import LogManager

# åˆ›å»ºæ¨¡æ‹ŸLLM
class MockLLM:
    def complete(self, prompt):
        class MockResponse:
            text = "æ¨Šç™»è¯»ä¹¦ä¼šçš„ç•Œé¢æœ‰ä»€ä¹ˆç‰¹è‰²åŠŸèƒ½ï¼Ÿ\nç”¨æˆ·ä½“éªŒè®¾è®¡çš„æ ¸å¿ƒåŸåˆ™æ˜¯ä»€ä¹ˆï¼Ÿ\nå¦‚ä½•è¯„ä¼°ç•Œé¢è®¾è®¡çš„æ•ˆæœï¼Ÿ"
        return MockResponse()

def test_with_llm():
    logger = LogManager()
    mock_llm = MockLLM()
    
    context = "æ¨Šç™»è¯»ä¹¦ä¼šé€šè¿‡å‹å¥½çš„ç”¨æˆ·ç•Œé¢è®¾è®¡ï¼Œç¡®ä¿ç”¨æˆ·ä¸€æ‰“å¼€ç•Œé¢å°±èƒ½æ¯«æ— éšœç¢åœ°ä½¿ç”¨ã€‚"
    
    print("ğŸ§ª æµ‹è¯•æ¨èé—®é¢˜ç”Ÿæˆ...")
    print(f"ğŸ“ ä¸Šä¸‹æ–‡: {context}")
    
    suggestions = generate_follow_up_questions_safe(
        context_text=context,
        num_questions=3,
        existing_questions=[],
        logger=logger,
        llm_model=mock_llm
    )
    
    print(f"âœ¨ ç”Ÿæˆç»“æœ: {suggestions}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯fallback
    fallback_questions = [
        "è¿™æœ¬ä¹¦çš„æ ¸å¿ƒè§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ä½œè€…çš„å†™ä½œèƒŒæ™¯å¦‚ä½•ï¼Ÿ", 
        "æœ‰å“ªäº›å®ç”¨çš„é˜…è¯»æŠ€å·§ï¼Ÿ"
    ]
    
    is_fallback = any(q in fallback_questions for q in suggestions)
    
    if is_fallback:
        print("âŒ ä»åœ¨ä½¿ç”¨fallbacké—®é¢˜")
        return False
    else:
        print("âœ… ä½¿ç”¨äº†çœŸæ­£çš„LLMç”Ÿæˆ")
        return True

if __name__ == "__main__":
    success = test_with_llm()
    print(f"\nç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
