#!/usr/bin/env python3
"""
æµ‹è¯•LLMè®¾ç½®ä¿®å¤
éªŒè¯æ¨èé—®é¢˜ç”Ÿæˆæ˜¯å¦ä½¿ç”¨çœŸæ­£çš„LLMè€Œä¸æ˜¯fallback
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from llama_index.core import Settings
from src.chat_utils_improved import generate_follow_up_questions_safe

def test_llm_availability():
    """æµ‹è¯•LLMå¯ç”¨æ€§"""
    print("ğŸ§ª æµ‹è¯•LLMå¯ç”¨æ€§...")
    
    print(f"Settings.llm: {getattr(Settings, 'llm', None)}")
    
    # æ¨¡æ‹Ÿæ¨èé—®é¢˜ç”Ÿæˆ
    context = "æ¨Šç™»è¯»ä¹¦ä¼šé€šè¿‡å‹å¥½çš„ç”¨æˆ·ç•Œé¢è®¾è®¡ï¼Œç¡®ä¿ç”¨æˆ·ä¸€æ‰“å¼€ç•Œé¢å°±èƒ½æ¯«æ— éšœç¢åœ°ä½¿ç”¨ã€‚"
    
    print(f"ğŸ“ ä¸Šä¸‹æ–‡: {context}")
    
    # ç”Ÿæˆæ¨èé—®é¢˜
    suggestions = generate_follow_up_questions_safe(
        context_text=context,
        num_questions=3,
        existing_questions=[],
        timeout=15
    )
    
    print(f"âœ¨ ç”Ÿæˆçš„æ¨èé—®é¢˜: {suggestions}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯fallbacké—®é¢˜
    fallback_questions = [
        "è¿™æœ¬ä¹¦çš„æ ¸å¿ƒè§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ä½œè€…çš„å†™ä½œèƒŒæ™¯å¦‚ä½•ï¼Ÿ", 
        "æœ‰å“ªäº›å®ç”¨çš„é˜…è¯»æŠ€å·§ï¼Ÿ"
    ]
    
    is_fallback = any(q in fallback_questions for q in suggestions)
    
    if is_fallback:
        print("âŒ ä½¿ç”¨äº†fallbacké—®é¢˜ï¼ŒLLMæœªæ­£ç¡®è®¾ç½®")
        return False
    else:
        print("âœ… ç”Ÿæˆäº†çœŸæ­£çš„æ¨èé—®é¢˜ï¼ŒLLMå·¥ä½œæ­£å¸¸")
        return True

if __name__ == "__main__":
    print("=" * 60)
    print("  LLMè®¾ç½®ä¿®å¤æµ‹è¯•")
    print("=" * 60)
    
    success = test_llm_availability()
    
    print("\n" + "=" * 60)
    if success:
        print("âœ… LLMè®¾ç½®æ­£å¸¸ï¼Œæ¨èé—®é¢˜ç”Ÿæˆä½¿ç”¨çœŸæ­£çš„LLM")
    else:
        print("âŒ LLMè®¾ç½®æœ‰é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        print("ğŸ’¡ å»ºè®®ï¼šç¡®ä¿åœ¨æ¨èé—®é¢˜ç”Ÿæˆå‰æ­£ç¡®è®¾ç½®LLMæ¨¡å‹")
    print("=" * 60)
