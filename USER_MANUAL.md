# RAG Pro Max 操作手册

**版本: v2.3.1  
**更新日期**: 2025-12-10

---

## 📑 目录

1. [快速开始](#1-快速开始)
2. [v2.0 新功能](#2-v20-新功能)
3. [基础配置](#3-基础配置)
4. [知识库管理](#4-知识库管理)
5. [文档上传](#5-文档上传)
6. [智能问答](#6-智能问答)
7. [高级功能](#7-高级功能)
8. [性能监控](#8-性能监控)
9. [推荐问题管理](#9-推荐问题管理)
10. [常见问题](#10-常见问题)
11. [故障排除](#11-故障排除)

---

## 2. v2.0 新功能

### 2.1 增量更新

**功能**: 智能检测文件变化，只处理修改的文件，效率提升70-90%

**使用方法**:
1. 在侧边栏找到"📈 增量更新"部分
2. 上传文件，点击"🔍 检查文件变化"
3. 查看变化报告（新增、修改、未变化）
4. 点击"🚀 执行增量更新"

**适用场景**: 大量文档需要频繁更新的场景

### 2.2 多模态支持

**功能**: 支持图片OCR识别和表格数据提取

**支持格式**:
- **图片**: JPG, PNG, PDF图片 (需要Tesseract)
- **表格**: PDF表格, Excel, CSV (需要Java)

**使用方法**:
1. 在侧边栏找到"🎨 多模态支持"部分
2. 上传多模态文件
3. 查看提取结果（文字、表格）
4. 执行多模态查询

### 2.3 API接口扩展

**功能**: 完整的RESTful API，支持程序化调用

**新增接口**:
- `POST /incremental-update` - 增量更新
- `POST /upload-multimodal` - 多模态文件上传
- `POST /query-multimodal` - 多模态查询
- `GET /kb/{kb_name}/incremental-stats` - 增量统计

**访问地址**: http://localhost:8000/docs

### 2.4 智能启动

**功能**: 自动检测v2.0功能可用性，向后兼容v1.8

**使用方法**: 使用原有启动命令，系统自动适配版本
```bash
./start.sh  # 自动检测并启用可用功能
```

---

## 1. 快速开始

### 1.1 安装依赖

```bash
# 克隆项目
git clone https://github.com/zhaosj0315/rag-pro-max.git
cd rag-pro-max

# 安装依赖
pip install -r requirements.txt
```

### 1.2 启动应用

```bash
# 推荐方式（自动测试）
./start.sh

# 或直接启动
streamlit run src/apppro.py
```

应用将在浏览器自动打开 http://localhost:8501

### 1.3 一键配置（新手推荐）

1. 点击侧边栏顶部的 **"⚡ 一键配置（推荐新手）"**
2. 系统自动配置默认设置
3. 创建知识库 → 上传文档 → 开始对话

**前提条件**: 需要先安装 Ollama
```bash
# macOS
brew install ollama
ollama serve
ollama pull qwen2.5:7b
```

---

## 2. 基础配置

### 2.1 LLM 配置

#### 选项 A: Ollama（本地，推荐）

1. 在侧边栏找到 **"⚙️ 基础配置"**
2. 选择 **"Ollama (本地)"**
3. 填写配置：
   - API Base URL: `http://localhost:11434`
   - 模型名称: `qwen2.5:7b`（或其他已下载的模型）

#### 选项 B: OpenAI（云端）

1. 选择 **"OpenAI / 兼容接口"**
2. 填写配置：
   - API Base URL: `https://api.openai.com/v1`
   - API Key: `sk-your-api-key-here`
   - 模型名称: `gpt-3.5-turbo` 或 `gpt-4`

### 2.2 嵌入模型配置

#### 选项 A: HuggingFace（本地，推荐）

- 提供商: **HuggingFace (本地/极速)**
- 模型: `BAAI/bge-small-zh-v1.5`（默认）
- 首次使用会自动下载模型

#### 选项 B: OpenAI

- 提供商: **OpenAI-Compatible**
- API Base: `https://api.openai.com/v1`
- API Key: 你的密钥
- 模型: `text-embedding-ada-002`

### 2.3 保存配置

配置完成后，设置会自动保存到 `app_config.json`

---

## 3. 知识库管理

### 3.1 创建知识库

1. 在侧边栏顶部找到 **"📚 知识库"** 下拉框
2. 选择 **"➕ 新建知识库..."**
3. 输入知识库名称（例如：`技术文档`）
4. 点击 **"创建新知识库"**

### 3.2 选择知识库

在下拉框中选择已创建的知识库即可切换

### 3.3 删除知识库

1. 选择要删除的知识库
2. 在侧边栏找到 **"🗑️ 删除知识库"** 按钮
3. 确认删除

### 3.4 重命名知识库

1. 选择要重命名的知识库
2. 在侧边栏找到 **"✏️ 重命名"** 按钮
3. 输入新名称

---

## 4. 文档上传

### 4.1 支持的文件格式

- 📄 文档: PDF, DOCX, TXT, MD
- 📊 表格: XLSX, CSV
- 📑 演示: PPTX
- 🌐 网页: HTML
- 📦 数据: JSON
- 🗜️ 压缩: ZIP

### 4.2 单文件上传

1. 选择或创建知识库
2. 点击 **"上传文档"** 按钮
3. 选择文件
4. 等待处理完成

### 4.3 批量上传

1. 点击 **"批量上传文件夹"** 按钮
2. 选择包含文档的文件夹
3. 系统自动处理所有支持的文件

### 4.4 上传选项

**操作模式**:
- **➕ 追加**: 添加新文档到现有知识库
- **🔄 覆盖**: 删除旧文档，重新构建

**性能选项**:
- **提取元数据**: 提取关键词、分类等（降低 30% 速度）
- 默认关闭，追求速度

### 4.5 处理进度

上传后会显示处理进度：
- 步骤 1/6: 检查现有索引
- 步骤 2/6: 扫描文件
- 步骤 3/6: 读取文档
- 步骤 4/6: 构建清单
- 步骤 5/6: 解析片段
- 步骤 6/6: 构建索引

---

## 5. 智能问答

### 5.1 开始对话

1. 选择知识库
2. 在底部输入框输入问题
3. 按 Enter 或点击发送
4. 等待回答生成

### 5.2 查看来源

回答下方会显示：
- 📄 引用来源（文件名、页码）
- 📊 相关文档片段
- 🔗 可点击查看详情

### 5.3 引用内容提问

1. 在回答中选择文本
2. 点击 **"📌 引用此内容提问"**
3. 输入新问题
4. 系统会基于引用内容回答

### 5.4 推荐问题

回答后会显示 3 个推荐问题：
- 点击问题按钮直接提问
- 推荐基于上下文生成

### 5.5 撤销提问

点击 **"↩️ 撤销提问"** 删除最后一组问答

---

待续...（第6-10部分）

## 6. 高级功能

### 6.1 检索增强

#### BM25 混合检索
- 位置: 侧边栏 → **🔧 高级配置**
- 功能: 关键词 + 语义双重检索
- 效果: 准确率提升 5-10%
- 勾选 **"启用 BM25 混合检索"**

#### Re-ranking 重排序
- 功能: Cross-Encoder 二次排序
- 效果: 准确率提升 10-20%
- 勾选 **"启用 Re-ranking 重排序"**

### 6.2 RAG 参数调整

在 **"🔧 高级配置"** 中调整：

- **Chunk Size**: 文档分块大小（默认 500）
  - 较小: 更精确，但可能丢失上下文
  - 较大: 更完整，但可能不够精确

- **Chunk Overlap**: 分块重叠长度（默认 50）
  - 避免重要信息被切断

- **Top K**: 检索文档数量（默认 5）
  - 较小: 更快，但可能遗漏信息
  - 较大: 更全面，但速度较慢

- **相似度阈值**: 过滤低相关文档（默认 0.7）
  - 较高: 更严格，结果更精准
  - 较低: 更宽松，结果更全面

### 6.3 对话历史管理

#### 导出对话
1. 选择知识库
2. 点击 **"💾 导出对话"**
3. 选择格式（JSON/TXT）
4. 保存到本地

#### 查看统计
点击 **"📊 查看统计"** 查看：
- 总对话轮数
- 总提问数
- 平均回答长度
- 使用时长

#### 清空对话
点击 **"🗑️ 清空对话"** 删除当前知识库的所有对话历史

---

## 7. 性能监控

### 7.1 打开监控面板

在侧边栏找到 **"📊 性能监控"**，点击展开

### 7.2 查询性能统计

显示内容：
- **平均耗时**: 所有查询的平均时间
- **最快**: 最快查询的时间
- **最慢**: 最慢查询的时间

### 7.3 查询统计

- **总查询数**: 累计查询次数
- **总耗时**: 累计查询时间

### 7.4 最近查询

显示最近一次查询的：
- 耗时
- 检索文档数

### 7.5 操作

- **🔄 刷新**: 更新统计数据
- **🗑️ 清空**: 重置所有统计

---

## 8. 推荐问题管理

### 8.1 打开管理面板

在侧边栏找到 **"💡 推荐问题管理"**，点击展开

### 8.2 自定义推荐

#### 添加推荐问题
1. 切换到 **"📝 自定义推荐"** 标签
2. 在输入框输入问题
3. 点击 **"➕ 添加"**

#### 删除推荐问题
点击问题右侧的 **"🗑️"** 按钮

#### 优先级
自定义推荐会优先显示在推荐列表中

### 8.3 统计信息

切换到 **"📊 统计信息"** 标签查看：
- 自定义推荐数量
- 历史记录数量
- 队列中问题数量

操作：
- **🗑️ 清空历史**: 删除所有历史记录
- **🗑️ 清空队列**: 删除队列中的问题

### 8.4 历史记录

切换到 **"📜 历史记录"** 标签查看最近生成的推荐问题（最多 20 条）

---

## 9. 常见问题

### Q1: 上传文档后没有反应？

**原因**:
- 文档格式不支持
- 文件过大（>100MB）
- 文档内容为空

**解决**:
1. 检查文件格式是否在支持列表中
2. 检查文件大小
3. 查看终端日志获取详细错误

### Q2: 对话没有引用来源？

**原因**:
- 知识库为空
- 相似度阈值过高
- 问题与知识库内容不相关

**解决**:
1. 确认知识库中有文档
2. 降低相似度阈值（0.5-0.6）
3. 调整问题表述

### Q3: 查询速度很慢？

**原因**:
- 知识库过大
- Top K 设置过高
- 启用了多个检索增强

**解决**:
1. 减少 Top K 值（3-5）
2. 关闭不必要的检索增强
3. 使用更快的嵌入模型

### Q4: 回答质量不高？

**原因**:
- 文档质量差
- RAG 参数不合适
- LLM 模型能力不足

**解决**:
1. 上传高质量文档
2. 启用 BM25 和 Re-ranking
3. 使用更强的 LLM 模型

### Q5: 如何使用本地模型？

**步骤**:
1. 安装 Ollama: `brew install ollama`
2. 下载模型: `ollama pull qwen2.5:7b`
3. 启动服务: `ollama serve`
4. 在应用中选择 Ollama 并配置

### Q6: 如何清理缓存？

```bash
# 清理向量数据库
rm -rf vector_db_storage/

# 清理对话历史
rm -rf chat_histories/

# 清理临时文件
rm -rf temp_uploads/

# 清理模型缓存
rm -rf hf_cache/
```

---

## 10. 故障排除

### 10.1 应用无法启动

**症状**: 运行 `streamlit run` 后报错

**检查**:
1. Python 版本是否 >= 3.8
2. 依赖是否完整安装
3. 端口 8501 是否被占用

**解决**:
```bash
# 检查 Python 版本
python --version

# 重新安装依赖
pip install -r requirements.txt --force-reinstall

# 更换端口
streamlit run src/apppro.py --server.port 8502
```

### 10.2 模型加载失败

**症状**: 提示 "模型加载失败"

**检查**:
1. 网络连接是否正常
2. API Key 是否正确
3. 模型名称是否正确

**解决**:
```bash
# 测试 Ollama 连接
curl http://localhost:11434/api/tags

# 测试 OpenAI 连接
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer YOUR_API_KEY"
```

### 10.3 向量化失败

**症状**: 文档上传后处理失败

**检查**:
1. 文档是否损坏
2. 内存是否充足
3. GPU 是否可用

**解决**:
1. 尝试上传其他文档
2. 关闭其他程序释放内存
3. 检查 GPU 驱动

### 10.4 查询无响应

**症状**: 输入问题后长时间无响应

**检查**:
1. LLM 服务是否正常
2. 网络是否稳定
3. 知识库是否过大

**解决**:
1. 重启 LLM 服务
2. 检查网络连接
3. 减少 Top K 值

### 10.5 查看日志

```bash
# 查看应用日志
ls -lh app_logs/

# 查看最新日志
tail -f app_logs/log_$(date +%Y%m%d).jsonl

# 查看详细日志
python view_logs.py --date $(date +%Y%m%d)
```

---

## 📞 获取帮助

### 文档资源
- [README.md](README.md) - 项目概述
- [CHANGELOG.md](CHANGELOG.md) - 更新日志
- [FAQ.md](FAQ.md) - 常见问题

### 在线支持
- GitHub Issues: https://github.com/zhaosj0315/rag-pro-max/issues
- GitHub Discussions: https://github.com/zhaosj0315/rag-pro-max/discussions

### 反馈建议
欢迎通过 GitHub Issues 提交：
- Bug 报告
- 功能建议
- 使用问题

---

**最后更新**: 2025-12-09  
**版本**: v1.5.1  
**作者**: AI Assistant

