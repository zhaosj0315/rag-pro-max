#!/usr/bin/env python3
"""
RAG Pro Max ä»£ç åŒæ­¥å·¥å…·
åŸºäºå››å±‚æ¶æ„è®¾è®¡çš„å®Œæ•´ä»£ç å’Œæ–‡æ¡£é€»è¾‘åŒæ­¥
"""

import os
import json
import shutil
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Tuple
import logging

class CodeSyncManager:
    """ä»£ç åŒæ­¥ç®¡ç†å™¨"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root or os.getcwd())
        self.sync_config = self._load_sync_config()
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = self.project_root / "sync_logs"
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / f"sync_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def _load_sync_config(self) -> Dict:
        """åŠ è½½åŒæ­¥é…ç½®"""
        config_file = self.project_root / "sync_config.json"
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # é»˜è®¤é…ç½®
        default_config = {
            "architecture_layers": {
                "ui": ["src/ui/", "src/app/", "src/auth/"],
                "service": ["src/services/", "src/processors/", "src/engines/"],
                "common": ["src/common/", "src/utils/", "src/config/"],
                "tools": ["src/api/", "src/monitoring/", "src/queue/"]
            },
            "core_files": [
                "src/apppro.py",
                "src/file_processor.py", 
                "src/rag_engine.py"
            ],
            "documentation": [
                "README.md",
                "DEPLOYMENT.md",
                "API_DOCUMENTATION.md",
                "CHANGELOG.md"
            ],
            "exclude_patterns": [
                "__pycache__",
                "*.pyc",
                ".git",
                "node_modules",
                "temp_uploads",
                "vector_db_storage",
                "chat_histories",
                "app_logs"
            ]
        }
        
        # ä¿å­˜é»˜è®¤é…ç½®
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, indent=2, ensure_ascii=False)
            
        return default_config
    
    def calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            self.logger.warning(f"æ— æ³•è®¡ç®—æ–‡ä»¶å“ˆå¸Œ: {file_path} - {e}")
            return ""
    
    def scan_codebase(self) -> Dict[str, Dict]:
        """æ‰«æä»£ç åº“"""
        codebase_info = {
            "timestamp": datetime.now().isoformat(),
            "layers": {},
            "core_files": {},
            "documentation": {},
            "statistics": {}
        }
        
        # æ‰«ææ¶æ„å±‚
        for layer_name, directories in self.sync_config["architecture_layers"].items():
            layer_files = {}
            for directory in directories:
                dir_path = self.project_root / directory
                if dir_path.exists():
                    layer_files.update(self._scan_directory(dir_path))
            codebase_info["layers"][layer_name] = layer_files
        
        # æ‰«ææ ¸å¿ƒæ–‡ä»¶
        for core_file in self.sync_config["core_files"]:
            file_path = self.project_root / core_file
            if file_path.exists():
                codebase_info["core_files"][core_file] = {
                    "hash": self.calculate_file_hash(file_path),
                    "size": file_path.stat().st_size,
                    "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                }
        
        # æ‰«ææ–‡æ¡£
        for doc_file in self.sync_config["documentation"]:
            file_path = self.project_root / doc_file
            if file_path.exists():
                codebase_info["documentation"][doc_file] = {
                    "hash": self.calculate_file_hash(file_path),
                    "size": file_path.stat().st_size,
                    "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                }
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_files = sum(len(files) for files in codebase_info["layers"].values())
        total_files += len(codebase_info["core_files"])
        total_files += len(codebase_info["documentation"])
        
        codebase_info["statistics"] = {
            "total_files": total_files,
            "layers_count": len(codebase_info["layers"]),
            "core_files_count": len(codebase_info["core_files"]),
            "documentation_count": len(codebase_info["documentation"])
        }
        
        return codebase_info
    
    def _scan_directory(self, directory: Path) -> Dict[str, Dict]:
        """æ‰«æç›®å½•"""
        files_info = {}
        
        for file_path in directory.rglob("*"):
            if file_path.is_file() and not self._should_exclude(file_path):
                relative_path = str(file_path.relative_to(self.project_root))
                files_info[relative_path] = {
                    "hash": self.calculate_file_hash(file_path),
                    "size": file_path.stat().st_size,
                    "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                    "extension": file_path.suffix
                }
        
        return files_info
    
    def _should_exclude(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤æ–‡ä»¶"""
        path_str = str(file_path)
        for pattern in self.sync_config["exclude_patterns"]:
            if pattern in path_str:
                return True
        return False
    
    def generate_sync_report(self) -> str:
        """ç”ŸæˆåŒæ­¥æŠ¥å‘Š"""
        codebase_info = self.scan_codebase()
        
        report = f"""
# RAG Pro Max ä»£ç åŒæ­¥æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {codebase_info['timestamp']}

## é¡¹ç›®æ¦‚è§ˆ
- æ€»æ–‡ä»¶æ•°: {codebase_info['statistics']['total_files']}
- æ¶æ„å±‚æ•°: {codebase_info['statistics']['layers_count']}
- æ ¸å¿ƒæ–‡ä»¶æ•°: {codebase_info['statistics']['core_files_count']}
- æ–‡æ¡£æ–‡ä»¶æ•°: {codebase_info['statistics']['documentation_count']}

## å››å±‚æ¶æ„åˆ†æ

### è¡¨ç°å±‚ (UI Layer)
"""
        
        # åˆ†æå„å±‚
        for layer_name, files in codebase_info["layers"].items():
            layer_display = {
                "ui": "è¡¨ç°å±‚ (UI Layer)",
                "service": "æœåŠ¡å±‚ (Service Layer)", 
                "common": "å…¬å…±å±‚ (Common Layer)",
                "tools": "å·¥å…·å±‚ (Tools Layer)"
            }
            
            report += f"\n### {layer_display.get(layer_name, layer_name)}\n"
            report += f"æ–‡ä»¶æ•°é‡: {len(files)}\n"
            
            # æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡
            extensions = {}
            for file_info in files.values():
                ext = file_info.get('extension', 'unknown')
                extensions[ext] = extensions.get(ext, 0) + 1
            
            report += "æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:\n"
            for ext, count in sorted(extensions.items()):
                report += f"  - {ext or 'æ— æ‰©å±•å'}: {count} ä¸ªæ–‡ä»¶\n"
        
        # æ ¸å¿ƒæ–‡ä»¶çŠ¶æ€
        report += "\n## æ ¸å¿ƒæ–‡ä»¶çŠ¶æ€\n"
        for file_name, file_info in codebase_info["core_files"].items():
            report += f"- {file_name}: {file_info['size']} bytes, ä¿®æ”¹æ—¶é—´: {file_info['modified']}\n"
        
        # æ–‡æ¡£çŠ¶æ€
        report += "\n## æ–‡æ¡£çŠ¶æ€\n"
        for doc_name, doc_info in codebase_info["documentation"].items():
            report += f"- {doc_name}: {doc_info['size']} bytes, ä¿®æ”¹æ—¶é—´: {doc_info['modified']}\n"
        
        return report
    
    def create_backup(self) -> str:
        """åˆ›å»ºå¤‡ä»½"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir = self.project_root / "backups" / f"backup_{timestamp}"
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"åˆ›å»ºå¤‡ä»½åˆ°: {backup_dir}")
        
        # å¤‡ä»½æ ¸å¿ƒæ–‡ä»¶å’Œç›®å½•
        backup_items = [
            "src/",
            "config/",
            "README.md",
            "requirements.txt"
        ]
        
        for item in backup_items:
            source = self.project_root / item
            if source.exists():
                if source.is_dir():
                    shutil.copytree(source, backup_dir / item, ignore=shutil.ignore_patterns(*self.sync_config["exclude_patterns"]))
                else:
                    shutil.copy2(source, backup_dir / item)
        
        # ä¿å­˜åŒæ­¥ä¿¡æ¯
        sync_info = self.scan_codebase()
        with open(backup_dir / "sync_info.json", 'w', encoding='utf-8') as f:
            json.dump(sync_info, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"å¤‡ä»½å®Œæˆ: {backup_dir}")
        return str(backup_dir)
    
    def validate_architecture(self) -> List[str]:
        """éªŒè¯æ¶æ„å®Œæ•´æ€§"""
        issues = []
        
        # æ£€æŸ¥æ ¸å¿ƒæ–‡ä»¶
        for core_file in self.sync_config["core_files"]:
            file_path = self.project_root / core_file
            if not file_path.exists():
                issues.append(f"ç¼ºå¤±æ ¸å¿ƒæ–‡ä»¶: {core_file}")
        
        # æ£€æŸ¥æ¶æ„å±‚ç›®å½•
        for layer_name, directories in self.sync_config["architecture_layers"].items():
            for directory in directories:
                dir_path = self.project_root / directory
                if not dir_path.exists():
                    issues.append(f"ç¼ºå¤±{layer_name}å±‚ç›®å½•: {directory}")
        
        # æ£€æŸ¥æ–‡æ¡£
        for doc_file in self.sync_config["documentation"]:
            file_path = self.project_root / doc_file
            if not file_path.exists():
                issues.append(f"ç¼ºå¤±æ–‡æ¡£æ–‡ä»¶: {doc_file}")
        
        return issues
    
    def sync_all(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´åŒæ­¥"""
        self.logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´ä»£ç åŒæ­¥...")
        
        # 1. éªŒè¯æ¶æ„
        issues = self.validate_architecture()
        if issues:
            self.logger.warning(f"å‘ç°æ¶æ„é—®é¢˜: {len(issues)} ä¸ª")
            for issue in issues:
                self.logger.warning(f"  - {issue}")
        
        # 2. åˆ›å»ºå¤‡ä»½
        backup_path = self.create_backup()
        
        # 3. æ‰«æä»£ç åº“
        codebase_info = self.scan_codebase()
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_sync_report()
        
        # 5. ä¿å­˜åŒæ­¥ç»“æœ
        sync_result_dir = self.project_root / "sync_results"
        sync_result_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜è¯¦ç»†ä¿¡æ¯
        with open(sync_result_dir / f"codebase_info_{timestamp}.json", 'w', encoding='utf-8') as f:
            json.dump(codebase_info, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æŠ¥å‘Š
        with open(sync_result_dir / f"sync_report_{timestamp}.md", 'w', encoding='utf-8') as f:
            f.write(report)
        
        sync_summary = {
            "timestamp": datetime.now().isoformat(),
            "backup_path": backup_path,
            "total_files": codebase_info["statistics"]["total_files"],
            "architecture_issues": len(issues),
            "sync_status": "completed"
        }
        
        self.logger.info(f"åŒæ­¥å®Œæˆ! æ€»æ–‡ä»¶æ•°: {sync_summary['total_files']}")
        self.logger.info(f"å¤‡ä»½è·¯å¾„: {backup_path}")
        self.logger.info(f"æŠ¥å‘Šä¿å­˜è‡³: sync_results/")
        
        return sync_summary

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ RAG Pro Max ä»£ç åŒæ­¥å·¥å…·")
    print("=" * 50)
    
    # åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨
    sync_manager = CodeSyncManager()
    
    try:
        # æ‰§è¡ŒåŒæ­¥
        result = sync_manager.sync_all()
        
        print("\nâœ… åŒæ­¥å®Œæˆ!")
        print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {result['total_files']}")
        print(f"ğŸ’¾ å¤‡ä»½è·¯å¾„: {result['backup_path']}")
        print(f"âš ï¸  æ¶æ„é—®é¢˜: {result['architecture_issues']} ä¸ª")
        
        # æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆ
        print("\nğŸ“‹ åŒæ­¥æŠ¥å‘Šé¢„è§ˆ:")
        print("-" * 30)
        report = sync_manager.generate_sync_report()
        print(report[:500] + "..." if len(report) > 500 else report)
        
    except Exception as e:
        print(f"âŒ åŒæ­¥å¤±è´¥: {e}")
        logging.error(f"åŒæ­¥å¤±è´¥: {e}", exc_info=True)

if __name__ == "__main__":
    main()
