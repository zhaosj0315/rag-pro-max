#!/usr/bin/env python3
"""
RAG Pro Max v1.7 å¯è¡Œæ€§æµ‹è¯•
æµ‹è¯•å¹¶å‘ä¼˜åŒ–åŠŸèƒ½
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.utils.async_pipeline import AsyncPipeline, run_async_pipeline
from src.utils.dynamic_batch import DynamicBatchOptimizer
from src.utils.smart_scheduler import SmartScheduler, TaskType
from src.utils.concurrency_manager import ConcurrencyManager
import time
import asyncio


def test_dynamic_batch_optimizer():
    """æµ‹è¯•åŠ¨æ€æ‰¹é‡ä¼˜åŒ–å™¨"""
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯• 1: åŠ¨æ€æ‰¹é‡ä¼˜åŒ–å™¨")
    print("="*60)
    
    optimizer = DynamicBatchOptimizer(embedding_dim=1024)
    
    test_cases = [
        (5, 512, "å°æ‰¹é‡"),
        (50, 2048, "ä¸­æ‰¹é‡"),
        (500, None, "å¤§æ‰¹é‡ï¼ˆåŠ¨æ€ï¼‰")
    ]
    
    passed = 0
    failed = 0
    
    for doc_count, expected_min, desc in test_cases:
        batch_size = optimizer.calculate_batch_size(doc_count)
        config = optimizer.get_optimal_config(doc_count)
        
        if expected_min is None or batch_size >= expected_min:
            print(f"   âœ… {desc}: {doc_count}æ–‡æ¡£ â†’ batch_size={batch_size}")
            print(f"      è®¾å¤‡: {config['device']}, å¯ç”¨å†…å­˜: {config['available_memory_gb']:.1f}GB")
            passed += 1
        else:
            print(f"   âŒ {desc}: æœŸæœ›>={expected_min}, å®é™…={batch_size}")
            failed += 1
    
    print(f"\n   ç»“æœ: {passed} é€šè¿‡, {failed} å¤±è´¥")
    return failed == 0


def test_smart_scheduler():
    """æµ‹è¯•æ™ºèƒ½ä»»åŠ¡è°ƒåº¦å™¨"""
    print("\n" + "="*60)
    print("ğŸ¯ æµ‹è¯• 2: æ™ºèƒ½ä»»åŠ¡è°ƒåº¦å™¨")
    print("="*60)
    
    def cpu_task(x):
        return x * 2
    
    def gpu_task(x):
        return x ** 2
    
    def io_task(x):
        time.sleep(0.001)
        return x + 1
    
    with SmartScheduler() as scheduler:
        # æäº¤ä¸åŒç±»å‹çš„ä»»åŠ¡
        cpu_future = scheduler.submit(TaskType.CPU_INTENSIVE, cpu_task, 10)
        gpu_future = scheduler.submit(TaskType.GPU_INTENSIVE, gpu_task, 5)
        io_future = scheduler.submit(TaskType.IO_INTENSIVE, io_task, 3)
        
        # è·å–ç»“æœ
        cpu_result = cpu_future.result()
        gpu_result = gpu_future.result()
        io_result = io_future.result()
        
        stats = scheduler.get_stats()
        
        print(f"   âœ… CPUä»»åŠ¡: 10 * 2 = {cpu_result}")
        print(f"   âœ… GPUä»»åŠ¡: 5 ** 2 = {gpu_result}")
        print(f"   âœ… IOä»»åŠ¡: 3 + 1 = {io_result}")
        print(f"   ğŸ“Š ç»Ÿè®¡: CPU={stats['cpu_tasks']}, GPU={stats['gpu_tasks']}, IO={stats['io_tasks']}")
    
    return True


def test_async_pipeline():
    """æµ‹è¯•å¼‚æ­¥å‘é‡åŒ–ç®¡é“"""
    print("\n" + "="*60)
    print("âš¡ æµ‹è¯• 3: å¼‚æ­¥å‘é‡åŒ–ç®¡é“")
    print("="*60)
    
    # æ¨¡æ‹Ÿæ–‡æ¡£
    documents = [f"doc_{i}" for i in range(20)]
    
    def parse_func(doc):
        time.sleep(0.01)  # æ¨¡æ‹Ÿè§£æ
        return f"parsed_{doc}"
    
    def embed_func(parsed):
        time.sleep(0.02)  # æ¨¡æ‹Ÿå‘é‡åŒ–
        return f"embedded_{parsed}"
    
    def store_func(embedded):
        time.sleep(0.005)  # æ¨¡æ‹Ÿå­˜å‚¨
        return f"stored_{embedded}"
    
    start = time.time()
    stats = run_async_pipeline(documents, parse_func, embed_func, store_func)
    elapsed = time.time() - start
    
    print(f"   âœ… å¤„ç†æ–‡æ¡£: {stats['stored']} ä¸ª")
    print(f"   â±ï¸  æ€»è€—æ—¶: {elapsed:.2f}s")
    print(f"   ğŸ“Š è§£æ: {stats['parse_time']:.2f}s")
    print(f"   ğŸ“Š å‘é‡åŒ–: {stats['embed_time']:.2f}s")
    print(f"   ğŸ“Š å­˜å‚¨: {stats['store_time']:.2f}s")
    print(f"   ğŸš€ ååé‡: {stats['stored']/elapsed:.1f} docs/s")
    
    # éªŒè¯å¹¶è¡Œæ•ˆæœ
    serial_time = stats['parse_time'] + stats['embed_time'] + stats['store_time']
    speedup = serial_time / elapsed
    
    print(f"   âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    return stats['stored'] == len(documents) and speedup > 1.5


def test_concurrency_manager():
    """æµ‹è¯•å¹¶å‘ä¼˜åŒ–ç®¡ç†å™¨"""
    print("\n" + "="*60)
    print("ğŸ”§ æµ‹è¯• 4: å¹¶å‘ä¼˜åŒ–ç®¡ç†å™¨")
    print("="*60)
    
    manager = ConcurrencyManager(embedding_dim=1024)
    
    # æµ‹è¯•è·å–æœ€ä¼˜batch size
    batch_sizes = []
    for doc_count in [5, 50, 500]:
        batch_size = manager.get_optimal_batch_size(doc_count)
        batch_sizes.append(batch_size)
        print(f"   âœ… {doc_count}æ–‡æ¡£ â†’ batch_size={batch_size}")
    
    # éªŒè¯batch sizeé€’å¢
    if batch_sizes[0] < batch_sizes[1] <= batch_sizes[2]:
        print(f"   âœ… Batch sizeé€’å¢åˆç†")
        return True
    else:
        print(f"   âŒ Batch sizeé€’å¢ä¸åˆç†")
        return False


def test_performance_comparison():
    """æµ‹è¯•æ€§èƒ½å¯¹æ¯”"""
    print("\n" + "="*60)
    print("ğŸ“ˆ æµ‹è¯• 5: æ€§èƒ½å¯¹æ¯”")
    print("="*60)
    
    documents = [f"doc_{i}" for i in range(50)]
    
    def parse_func(doc):
        time.sleep(0.005)
        return f"parsed_{doc}"
    
    def embed_func(parsed):
        time.sleep(0.01)
        return f"embedded_{parsed}"
    
    def store_func(embedded):
        time.sleep(0.002)
        return f"stored_{embedded}"
    
    # ä¸²è¡Œå¤„ç†
    print("   æµ‹è¯•ä¸²è¡Œå¤„ç†...")
    start = time.time()
    for doc in documents:
        parsed = parse_func(doc)
        embedded = embed_func(parsed)
        stored = store_func(embedded)
    serial_time = time.time() - start
    print(f"   â±ï¸  ä¸²è¡Œè€—æ—¶: {serial_time:.2f}s")
    
    # å¼‚æ­¥ç®¡é“å¤„ç†
    print("   æµ‹è¯•å¼‚æ­¥ç®¡é“...")
    start = time.time()
    stats = run_async_pipeline(documents, parse_func, embed_func, store_func)
    pipeline_time = time.time() - start
    print(f"   â±ï¸  ç®¡é“è€—æ—¶: {pipeline_time:.2f}s")
    
    speedup = serial_time / pipeline_time
    print(f"   âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    if speedup > 1.5:
        print(f"   âœ… æ€§èƒ½æå‡æ˜¾è‘— (>{speedup:.1f}x)")
        return True
    else:
        print(f"   âš ï¸ æ€§èƒ½æå‡æœ‰é™ ({speedup:.1f}x)")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸš€ RAG Pro Max v1.7 å¯è¡Œæ€§æµ‹è¯•")
    print("="*60)
    print("\næµ‹è¯•å†…å®¹:")
    print("  1. åŠ¨æ€æ‰¹é‡ä¼˜åŒ–å™¨")
    print("  2. æ™ºèƒ½ä»»åŠ¡è°ƒåº¦å™¨")
    print("  3. å¼‚æ­¥å‘é‡åŒ–ç®¡é“")
    print("  4. å¹¶å‘ä¼˜åŒ–ç®¡ç†å™¨")
    print("  5. æ€§èƒ½å¯¹æ¯”")
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("åŠ¨æ€æ‰¹é‡ä¼˜åŒ–å™¨", test_dynamic_batch_optimizer()))
    results.append(("æ™ºèƒ½ä»»åŠ¡è°ƒåº¦å™¨", test_smart_scheduler()))
    results.append(("å¼‚æ­¥å‘é‡åŒ–ç®¡é“", test_async_pipeline()))
    results.append(("å¹¶å‘ä¼˜åŒ–ç®¡ç†å™¨", test_concurrency_manager()))
    results.append(("æ€§èƒ½å¯¹æ¯”", test_performance_comparison()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*60)
    
    passed = sum(1 for _, result in results if result)
    failed = len(results) - passed
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {status}: {name}")
    
    print(f"\n   æ€»è®¡: {passed}/{len(results)} é€šè¿‡")
    
    if failed == 0:
        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼v1.7 å¯è¡Œæ€§éªŒè¯æˆåŠŸ")
        print("="*60)
        print("\né¢„æœŸæ”¶ç›Š:")
        print("  âš¡ GPUåˆ©ç”¨ç‡æå‡ 15%+")
        print("  ğŸš€ å¤„ç†é€Ÿåº¦æå‡ 40%+")
        print("  ğŸ’¾ å†…å­˜å ç”¨å‡å°‘ 33%")
        return 0
    else:
        print("\n" + "="*60)
        print(f"âŒ {failed} ä¸ªæµ‹è¯•å¤±è´¥")
        print("="*60)
        return 1


if __name__ == "__main__":
    sys.exit(main())
