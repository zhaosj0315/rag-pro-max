"""
å¢å¼ºç‰ˆç½‘é¡µçˆ¬è™« - é›†æˆå¼‚æ­¥å¹¶å‘å’ŒåŸæœ‰åŠŸèƒ½
"""

import asyncio
from .async_web_crawler import AsyncWebCrawler
from .web_crawler import WebCrawler
import time
from pathlib import Path

class EnhancedWebCrawler:
    def __init__(self):
        self.sync_crawler = WebCrawler()
        self.async_crawler = None
    
    async def crawl_async(
        self,
        start_url: str,
        max_depth: int = 3,
        max_pages: int = 8,
        parser_type: str = "default",
        exclude_patterns: list = None,
        status_callback=None,
        use_async: bool = True,
        max_concurrent: int = 10,
        ignore_robots: bool = False
    ):
        """å¼‚æ­¥çˆ¬å–å…¥å£"""
        
        if not use_async:
            # ä½¿ç”¨åŸæœ‰åŒæ­¥çˆ¬è™«
            return self.sync_crawler.crawl_recursive(
                start_url, max_depth, max_pages, parser_type, exclude_patterns, status_callback
            )
        
        # ä½¿ç”¨æ–°çš„å¼‚æ­¥çˆ¬è™«
        async with AsyncWebCrawler(max_concurrent=max_concurrent, ignore_robots=ignore_robots) as crawler:
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            timestamp = int(time.time())
            temp_dir = f"temp_crawl_{timestamp}"
            
            if status_callback:
                status_callback(f"ğŸš€ å¯ç”¨å¼‚æ­¥çˆ¬è™« (å¹¶å‘:{max_concurrent})")
            
            try:
                files = await crawler.crawl_recursive(
                    start_url=start_url,
                    max_depth=max_depth,
                    max_pages_per_level=max_pages,
                    output_dir=temp_dir,
                    status_callback=status_callback
                )
                
                # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼ - ç§»åŠ¨æ–‡ä»¶åˆ°temp_uploadsç›®å½•
                import shutil
                import os
                from urllib.parse import urlparse
                from datetime import datetime
                
                # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                os.makedirs("temp_uploads", exist_ok=True)
                
                # ç”Ÿæˆæœ€ç»ˆç›®å½•å
                try:
                    domain = urlparse(start_url).netloc.replace('.', '_')
                except:
                    domain = "unknown"
                
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                final_dir = os.path.join("temp_uploads", f"Web_{domain}_{timestamp}")
                os.makedirs(final_dir, exist_ok=True)
                
                # ç§»åŠ¨æ–‡ä»¶å¹¶è¿”å›æ–°è·¯å¾„
                moved_files = []
                for file_path in files:
                    if os.path.exists(file_path):
                        filename = os.path.basename(file_path)
                        new_path = os.path.join(final_dir, filename)
                        shutil.move(file_path, new_path)
                        moved_files.append(new_path)
                
                # æ¸…ç†ä¸´æ—¶ç›®å½•
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)
                
                if status_callback:
                    status_callback(f"âœ… å¼‚æ­¥çˆ¬å–å®Œæˆï¼Œæ–‡ä»¶å·²ç§»åŠ¨åˆ°: {final_dir}")
                
                return moved_files
                
            except Exception as e:
                if status_callback:
                    status_callback(f"âŒ å¼‚æ­¥çˆ¬å–å¤±è´¥: {e}")
                    status_callback(f"ğŸ”„ å›é€€åˆ°åŒæ­¥æ¨¡å¼")
                
                # å›é€€åˆ°åŒæ­¥çˆ¬è™«
                return self.sync_crawler.crawl_recursive(
                    start_url, max_depth, max_pages, parser_type, exclude_patterns, status_callback
                )
    
    def crawl_sync(self, *args, **kwargs):
        """åŒæ­¥çˆ¬å–å…¥å£ï¼ˆå…¼å®¹æ€§ï¼‰"""
        return self.sync_crawler.crawl_recursive(*args, **kwargs)

# å·¥å‚å‡½æ•°
def create_crawler(async_mode: bool = True) -> EnhancedWebCrawler:
    """åˆ›å»ºçˆ¬è™«å®ä¾‹"""
    return EnhancedWebCrawler()

# å¼‚æ­¥åŒ…è£…å™¨
def run_async_crawl(*args, **kwargs):
    """åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥çˆ¬è™«"""
    crawler = EnhancedWebCrawler()
    
    try:
        # å°è¯•è·å–å½“å‰äº‹ä»¶å¾ªç¯
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # å¦‚æœå·²æœ‰äº‹ä»¶å¾ªç¯åœ¨è¿è¡Œï¼Œåˆ›å»ºä»»åŠ¡
            return asyncio.create_task(crawler.crawl_async(*args, **kwargs))
        else:
            # è¿è¡Œæ–°çš„äº‹ä»¶å¾ªç¯
            return loop.run_until_complete(crawler.crawl_async(*args, **kwargs))
    except RuntimeError:
        # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
        return asyncio.run(crawler.crawl_async(*args, **kwargs))
