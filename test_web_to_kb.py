#!/usr/bin/env python3
"""
ç½‘é¡µæŠ“å–åˆ°çŸ¥è¯†åº“åŠŸèƒ½æµ‹è¯•è„šæœ¬
"""

import os
import sys
import time

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("ğŸ§ª æµ‹è¯•1: æ¨¡å—å¯¼å…¥")
    try:
        from src.processors.web_to_kb_simple import (
            crawl_and_create_kb, 
            generate_kb_name_from_web, 
            get_preset_search_sites
        )
        print("âœ… ç®€åŒ–ç‰ˆæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        from src.processors.web_to_kb_processor import WebToKBProcessor
        print("âœ… å®Œæ•´ç‰ˆæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        from src.ui.web_to_kb_interface import WebToKBInterface
        print("âœ… UIç•Œé¢æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_kb_naming():
    """æµ‹è¯•æ™ºèƒ½å‘½ååŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•2: æ™ºèƒ½å‘½ååŠŸèƒ½")
    
    try:
        from src.processors.web_to_kb_simple import generate_kb_name_from_web
        
        test_cases = [
            ("https://zh.wikipedia.org/wiki/Python", "ç™¾ç§‘_Python"),
            ("https://github.com/python/cpython", "é¡¹ç›®_cpython"),
            ("https://docs.python.org/3/", "docs_"),
            ("https://stackoverflow.com/questions/tagged/python", "ç¼–ç¨‹é—®ç­”"),
            ("https://blog.csdn.net/article/python", "CSDNæŠ€æœ¯"),
        ]
        
        for url, expected_prefix in test_cases:
            result = generate_kb_name_from_web(url, 5)
            print(f"  URL: {url}")
            print(f"  ç”Ÿæˆåç§°: {result}")
            if expected_prefix in result or expected_prefix.startswith(result[:5]):
                print("  âœ… é€šè¿‡")
            else:
                print("  âš ï¸ å¯èƒ½éœ€è¦è°ƒæ•´")
            print()
        
        return True
    except Exception as e:
        print(f"âŒ æ™ºèƒ½å‘½åæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_preset_sites():
    """æµ‹è¯•é¢„è®¾ç½‘ç«™"""
    print("ğŸ§ª æµ‹è¯•3: é¢„è®¾ç½‘ç«™é…ç½®")
    
    try:
        from src.processors.web_to_kb_simple import get_preset_search_sites
        
        sites = get_preset_search_sites()
        print(f"  é¢„è®¾ç½‘ç«™æ•°é‡: {len(sites)}")
        
        required_sites = ["ç»´åŸºç™¾ç§‘", "ç™¾åº¦ç™¾ç§‘", "çŸ¥ä¹", "CSDN", "GitHub", "Stack Overflow"]
        for site in required_sites:
            if site in sites:
                url_template = sites[site]
                if "{keyword}" in url_template:
                    print(f"  âœ… {site}: {url_template}")
                else:
                    print(f"  âš ï¸ {site}: ç¼ºå°‘å…³é”®è¯å ä½ç¬¦")
            else:
                print(f"  âŒ ç¼ºå°‘ç½‘ç«™: {site}")
        
        return True
    except Exception as e:
        print(f"âŒ é¢„è®¾ç½‘ç«™æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_directory_structure():
    """æµ‹è¯•ç›®å½•ç»“æ„"""
    print("\nğŸ§ª æµ‹è¯•4: ç›®å½•ç»“æ„")
    
    required_dirs = [
        "temp_uploads",
        "vector_db_storage", 
        "src/processors",
        "src/ui"
    ]
    
    all_good = True
    for dir_path in required_dirs:
        if os.path.exists(dir_path):
            print(f"  âœ… {dir_path}")
        else:
            print(f"  âŒ ç¼ºå°‘ç›®å½•: {dir_path}")
            all_good = False
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    required_files = [
        "src/processors/web_crawler.py",
        "src/processors/web_to_kb_simple.py",
        "src/processors/web_to_kb_processor.py",
        "src/ui/web_to_kb_interface.py"
    ]
    
    for file_path in required_files:
        if os.path.exists(file_path):
            print(f"  âœ… {file_path}")
        else:
            print(f"  âŒ ç¼ºå°‘æ–‡ä»¶: {file_path}")
            all_good = False
    
    return all_good

def test_web_crawler():
    """æµ‹è¯•ç½‘é¡µæŠ“å–å™¨"""
    print("\nğŸ§ª æµ‹è¯•5: ç½‘é¡µæŠ“å–å™¨")
    
    try:
        from src.processors.web_crawler import WebCrawler
        
        crawler = WebCrawler()
        print("  âœ… WebCrawler å®ä¾‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•URLä¿®å¤åŠŸèƒ½
        test_urls = [
            ("python.org", "https://python.org"),
            ("https://python.org", "https://python.org"),
            ("docs.python.org/3", "https://docs.python.org/3")
        ]
        
        for input_url, expected in test_urls:
            fixed = crawler._fix_url(input_url)
            if fixed == expected:
                print(f"  âœ… URLä¿®å¤: {input_url} â†’ {fixed}")
            else:
                print(f"  âš ï¸ URLä¿®å¤: {input_url} â†’ {fixed} (æœŸæœ›: {expected})")
        
        return True
    except Exception as e:
        print(f"âŒ ç½‘é¡µæŠ“å–å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ ç½‘é¡µæŠ“å–åˆ°çŸ¥è¯†åº“åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    tests = [
        test_imports,
        test_directory_structure,
        test_kb_naming,
        test_preset_sites,
        test_web_crawler
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
            print()
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}\n")
    
    print("=" * 50)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŠŸèƒ½å¯ä»¥æ­£å¸¸ä½¿ç”¨")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œ python demo_web_to_kb.py æŸ¥çœ‹æ¼”ç¤º")
        print("2. æŒ‰ç…§ WEB_TO_KB_INTEGRATION.md é›†æˆåˆ°ä¸»åº”ç”¨")
        print("3. åœ¨Streamlitåº”ç”¨ä¸­æµ‹è¯•å®Œæ•´åŠŸèƒ½")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³ç»„ä»¶")
        print("ğŸ“– æŸ¥çœ‹ WEB_TO_KB_INTEGRATION.md äº†è§£è¯¦ç»†ä¿¡æ¯")

if __name__ == "__main__":
    main()
