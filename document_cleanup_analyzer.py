#!/usr/bin/env python3
"""
RAG Pro Max æ–‡æ¡£æ¸…ç†åˆ†æå™¨
è¯†åˆ«å¯æ¸…ç†çš„è¿‡ç¨‹æ–‡æ¡£å’Œéœ€è¦ä¿ç•™çš„æ ¸å¿ƒæ–‡æ¡£
"""

import os
from pathlib import Path
from datetime import datetime

class DocumentCleanupAnalyzer:
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        
    def analyze_documents(self):
        """åˆ†ææ‰€æœ‰æ–‡æ¡£ï¼Œåˆ†ç±»ä¸ºæ ¸å¿ƒæ–‡æ¡£å’Œè¿‡ç¨‹æ–‡æ¡£"""
        
        # æ°¸ä¹…ä¿æŠ¤åˆ—è¡¨ - è¿™äº›æ–‡æ¡£ç»å¯¹ä¸èƒ½åˆ é™¤
        PROTECTED_DOCS = [
            'post_development_sync_standard.md',
            'development_cleanup_standard.md', 
            'sync_audit_report',
            'documentation_maintenance_standard.md',
            'non_essential_push_standard.md',
            'readme.md',
            'changelog.md',
            'faq.md',
            'deployment.md',
            'user_manual.md',
            'api_documentation.md',
            'architecture.md',
            'development_standard.md',
            'single_feature_iteration_standard.md',
            'testing.md',
            'contributing.md'
        ]
        
        # è·å–æ‰€æœ‰markdownæ–‡æ¡£
        all_docs = list(self.project_root.rglob("*.md"))
        
        # åˆ†ç±»æ–‡æ¡£
        core_docs = []      # æ ¸å¿ƒåŠŸèƒ½æ–‡æ¡£ - å¿…é¡»ä¿ç•™
        process_docs = []   # è¿‡ç¨‹æ–‡æ¡£ - å¯ä»¥æ¸…ç†
        temp_docs = []      # ä¸´æ—¶æ–‡æ¡£ - åº”è¯¥æ¸…ç†
        
        for doc in all_docs:
            doc_name = doc.name.lower()
            doc_path = str(doc.relative_to(self.project_root))
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ä¿æŠ¤åˆ—è¡¨ä¸­
            is_protected = any(protected in doc_name for protected in PROTECTED_DOCS)
            
            if is_protected:
                core_docs.append({
                    "path": doc_path,
                    "name": doc.name,
                    "reason": "ğŸ”’ æ°¸ä¹…ä¿æŠ¤æ–‡æ¡£ - ç¦æ­¢åˆ é™¤",
                    "size_kb": doc.stat().st_size / 1024
                })
                continue
                core_docs.append({
                    "path": doc_path,
                    "name": doc.name,
                    "reason": "ç”¨æˆ·æˆ–å¼€å‘è€…æ ¸å¿ƒæ–‡æ¡£",
                    "size_kb": doc.stat().st_size / 1024
                })
            
            # ä¸´æ—¶å·¥ä½œè®¡åˆ’ - å¯ä»¥æ¸…ç†
            elif "work_plans/" in doc_path:
                temp_docs.append({
                    "path": doc_path,
                    "name": doc.name,
                    "reason": "ä¸´æ—¶å·¥ä½œè®¡åˆ’ï¼Œå·²å®Œæˆå¯åˆ é™¤",
                    "size_kb": doc.stat().st_size / 1024
                })
            
            # å…¶ä»–æ ‡å‡†æ–‡æ¡£ - è°¨æ…å¤„ç†
            elif any(keyword in doc_name for keyword in [
                'standard', 'cleanup', 'sync', 'audit', 'maintenance',
                'push', 'development', 'logging', 'notification',
                'enterprise', 'kanban', 'template', 'sop'
            ]):
                # åªæœ‰æ˜ç¡®å¯åˆ é™¤çš„æ‰æ”¾å…¥process_docs
                if any(deletable in doc_name for deletable in [
                    'non_essential_push', 'logging_notification', 
                    'enterprise_document', 'kanban_template'
                ]):
                    process_docs.append({
                        "path": doc_path,
                        "name": doc.name,
                        "reason": "éæ ¸å¿ƒæ ‡å‡†æ–‡æ¡£ï¼Œå¯åˆ é™¤",
                        "size_kb": doc.stat().st_size / 1024
                    })
                else:
                    # å…¶ä»–æ ‡å‡†æ–‡æ¡£é»˜è®¤ä¿ç•™
                    core_docs.append({
                        "path": doc_path,
                        "name": doc.name,
                        "reason": "é‡è¦æ ‡å‡†æ–‡æ¡£ - ä¿ç•™",
                        "size_kb": doc.stat().st_size / 1024
                    })
            
            # å…¶ä»–æ–‡æ¡£
            else:
                # æ£€æŸ¥å†…å®¹åˆ¤æ–­é‡è¦æ€§
                try:
                    content = doc.read_text(encoding='utf-8')
                    if len(content) < 1000:  # å°æ–‡æ¡£å¯èƒ½æ˜¯ä¸´æ—¶çš„
                        temp_docs.append({
                            "path": doc_path,
                            "name": doc.name,
                            "reason": "å†…å®¹è¾ƒå°‘ï¼Œå¯èƒ½æ˜¯ä¸´æ—¶æ–‡æ¡£",
                            "size_kb": doc.stat().st_size / 1024
                        })
                    else:
                        core_docs.append({
                            "path": doc_path,
                            "name": doc.name,
                            "reason": "å†…å®¹ä¸°å¯Œï¼Œæš‚æ—¶ä¿ç•™",
                            "size_kb": doc.stat().st_size / 1024
                        })
                except:
                    process_docs.append({
                        "path": doc_path,
                        "name": doc.name,
                        "reason": "æ— æ³•è¯»å–ï¼Œå»ºè®®æ£€æŸ¥",
                        "size_kb": doc.stat().st_size / 1024
                    })
        
        return {
            "core_docs": core_docs,
            "process_docs": process_docs,
            "temp_docs": temp_docs,
            "total_docs": len(all_docs)
        }
    
    def generate_cleanup_plan(self, analysis):
        """ç”Ÿæˆæ¸…ç†è®¡åˆ’"""
        
        # è®¡ç®—å¯èŠ‚çœçš„ç©ºé—´
        process_size = sum(doc["size_kb"] for doc in analysis["process_docs"])
        temp_size = sum(doc["size_kb"] for doc in analysis["temp_docs"])
        total_cleanup_size = process_size + temp_size
        
        cleanup_plan = f"""# RAG Pro Max æ–‡æ¡£æ¸…ç†è®¡åˆ’

## ğŸ“Š åˆ†æç»“æœ

- **æ€»æ–‡æ¡£æ•°**: {analysis['total_docs']} ä¸ª
- **æ ¸å¿ƒæ–‡æ¡£**: {len(analysis['core_docs'])} ä¸ª (å¿…é¡»ä¿ç•™)
- **è¿‡ç¨‹æ–‡æ¡£**: {len(analysis['process_docs'])} ä¸ª (å¯ä»¥æ¸…ç†)
- **ä¸´æ—¶æ–‡æ¡£**: {len(analysis['temp_docs'])} ä¸ª (åº”è¯¥æ¸…ç†)
- **å¯èŠ‚çœç©ºé—´**: {total_cleanup_size:.1f} KB

---

## âœ… æ ¸å¿ƒæ–‡æ¡£ (ä¿ç•™)

è¿™äº›æ–‡æ¡£å¯¹é¡¹ç›®åŠŸèƒ½å’Œç”¨æˆ·ä½¿ç”¨è‡³å…³é‡è¦ï¼š

"""
        
        for doc in sorted(analysis["core_docs"], key=lambda x: x["name"]):
            cleanup_plan += f"- **{doc['name']}** - {doc['reason']} ({doc['size_kb']:.1f}KB)\n"
        
        cleanup_plan += f"""

---

## ğŸ—‘ï¸ å»ºè®®æ¸…ç†çš„æ–‡æ¡£

### è¿‡ç¨‹æ–‡æ¡£ ({len(analysis['process_docs'])} ä¸ª)
è¿™äº›æ˜¯å¼€å‘è¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ ‡å‡†æ–‡æ¡£ï¼Œå¯ä»¥æ•´åˆæˆ–åˆ é™¤ï¼š

"""
        
        for doc in sorted(analysis["process_docs"], key=lambda x: x["size_kb"], reverse=True):
            cleanup_plan += f"- `{doc['path']}` - {doc['reason']} ({doc['size_kb']:.1f}KB)\n"
        
        cleanup_plan += f"""

### ä¸´æ—¶æ–‡æ¡£ ({len(analysis['temp_docs'])} ä¸ª)
è¿™äº›æ˜¯ä¸´æ—¶ç”Ÿæˆçš„å·¥ä½œæ–‡æ¡£ï¼Œå®Œæˆåå¯ä»¥åˆ é™¤ï¼š

"""
        
        for doc in sorted(analysis["temp_docs"], key=lambda x: x["size_kb"], reverse=True):
            cleanup_plan += f"- `{doc['path']}` - {doc['reason']} ({doc['size_kb']:.1f}KB)\n"
        
        cleanup_plan += f"""

---

## ğŸ¯ æ¸…ç†å»ºè®®

### ç«‹å³åˆ é™¤
```bash
# åˆ é™¤ä¸´æ—¶å·¥ä½œè®¡åˆ’
rm -rf work_plans/

# åˆ é™¤è¿‡ç¨‹æ ‡å‡†æ–‡æ¡£
rm DEVELOPMENT_CLEANUP_STANDARD.md
rm POST_DEVELOPMENT_SYNC_STANDARD.md
rm DOCUMENTATION_MAINTENANCE_STANDARD.md
rm NON_ESSENTIAL_PUSH_STANDARD.md
rm LOGGING_AND_NOTIFICATION_STANDARD.md
rm ENTERPRISE_DOCUMENT_MANAGEMENT_STANDARD.md
rm CONTINUOUS_QUALITY_SOP.md
rm PROJECT_KANBAN_TEMPLATE.md
rm SYNC_AUDIT_REPORT_v3.2.2.md
```

### æ•´åˆå»ºè®®
1. **å¼€å‘è§„èŒƒæ•´åˆ**: å°†å¤šä¸ªSTANDARDæ–‡æ¡£æ•´åˆåˆ°`DEVELOPMENT_STANDARD.md`
2. **æ¥å£æ–‡æ¡£æ•´åˆ**: å°†`INTERFACE_SUMMARY.md`å’Œ`INTERNAL_API.md`æ•´åˆåˆ°`API_DOCUMENTATION.md`
3. **æŒ‡å—æ–‡æ¡£æ•´åˆ**: å°†`CONTINUOUS_OPTIMIZATION_GUIDE.md`æ•´åˆåˆ°`README.md`

### ä¿ç•™çš„æ ¸å¿ƒæ–‡æ¡£ç»“æ„
```
docs/
â”œâ”€â”€ README.md              # é¡¹ç›®ä»‹ç»å’Œå¿«é€Ÿå¼€å§‹
â”œâ”€â”€ CHANGELOG.md           # ç‰ˆæœ¬æ›´æ–°è®°å½•
â”œâ”€â”€ FAQ.md                 # å¸¸è§é—®é¢˜
â”œâ”€â”€ USER_MANUAL.md         # ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ
â”œâ”€â”€ DEPLOYMENT.md          # éƒ¨ç½²æŒ‡å—
â”œâ”€â”€ API_DOCUMENTATION.md   # APIæ–‡æ¡£
â”œâ”€â”€ ARCHITECTURE.md        # æ¶æ„è¯´æ˜
â”œâ”€â”€ DEVELOPMENT_STANDARD.md # å¼€å‘è§„èŒƒ
â”œâ”€â”€ SINGLE_FEATURE_ITERATION_STANDARD.md # è¿­ä»£è§„èŒƒ
â”œâ”€â”€ TESTING.md             # æµ‹è¯•æŒ‡å—
â”œâ”€â”€ CONTRIBUTING.md        # è´¡çŒ®æŒ‡å—
â””â”€â”€ FIRST_TIME_GUIDE.md    # æ–°æ‰‹æŒ‡å—
```

---

## ğŸ“ˆ æ¸…ç†æ•ˆæœ

- **æ–‡æ¡£æ•°é‡**: {analysis['total_docs']} â†’ {len(analysis['core_docs'])} (-{len(analysis['process_docs']) + len(analysis['temp_docs'])})
- **èŠ‚çœç©ºé—´**: {total_cleanup_size:.1f} KB
- **ç»´æŠ¤æˆæœ¬**: å¤§å¹…é™ä½
- **æ–‡æ¡£è´¨é‡**: æ›´åŠ èšç„¦æ ¸å¿ƒåŠŸèƒ½

æ¸…ç†åé¡¹ç›®æ–‡æ¡£å°†æ›´åŠ ç®€æ´ã€èšç„¦ï¼Œä¾¿äºç”¨æˆ·å’Œå¼€å‘è€…ä½¿ç”¨ã€‚
"""
        
        return cleanup_plan
    
    def execute_cleanup(self, analysis, confirm=False):
        """æ‰§è¡Œæ–‡æ¡£æ¸…ç†"""
        if not confirm:
            print("âš ï¸ è¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¼šå®é™…åˆ é™¤æ–‡ä»¶")
            print("å¦‚éœ€æ‰§è¡Œæ¸…ç†ï¼Œè¯·è®¾ç½® confirm=True")
            return
        
        deleted_count = 0
        
        # åˆ é™¤ä¸´æ—¶æ–‡æ¡£
        for doc in analysis["temp_docs"]:
            doc_path = self.project_root / doc["path"]
            if doc_path.exists():
                doc_path.unlink()
                deleted_count += 1
                print(f"âœ… å·²åˆ é™¤: {doc['path']}")
        
        # åˆ é™¤è¿‡ç¨‹æ–‡æ¡£ï¼ˆè°¨æ…ï¼‰
        for doc in analysis["process_docs"]:
            if any(keyword in doc["name"].lower() for keyword in [
                "cleanup", "sync", "audit", "maintenance", "push"
            ]):
                doc_path = self.project_root / doc["path"]
                if doc_path.exists():
                    doc_path.unlink()
                    deleted_count += 1
                    print(f"âœ… å·²åˆ é™¤: {doc['path']}")
        
        print(f"\nğŸ‰ æ¸…ç†å®Œæˆï¼å…±åˆ é™¤ {deleted_count} ä¸ªæ–‡æ¡£")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    project_root = sys.argv[1] if len(sys.argv) > 1 else os.getcwd()
    
    analyzer = DocumentCleanupAnalyzer(project_root)
    
    print("ğŸ” åˆ†æé¡¹ç›®æ–‡æ¡£...")
    analysis = analyzer.analyze_documents()
    
    print("ğŸ“‹ ç”Ÿæˆæ¸…ç†è®¡åˆ’...")
    cleanup_plan = analyzer.generate_cleanup_plan(analysis)
    
    # ä¿å­˜æ¸…ç†è®¡åˆ’
    plan_file = Path(project_root) / f"DOCUMENT_CLEANUP_PLAN_{datetime.now().strftime('%Y%m%d')}.md"
    plan_file.write_text(cleanup_plan, encoding='utf-8')
    
    print(f"ğŸ“„ æ¸…ç†è®¡åˆ’å·²ä¿å­˜: {plan_file}")
    
    # è¾“å‡ºæ‘˜è¦
    print(f"\nğŸ“Š æ–‡æ¡£åˆ†ææ‘˜è¦:")
    print(f"æ€»æ–‡æ¡£: {analysis['total_docs']} ä¸ª")
    print(f"æ ¸å¿ƒæ–‡æ¡£: {len(analysis['core_docs'])} ä¸ª (ä¿ç•™)")
    print(f"è¿‡ç¨‹æ–‡æ¡£: {len(analysis['process_docs'])} ä¸ª (å¯æ¸…ç†)")
    print(f"ä¸´æ—¶æ–‡æ¡£: {len(analysis['temp_docs'])} ä¸ª (åº”æ¸…ç†)")
    
    # è¯¢é—®æ˜¯å¦æ‰§è¡Œæ¸…ç†
    print(f"\næ˜¯å¦è¦æ‰§è¡Œæ¸…ç†ï¼Ÿ(è¾“å…¥ 'yes' ç¡®è®¤)")
    user_input = input().strip().lower()
    
    if user_input == 'yes':
        analyzer.execute_cleanup(analysis, confirm=True)
    else:
        print("æ¸…ç†å·²å–æ¶ˆï¼Œè¯·æŸ¥çœ‹æ¸…ç†è®¡åˆ’æ–‡ä»¶")

if __name__ == "__main__":
    main()
