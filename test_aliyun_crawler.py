#!/usr/bin/env python3
"""
é˜¿é‡Œäº‘æ–‡æ¡£çˆ¬è™«æµ‹è¯• - éªŒè¯é“¾æ¥æå–æ•ˆæœ
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_aliyun_crawler():
    """æµ‹è¯•é˜¿é‡Œäº‘æ–‡æ¡£çˆ¬è™«"""
    print("ğŸ§ª æµ‹è¯•é˜¿é‡Œäº‘æ–‡æ¡£çˆ¬è™«...")
    
    try:
        from src.processors.web_crawler import WebCrawler
        import tempfile
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            crawler = WebCrawler(temp_dir)
            
            # æµ‹è¯•URL
            test_url = "https://help.aliyun.com/"
            
            print(f"ğŸ“‚ å¼€å§‹æµ‹è¯•çˆ¬å–: {test_url}")
            print("âš™ï¸ è®¾ç½®: é€’å½’æ·±åº¦2, æ¯å±‚50é¡µ")
            
            def status_callback(msg):
                print(f"  {msg}")
            
            # æ‰§è¡Œçˆ¬å–
            saved_files = crawler.crawl_advanced(
                start_url=test_url,
                max_depth=2,
                max_pages=50,  # å¢åŠ æ¯å±‚é¡µæ•°
                exclude_patterns=[],
                parser_type="documentation",
                status_callback=status_callback
            )
            
            print(f"\nğŸ“Š çˆ¬å–ç»“æœ:")
            print(f"  æ€»é¡µé¢æ•°: {len(saved_files)}")
            
            # åˆ†æä¿å­˜çš„æ–‡ä»¶
            if saved_files:
                print(f"\nğŸ“„ ä¿å­˜çš„æ–‡ä»¶:")
                for i, file_path in enumerate(saved_files[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
                    file_name = os.path.basename(file_path)
                    file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
                    print(f"  {i+1}. {file_name} ({file_size} bytes)")
                
                if len(saved_files) > 10:
                    print(f"  ... è¿˜æœ‰ {len(saved_files) - 10} ä¸ªæ–‡ä»¶")
            
            return len(saved_files)
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0

def analyze_link_extraction():
    """åˆ†æé“¾æ¥æå–æ•ˆæœ"""
    print("\nğŸ” åˆ†æé“¾æ¥æå–æ•ˆæœ...")
    
    try:
        import requests
        from bs4 import BeautifulSoup
        from src.processors.web_crawler import WebCrawler
        import tempfile
        
        # è·å–é˜¿é‡Œäº‘é¦–é¡µ
        url = "https://help.aliyun.com/"
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        with tempfile.TemporaryDirectory() as temp_dir:
            crawler = WebCrawler(temp_dir)
            
            # æå–é“¾æ¥
            links = crawler._extract_links(soup, url)
            
            print(f"ğŸ“Š é“¾æ¥æå–ç»Ÿè®¡:")
            print(f"  å‘ç°é“¾æ¥æ•°: {len(links)}")
            
            # åˆ†æé“¾æ¥ç±»å‹
            doc_links = [l for l in links if any(k in l.lower() for k in ['doc', 'help', 'guide'])]
            product_links = [l for l in links if any(k in l.lower() for k in ['product', 'service'])]
            
            print(f"  æ–‡æ¡£ç±»é“¾æ¥: {len(doc_links)}")
            print(f"  äº§å“ç±»é“¾æ¥: {len(product_links)}")
            
            # æ˜¾ç¤ºå‰10ä¸ªé“¾æ¥
            print(f"\nğŸ”— å‰10ä¸ªé“¾æ¥:")
            for i, link in enumerate(links[:10]):
                print(f"  {i+1}. {link}")
            
            return len(links)
            
    except Exception as e:
        print(f"âŒ é“¾æ¥åˆ†æå¤±è´¥: {e}")
        return 0

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("  é˜¿é‡Œäº‘æ–‡æ¡£çˆ¬è™«æµ‹è¯•")
    print("=" * 60)
    
    # åˆ†æé“¾æ¥æå–
    link_count = analyze_link_extraction()
    
    # æµ‹è¯•å®é™…çˆ¬å–
    if link_count > 0:
        print(f"\nâœ… é“¾æ¥æå–æ­£å¸¸ï¼Œå‘ç° {link_count} ä¸ªé“¾æ¥")
        
        # è¯¢é—®æ˜¯å¦è¿›è¡Œå®é™…çˆ¬å–æµ‹è¯•
        print("\nğŸ¤” æ˜¯å¦è¿›è¡Œå®é™…çˆ¬å–æµ‹è¯•ï¼Ÿ(è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)")
        print("   è¾“å…¥ 'y' ç»§ç»­ï¼Œå…¶ä»–é”®è·³è¿‡...")
        
        # ç”±äºè¿™æ˜¯è‡ªåŠ¨åŒ–è„šæœ¬ï¼Œæˆ‘ä»¬ç›´æ¥è¿›è¡Œå°è§„æ¨¡æµ‹è¯•
        print("ğŸš€ è¿›è¡Œå°è§„æ¨¡çˆ¬å–æµ‹è¯•...")
        page_count = test_aliyun_crawler()
        
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"  å‘ç°é“¾æ¥: {link_count} ä¸ª")
        print(f"  æˆåŠŸçˆ¬å–: {page_count} é¡µ")
        
        if page_count < 10:
            print("\nâš ï¸  çˆ¬å–é¡µé¢è¾ƒå°‘ï¼Œå¯èƒ½çš„åŸå› :")
            print("  1. ç½‘ç«™æœ‰åçˆ¬æœºåˆ¶")
            print("  2. é“¾æ¥è¿‡æ»¤å¤ªä¸¥æ ¼")
            print("  3. ç½‘ç»œè¿æ¥é—®é¢˜")
            print("  4. éœ€è¦å¢åŠ æ¯å±‚é¡µæ•°é™åˆ¶")
        else:
            print(f"\nâœ… çˆ¬å–æ•ˆæœè‰¯å¥½ï¼")
    else:
        print("âŒ é“¾æ¥æå–å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œçˆ¬å–æµ‹è¯•")

if __name__ == "__main__":
    main()
