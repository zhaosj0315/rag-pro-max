#!/usr/bin/env python3
"""
æµ‹è¯•å…¨å±€LLMè®¾ç½®
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.utils.model_manager import set_global_llm_model
from llama_index.core import Settings

def test_global_llm():
    """æµ‹è¯•å…¨å±€LLMè®¾ç½®"""
    print("ğŸ” æµ‹è¯•å…¨å±€LLMè®¾ç½®...")
    
    # è®¾ç½®å…¨å±€LLM
    success = set_global_llm_model(
        provider="Ollama",
        model_name="gpt-oss:20b",
        api_url="http://localhost:11434"
    )
    
    if not success:
        print("âŒ å…¨å±€LLMè®¾ç½®å¤±è´¥")
        return False
    
    print("âœ… å…¨å±€LLMè®¾ç½®æˆåŠŸ")
    
    # æµ‹è¯•LLMè°ƒç”¨
    try:
        print("ğŸ”„ æµ‹è¯•å…¨å±€LLMè°ƒç”¨...")
        if Settings.llm:
            response = Settings.llm.complete("Hello")
            print(f"âœ… å…¨å±€LLMè°ƒç”¨æˆåŠŸ: {response.text[:50]}...")
            return True
        else:
            print("âŒ Settings.llmä¸ºç©º")
            return False
    except Exception as e:
        print(f"âŒ å…¨å±€LLMè°ƒç”¨å¤±è´¥: {str(e)}")
        return False

if __name__ == "__main__":
    success = test_global_llm()
    sys.exit(0 if success else 1)
