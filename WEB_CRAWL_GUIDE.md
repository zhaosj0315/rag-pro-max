# 🌐 网页抓取功能使用指南 (v2.3.1)

## 功能概述

RAG Pro Max v2.3.1 内置了强大的网页抓取功能，可以自动抓取网页内容并添加到知识库中。支持智能URL修复和自动格式转换。

## 🚀 快速开始

### 1. 访问网页抓取功能

1. 启动 RAG Pro Max 应用：`streamlit run src/apppro.py`
2. 在主界面找到 **"🌐 网页抓取"** 标签页
3. 点击切换到网页抓取模式

### 2. 输入网址（v2.3.1 优化）

**支持的URL格式：**
- ✅ `python.org` → 自动转换为 `https://python.org`
- ✅ `www.python.org` → 自动转换为 `https://www.python.org`
- ✅ `https://python.org` → 保持不变
- ✅ `http://example.com` → 保持不变
- ✅ `github.com/user/repo` → 自动转换为 `https://github.com/user/repo`

**🎯 新特性**: 系统会自动修复URL格式，无需手动添加 `https://` 前缀！

### 3. 配置抓取参数

- **深度 (1-3)**：抓取层级深度
  - `1`：仅抓取当前页面
  - `2`：抓取当前页面 + 直接链接页面  
  - `3`：抓取当前页面 + 2级链接页面

- **页数 (1-50)**：最大抓取页面数量限制

### 4. 开始抓取

点击 **"🕷️ 开始抓取"** 按钮，系统将：
1. 🔧 自动修复URL格式
2. ✅ 验证URL有效性
3. 🕷️ 开始抓取网页内容
4. 📊 显示实时进度
5. 💾 保存抓取结果
6. 🔄 自动添加到知识库

## 📋 抓取结果

### 文件格式

抓取的内容保存为 `.txt` 文件，包含完整元数据：

```
URL: https://python.org
Title: Welcome to Python.org
Crawl Time: 2025-12-13 08:37:17

[网页正文内容]
```

### 文件命名规则

文件名格式：`{页面标题}_{URL哈希}.txt`

示例：`Welcome_to_Python_org_b020aa1e.txt`

### 存储位置

- **临时存储**: `temp_uploads/web_crawl/`
- **最终存储**: 自动添加到当前选中的知识库

## 🔧 技术特性

### 智能URL处理 (v2.3.1 新增)
- 🔧 自动添加 `https://` 协议前缀
- ✅ URL格式验证和修复
- 💬 友好的错误提示信息
- 🎯 支持简化输入（如直接输入域名）

### 内容提取算法
- 🧹 自动移除导航、页脚、脚本等无关内容
- 📝 智能提取主要文本内容
- 📋 保留页面标题和元数据
- 🔍 过滤内容过少的页面（<50字符）

### 礼貌爬取机制
- ⏱️ 请求间隔 0.5 秒
- 🌐 标准浏览器 User-Agent
- 🔒 同域名限制（防止跨站抓取）
- ⏰ 超时保护 (10秒)

### 安全限制
- 📊 最大页面数量限制 (50页)
- 📏 最大抓取深度限制 (3层)
- 🛡️ 内容质量过滤
- 🚫 恶意网站检测

## 📊 使用示例

### 示例1：抓取Python官网

```
输入URL: python.org
深度: 1
页数: 1
```

**结果**: 
- 自动转换为 `https://python.org`
- 抓取Python官网首页
- 提取6500+字符的内容
- 保存为 `Welcome_to_Python_org_xxx.txt`

### 示例2：抓取GitHub项目

```
输入URL: github.com/microsoft/vscode
深度: 2  
页数: 5
```

**结果**:
- 自动转换为 `https://github.com/microsoft/vscode`
- 抓取项目主页 + 相关页面
- 获取项目文档和说明

### 示例3：抓取技术博客

```
输入URL: blog.example.com
深度: 2
页数: 10
```

**结果**:
- 抓取博客首页和文章列表
- 获取多篇技术文章内容
- 构建完整的技术知识库

## ❗ 常见问题与解决方案

### Q: 输入 `python.org` 还是提示 "Invalid URL" 错误？

**A**: 这是旧版本: v2.3.1已完全修复：
- ✅ 现在支持直接输入域名
- ✅ 自动添加 `https://` 前缀
- ✅ 智能URL格式修复

### Q: 抓取失败怎么办？

**A**: 按以下步骤排查：
1. 🌐 检查网络连接是否正常
2. 🔍 确认目标网站是否可访问
3. 🔧 尝试完整URL格式
4. 🚫 检查是否被反爬虫机制阻止

### Q: 抓取到的内容在哪里？

**A**: 抓取流程：
1. 📁 临时保存到 `temp_uploads/web_crawl/`
2. 🔄 自动添加到当前知识库
3. 💬 可以直接在对话中搜索和问答
4. 📊 在知识库管理中查看文档列表

### Q: 如何查看抓取到的具体内容？

**A**: 多种查看方式：
- 📋 抓取完成后显示文件信息
- 📁 直接查看临时文件夹
- 💬 在知识库中搜索相关内容
- 🔍 使用文档预览功能

### Q: 抓取速度慢怎么办？

**A**: 优化建议：
- 🎯 减少抓取深度和页数
- 🌐 选择网络状况好的时间
- 🔧 检查目标网站响应速度
- ⚡ 使用有线网络连接

## 🎯 最佳实践

### 1. 选择合适的抓取策略

**单页面深度抓取**:
- 深度: 1, 页数: 1
- 适合: 详细文档、长文章

**主题相关抓取**:
- 深度: 2, 页数: 5-10  
- 适合: 产品文档、技术博客

**完整站点抓取**:
- 深度: 3, 页数: 20-50
- 适合: 小型网站、项目文档

### 2. URL输入技巧

```
✅ 推荐输入方式:
python.org
github.com/user/repo
docs.python.org/3/

❌ 避免的输入:
localhost (本地地址)
file:// (本地文件)
javascript: (脚本链接)
```

### 3. 内容质量控制

- 🎯 选择内容丰富的页面
- 🚫 避免纯导航页面
- 📝 优先选择文档类网站
- 🔍 验证抓取结果质量

### 4. 知识库管理

- 📚 为不同主题创建专门知识库
- 🏷️ 使用有意义的知识库名称
- 🔄 定期更新网页内容
- 🧹 清理无用的抓取结果

## 🔄 更新日志

### v2.3.1 (2025-12-13) - 智能URL处理
- ✅ **智能URL修复**: 自动添加协议前缀
- ✅ **简化输入**: 支持直接输入域名
- ✅ **错误提示优化**: 更友好的错误信息
- ✅ **内容过滤增强**: 过滤低质量内容
- ✅ **界面优化**: 更舒适的布局和交互

### v2.3.1 (2025-12-10) - 首次发布
- ✅ 网页抓取核心功能
- ✅ 递归抓取支持
- ✅ 智能内容提取
- ✅ 实时进度显示
- ✅ 知识库集成

## 🛠️ 技术架构

### 处理流程

```
用户输入URL → URL修复 → 格式验证 → 网页抓取 → 内容提取 → 文件保存 → 知识库集成
```

### 核心组件

- **WebCrawler**: 网页抓取引擎
- **BeautifulSoup**: HTML解析器  
- **Requests**: HTTP客户端
- **文件处理器**: 统一后端处理
- **向量化引擎**: 知识库集成

---

**💡 提示**: 网页抓取功能完全可用且持续优化中。如遇问题请检查网络连接和URL格式，或查看 [FAQ文档](./FAQ.md)。
