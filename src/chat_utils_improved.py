"""
æ”¹è¿›çš„å¯¹è¯å¤„ç†å·¥å…· - ç¨³å®šæ€§ä¼˜å…ˆ
- å®‰å…¨çš„æµå¼è¾“å‡º
- å¼•ç”¨æºéªŒè¯
- è¿½é—®ç”Ÿæˆå®¹é”™
- å¯¹è¯å†å²ä¿æŠ¤
"""

import os
import json
import shutil
import threading
import time
import re
from datetime import datetime
from pathlib import Path
from collections import Counter
from llama_index.core import Settings
import re

HISTORY_DIR = "chat_histories"
Path(HISTORY_DIR).mkdir(parents=True, exist_ok=True)


def save_chat_history_safe(kb_id, messages, logger=None):
    """
    å®‰å…¨ä¿å­˜å¯¹è¯å†å²
    - éªŒè¯æ•°æ®å®Œæ•´æ€§
    - åŸå­æ“ä½œ
    - è‡ªåŠ¨å¤‡ä»½
    """
    path = os.path.join(HISTORY_DIR, f"{kb_id}.json")
    
    # éªŒè¯æ•°æ®æ ¼å¼
    if not isinstance(messages, list):
        if logger:
            logger.log_error("å¯¹è¯ä¿å­˜", "æ•°æ®æ ¼å¼é”™è¯¯", {"kb_id": kb_id})
        return False
    
    try:
        # éªŒè¯æ¯æ¡æ¶ˆæ¯çš„æ ¼å¼
        for msg in messages:
            if not isinstance(msg, dict) or "role" not in msg or "content" not in msg:
                if logger:
                    logger.log_error("å¯¹è¯ä¿å­˜", "æ¶ˆæ¯æ ¼å¼é”™è¯¯", {"kb_id": kb_id})
                return False
        
        # å…ˆå†™ä¸´æ—¶æ–‡ä»¶
        temp_path = path + ".tmp"
        with open(temp_path, 'w', encoding='utf-8') as f:
            json.dump(messages, f, indent=2, ensure_ascii=False)
        
        # éªŒè¯ä¸´æ—¶æ–‡ä»¶å¯è¯»
        with open(temp_path, 'r', encoding='utf-8') as f:
            json.load(f)
        
        # å¤‡ä»½åŸæ–‡ä»¶
        if os.path.exists(path):
            backup_path = path + ".bak"
            shutil.copy2(path, backup_path)
        
        # åŸå­æ“ä½œï¼šæ›¿æ¢
        shutil.move(temp_path, path)
        
        if logger:
            logger.log("å¯¹è¯ä¿å­˜", "success", f"âœ… å¯¹è¯å†å²å·²ä¿å­˜: {kb_id}", {"kb_id": kb_id})
        
        return True
        
    except Exception as e:
        if logger:
            logger.log_error("å¯¹è¯ä¿å­˜", str(e), {"kb_id": kb_id})
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass
        
        return False


def load_chat_history_safe(kb_id, logger=None):
    """
    å®‰å…¨åŠ è½½å¯¹è¯å†å²
    - è‡ªåŠ¨æ¢å¤æŸåçš„æ–‡ä»¶
    - éªŒè¯æ•°æ®å®Œæ•´æ€§
    """
    path = os.path.join(HISTORY_DIR, f"{kb_id}.json")
    
    if not os.path.exists(path):
        return []
    
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            return data
        else:
            if logger:
                logger.log_error("å¯¹è¯åŠ è½½", "æ•°æ®æ ¼å¼é”™è¯¯", {"kb_id": kb_id})
            return []
    
    except json.JSONDecodeError:
        # å°è¯•ä»å¤‡ä»½æ¢å¤
        backup_path = path + ".bak"
        if os.path.exists(backup_path):
            try:
                with open(backup_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if logger:
                    logger.log("å¯¹è¯åŠ è½½", "warning", f"âš ï¸ ä»å¤‡ä»½æ¢å¤å¯¹è¯å†å²: {kb_id}", {"kb_id": kb_id})
                return data if isinstance(data, list) else []
            except:
                pass
        
        if logger:
            logger.log_error("å¯¹è¯åŠ è½½", "æ–‡ä»¶æŸåä¸”æ— å¤‡ä»½", {"kb_id": kb_id})
        return []
    
    except Exception as e:
        if logger:
            logger.log_error("å¯¹è¯åŠ è½½", str(e), {"kb_id": kb_id})
        return []


def stream_response_safe(chat_engine, prompt, max_retries=2, logger=None):
    """
    å®‰å…¨çš„æµå¼å“åº”
    - è‡ªåŠ¨é‡è¯•
    - å®Œæ•´æ€§æ£€æŸ¥
    - è¶…æ—¶æ§åˆ¶
    """
    for attempt in range(max_retries):
        try:
            full_text = ""
            response = chat_engine.stream_chat(prompt)
            
            # æµå¼è¾“å‡º
            for token in response.response_gen:
                if token:  # è¿‡æ»¤ç©ºtoken
                    full_text += token
                    yield token
            
            # éªŒè¯å“åº”å®Œæ•´æ€§
            if not full_text.strip():
                if logger:
                    logger.log_error("æµå¼è¾“å‡º", "å“åº”ä¸ºç©º", {"attempt": attempt + 1})
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                raise ValueError("å“åº”ä¸ºç©º")
            
            # è¿”å›å®Œæ•´å“åº”å¯¹è±¡
            return response, full_text
            
        except Exception as e:
            if logger:
                logger.log_error("æµå¼è¾“å‡º", str(e), {"attempt": attempt + 1})
            
            if attempt < max_retries - 1:
                time.sleep(1)  # ç­‰å¾…åé‡è¯•
                continue
            
            raise


def extract_sources_safe(response, min_score=0.3, logger=None):
    """
    å®‰å…¨æå–å¼•ç”¨æº
    - éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
    - è¿‡æ»¤ä½è´¨é‡æº
    - é˜²æ­¢æ˜¾ç¤ºæ— æ•ˆå†…å®¹
    """
    sources = []
    
    if not hasattr(response, 'source_nodes') or not response.source_nodes:
        return sources
    
    for node in response.source_nodes:
        try:
            # éªŒè¯å¿…è¦å­—æ®µ
            file_name = node.metadata.get('file_name', 'Unknown') if hasattr(node, 'metadata') else 'Unknown'
            text = node.text if hasattr(node, 'text') else ""
            score = float(node.score or 0.0) if hasattr(node, 'score') else 0.0
            
            # è¿‡æ»¤ä½è´¨é‡æº
            if score < min_score:
                continue
            
            # æˆªæ–­è¿‡é•¿æ–‡æœ¬
            text_preview = text[:200].replace("\n", " ").strip()
            if not text_preview:
                continue
            
            sources.append({
                "file": str(file_name)[:100],  # é˜²æ­¢è¿‡é•¿
                "score": round(score, 3),
                "text": text_preview + "..."
            })
        
        except Exception as e:
            if logger:
                logger.log_error("æºæå–", str(e), {"node": str(node)[:100]})
            continue
    
    return sources


def _extract_keywords(text, max_keywords=5):
    """æå–æ–‡æœ¬å…³é”®è¯"""
    try:
        import jieba
        # ä½¿ç”¨ jieba åˆ†è¯
        words = jieba.lcut(text)
    except:
        # é™çº§ï¼šç®€å•åˆ†è¯
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.split()
    
    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ç­‰', 'åŠ', 'ä»¥', 'ä¸º', 'è¿™', 'é‚£', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'ä¸ª', 'ä¸­', 'ä¹Ÿ', 'éƒ½', 'å°±', 'è€Œ', 'è¦', 'ä¼š', 'å¯ä»¥', 'èƒ½', 'è¯´', 'å¯¹', 'ä½†', 'ä¸', 'æ²¡æœ‰'}
    keywords = [w for w in words if len(w) > 1 and w not in stop_words]
    
    # ç»Ÿè®¡è¯é¢‘
    word_freq = Counter(keywords)
    # è¿”å›é«˜é¢‘è¯
    return [word for word, _ in word_freq.most_common(max_keywords)]


def generate_follow_up_questions_safe(context_text, num_questions=3, existing_questions=None, timeout=10, logger=None, query_engine=None):
    """
    å®‰å…¨çš„è¿½é—®ç”Ÿæˆï¼ˆå¸¦é™çº§ç­–ç•¥ï¼‰
    - åŒ…å«é™çº§é€»è¾‘
    - è¶…æ—¶æ§åˆ¶
    - çº¿ç¨‹éš”ç¦»
    - çŸ¥è¯†åº“å†…å®¹éªŒè¯ï¼ˆæ–°å¢ï¼‰
    """
    result = {"questions": []}
    
    # é™çº§é—®é¢˜åº“
    fallback_questions = [
        "èƒ½å¦è¯¦ç»†è§£é‡Šä¸€ä¸‹è¿™ä¸ªæ¦‚å¿µï¼Ÿ",
        "è¿™ä¸ªæ–¹æ¡ˆæœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ",
        "æœ‰æ²¡æœ‰ç›¸å…³çš„å®é™…æ¡ˆä¾‹ï¼Ÿ",
        "è¿™ä¸å¸¸è§åšæ³•æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "å¦‚ä½•å¤„ç†å…¶ä¸­å¯èƒ½å‡ºç°çš„é”™è¯¯ï¼Ÿ"
    ]
    
    # æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´é™çº§é—®é¢˜
    if "å¦‚ä½•" in context_text or "æ€ä¹ˆ" in context_text:
        fallback_questions.insert(0, "å…·ä½“çš„æ“ä½œæ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ")
    if "åŸå› " in context_text or "ä¸ºä»€ä¹ˆ" in context_text:
        fallback_questions.insert(0, "è¿˜æœ‰å…¶ä»–å¯èƒ½çš„åŸå› å—ï¼Ÿ")
    if "ä»£ç " in context_text or "Python" in context_text:
        fallback_questions.insert(0, "èƒ½å¦æä¾›æ›´è¯¦ç»†çš„ä»£ç ç¤ºä¾‹ï¼Ÿ")
        
    fallback = fallback_questions[:num_questions]

    def _generate():
        if not hasattr(Settings, 'llm') or not Settings.llm: 
            result["questions"] = fallback
            return

        try:
            # å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæé«˜é€Ÿåº¦
            short_context = context_text[-2000:] 
            
            # æ’é™¤å·²é—®è¿‡çš„é—®é¢˜
            existing_str = "\n".join(existing_questions) if existing_questions else ""
            
            # ğŸ†• å°è¯•ä»çŸ¥è¯†åº“è·å–ç›¸å…³ä¸»é¢˜
            kb_topics = ""
            if query_engine:
                try:
                    # æå–å…³é”®è¯æŸ¥è¯¢çŸ¥è¯†åº“
                    keywords = _extract_keywords(short_context)
                    if keywords:
                        kb_query = " ".join(keywords[:3])  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
                        kb_response = query_engine.query(kb_query)
                        if kb_response and hasattr(kb_response, 'source_nodes'):
                            # è·å–ç›¸å…³æ–‡æ¡£çš„æ ‡é¢˜æˆ–æ‘˜è¦
                            topics = []
                            for node in kb_response.source_nodes[:2]:  # åªå–å‰2ä¸ª
                                if hasattr(node, 'metadata') and 'file_name' in node.metadata:
                                    topics.append(node.metadata['file_name'])
                            if topics:
                                kb_topics = f"\nçŸ¥è¯†åº“ç›¸å…³ä¸»é¢˜ï¼š{', '.join(topics)}"
                except:
                    pass  # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
            
            prompt = (
                f"åŸºäºä»¥ä¸‹å›ç­”ï¼Œæå‡º {num_questions * 2} ä¸ªç®€çŸ­çš„è¿½é—®é—®é¢˜ã€‚\n"
                f"è¦æ±‚ï¼š\n1. åªéœ€è¦é—®é¢˜ï¼Œä¸è¦åºå·\n2. ç®€çŸ­ï¼ˆ15å­—ä»¥å†…ï¼‰\n3. æœ‰å¯å‘æ€§\n"
                f"4. ç»“åˆçŸ¥è¯†åº“å†…å®¹ï¼Œæå‡ºç”¨æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„ç›¸å…³é—®é¢˜\n"
                f"{'é¿å…ï¼š' + existing_str if existing_str else ''}\n"
                f"{kb_topics}\n\n"
                f"å†…å®¹ï¼š\n{short_context}"
            )
            
            resp = Settings.llm.complete(prompt)
            text = resp.text.strip()
            
            questions = [re.sub(r'^[\\d\\.\\-\\s]+', '', q).strip() for q in text.split('\n') if q.strip()]
            
            # éªŒè¯é—®é¢˜æ˜¯å¦èƒ½åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°å†…å®¹
            if query_engine and questions:
                valid_questions = []
                for q in questions[:num_questions * 2]:  # å¤šç”Ÿæˆä¸€äº›å¤‡é€‰
                    try:
                        # å¿«é€Ÿæ£€ç´¢éªŒè¯
                        retriever = query_engine.retriever
                        nodes = retriever.retrieve(q)
                        # æ£€æŸ¥æ˜¯å¦æœ‰é«˜ç›¸å…³åº¦çš„ç»“æœ
                        if nodes and len(nodes) > 0 and nodes[0].score > 0.3:
                            valid_questions.append(q)
                            if len(valid_questions) >= num_questions:
                                break
                    except:
                        continue
                
                if valid_questions:
                    result["questions"] = valid_questions[:num_questions]
                else:
                    result["questions"] = fallback
            elif not questions:
                result["questions"] = fallback
            else:
                result["questions"] = questions[:num_questions]
                
        except Exception as e:
            if logger:
                logger.log_error("è¿½é—®ç”Ÿæˆ", str(e))
            result["questions"] = fallback
    
    # ä½¿ç”¨çº¿ç¨‹æ‰§è¡Œå¹¶è®¾ç½®è¶…æ—¶
    thread = threading.Thread(target=_generate, daemon=True)
    thread.start()
    thread.join(timeout=timeout)
    
    if thread.is_alive():
        if logger:
            logger.log_error("è¿½é—®ç”Ÿæˆ", "è¶…æ—¶")
        return fallback
    
    return result["questions"]


def validate_message_format(message):
    """éªŒè¯æ¶ˆæ¯æ ¼å¼"""
    if not isinstance(message, dict):
        return False
    
    required_fields = ["role", "content"]
    for field in required_fields:
        if field not in message:
            return False
    
    if message["role"] not in ["user", "assistant"]:
        return False
    
    if not isinstance(message["content"], str):
        return False
    
    return True


def clean_chat_history(kb_id, max_messages=1000, logger=None):
    """
    æ¸…ç†è¿‡é•¿çš„å¯¹è¯å†å²
    - ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
    - é˜²æ­¢æ–‡ä»¶è¿‡å¤§
    """
    messages = load_chat_history_safe(kb_id, logger)
    
    if len(messages) > max_messages:
        messages = messages[-max_messages:]
        save_chat_history_safe(kb_id, messages, logger)
        
        if logger:
            logger.log("å¯¹è¯æ¸…ç†", "info", f"âœ… æ¸…ç†å¯¹è¯å†å²: {kb_id} (ä¿ç•™æœ€è¿‘ {max_messages} æ¡)", 
                      {"kb_id": kb_id, "kept": max_messages})


def export_chat_history(kb_id, export_format="json", logger=None):
    """
    å¯¼å‡ºå¯¹è¯å†å²
    - æ”¯æŒJSONå’ŒMarkdownæ ¼å¼
    """
    messages = load_chat_history_safe(kb_id, logger)
    
    if export_format == "json":
        return json.dumps(messages, indent=2, ensure_ascii=False)
    
    elif export_format == "markdown":
        md_content = f"# å¯¹è¯å†å²: {kb_id}\n\n"
        md_content += f"å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        for msg in messages:
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            
            if role == "user":
                md_content += f"## ğŸ‘¤ ç”¨æˆ·\n\n{content}\n\n"
            else:
                md_content += f"## ğŸ¤– åŠ©æ‰‹\n\n{content}\n\n"
        
        return md_content
    
    else:
        if logger:
            logger.log_error("å¯¼å‡º", f"ä¸æ”¯æŒçš„æ ¼å¼: {export_format}")
        return None