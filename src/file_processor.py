"æ–‡ä»¶å¤„ç†ç»“æœè¿½è¸ª"
import os
from pathlib import Path
from typing import Dict, List, Tuple

SUPPORTED_FORMATS = {'.pdf', '.txt', '.docx', '.md', '.xlsx', '.csv', '.json', '.pptx', '.ppt'}

class FileProcessResult:
    def __init__(self):
        self.success: List[Dict] = []  # [{'file': name, 'size': bytes, 'docs': count}]
        self.failed: List[Dict] = []   # [{'file': name, 'reason': error_msg}]
        self.skipped: List[Dict] = []  # [{'file': name, 'reason': why}]
    
    def add_success(self, filename: str, size: int, doc_count: int):
        self.success.append({'file': filename, 'size': size, 'docs': doc_count})
    
    def add_failed(self, filename: str, reason: str):
        self.failed.append({'file': filename, 'reason': reason})
    
    def add_skipped(self, filename: str, reason: str):
        self.skipped.append({'file': filename, 'reason': reason})
    
    def get_summary(self) -> Dict:
        return {
            'total': len(self.success) + len(self.failed) + len(self.skipped),
            'success': len(self.success),
            'failed': len(self.failed),
            'skipped': len(self.skipped),
            'total_docs': sum(f['docs'] for f in self.success),
            'total_size': sum(f['size'] for f in self.success),
        }
    
    def get_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        summary = self.get_summary()
        report = f"\nğŸ“Š æ–‡ä»¶å¤„ç†æŠ¥å‘Š\n"
        report += f"{ '='*50}\n"
        report += f"âœ… æˆåŠŸ: {summary['success']} ä¸ªæ–‡ä»¶ ({summary['total_docs']} ä¸ªæ–‡æ¡£)\n"
        report += f"âŒ å¤±è´¥: {summary['failed']} ä¸ªæ–‡ä»¶\n"
        report += f"â­ï¸  è·³è¿‡: {summary['skipped']} ä¸ªæ–‡ä»¶\n"
        report += f"{ '='*50}\n"
        
        if self.failed:
            report += f"\nâŒ å¤±è´¥æ–‡ä»¶è¯¦æƒ…:\n"
            for item in self.failed[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                report += f"  â€¢ {item['file']}: {item['reason']}\n"
            if len(self.failed) > 10:
                report += f"  ... è¿˜æœ‰ {len(self.failed) - 10} ä¸ªå¤±è´¥æ–‡ä»¶\n"
        
        if self.skipped:
            report += f"\nâ­ï¸  è·³è¿‡æ–‡ä»¶è¯¦æƒ…:\n"
            for item in self.skipped[:10]:
                report += f"  â€¢ {item['file']}: {item['reason']}\n"
            if len(self.skipped) > 10:
                report += f"  ... è¿˜æœ‰ {len(self.skipped) - 10} ä¸ªè·³è¿‡æ–‡ä»¶\n"
        
        return report


from typing import List, Tuple
from pathlib import Path
import os
import multiprocessing as mp

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
SUPPORTED_FORMATS = {'.pdf', '.txt', '.docx', '.md', '.xlsx', '.xls', '.csv', '.json'}

# OCRå¤šè¿›ç¨‹å‡½æ•°ï¼ˆå¿…é¡»åœ¨æ¨¡å—çº§åˆ«ï¼‰
def _ocr_page(args):
    """OCRå•é¡µå¤„ç†ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰"""
    import pytesseract
    import os
    
    idx, img = args
    try:
        # è®¾ç½®OCRé…ç½®ï¼Œæå‡è¯†åˆ«é€Ÿåº¦å’Œå‡†ç¡®ç‡
        config = '--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡äº¿é›¶å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾ä½°ä»Ÿè¬å„„'
        
        # å¤šè¯­è¨€è¯†åˆ«
        text = pytesseract.image_to_string(img, lang='chi_sim+eng', config=config)
        
        # æ¸…ç†æ–‡æœ¬
        if text:
            text = text.strip()
            # ç§»é™¤è¿‡çŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯å™ªå£°ï¼‰
            lines = [line.strip() for line in text.split('\n') if len(line.strip()) > 2]
            text = '\n'.join(lines)
        
        return idx, text if text else ""
    except Exception as e:
        print(f"   âš ï¸  ç¬¬{idx}é¡µOCRå¤±è´¥: {str(e)[:30]}")
        return idx, ""

# å°†æ–‡ä»¶åŠ è½½å‡½æ•°ç§»åˆ°æ¨¡å—çº§åˆ«ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰
def _load_single_file(file_info, use_ocr=True):
    """å•ä¸ªæ–‡ä»¶åŠ è½½å‡½æ•°ï¼ˆä¼˜åŒ–ï¼šç›´æ¥è¯»å–æ–‡ä»¶å†…å®¹ï¼Œé¿å… SimpleDirectoryReader å¼€é”€ï¼‰"""
    # å±è”½å­è¿›ç¨‹ä¸­çš„è­¦å‘Šå’Œæ—¥å¿—
    import warnings
    import logging
    import os
    import uuid  # æ–°å¢å¯¼å…¥
    from datetime import datetime
    from llama_index.core import Document
    
    warnings.filterwarnings('ignore')
    logging.getLogger('streamlit').setLevel(logging.ERROR)
    logging.getLogger('pypdf').setLevel(logging.ERROR)
    logging.getLogger('pdfminer').setLevel(logging.ERROR)
    
    try:
        # æ­£ç¡®è§£åŒ… (path, filename, extension)
        file_path, file_name, file_ext = file_info
        
        # [æ–°å¢] 1. æå–ä¸°å¯Œçš„ç³»ç»Ÿå…ƒæ•°æ®
        try:
            file_stat = os.stat(file_path)
            creation_date = datetime.fromtimestamp(file_stat.st_ctime).strftime('%Y-%m-%d')
            last_modified_date = datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d')
            parent_folder = os.path.basename(os.path.dirname(file_path))
            
            base_metadata = {
                "file_name": file_name,
                "file_path": str(file_path),
                "file_size": file_stat.st_size,
                "creation_date": creation_date,
                "last_modified_date": last_modified_date,
                "file_extension": file_ext.lower(),
                "parent_folder": parent_folder
            }
        except Exception as e:
            # å¦‚æœå…ƒæ•°æ®æå–å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ä¿¡æ¯å…œåº•
            base_metadata = {
                "file_name": file_name,
                "file_path": str(file_path)
            }
            # ä»…åœ¨è°ƒè¯•æ—¶æ‰“å°ï¼Œé¿å…æ—¥å¿—åˆ·å±
            # print(f"âš ï¸  å…ƒæ•°æ®æå–è­¦å‘Š: {e}")

        size = os.path.getsize(file_path)
        ext = file_ext.lower() # ç»Ÿä¸€ä½¿ç”¨å°å†™æ‰©å±•å
        
        # æ£€æŸ¥æ ¼å¼æ”¯æŒ
        if ext not in SUPPORTED_FORMATS:
            return None, file_name, 'skipped', f"ä¸æ”¯æŒçš„æ ¼å¼: {ext}", 'skip'
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if size > 100 * 1024 * 1024:  # 100MB
            return None, file_name, 'skipped', "æ–‡ä»¶è¿‡å¤§ (>100MB)", 'skip'
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¿«é€Ÿè¯»å–
        if ext in ['.txt', '.md', '.py', '.js', '.json', '.xml', '.html', '.css', '.yaml', '.yml', '.sh', '.sql', 
                   '.log', '.ini', '.conf', '.cfg', '.csv', '.tsv', '.properties', '.env', '.rst', '.toml']:
            # æ–‡æœ¬æ–‡ä»¶ï¼šç›´æ¥è¯»å–ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
            # [ä¿®æ”¹] æ³¨å…¥ base_metadata å¹¶æ˜¾å¼è®¾ç½® doc_id
            docs = [Document(text=text, metadata=base_metadata, id_=str(uuid.uuid4()))]
            read_mode = 'fast'
        
        elif ext in ['.xlsx', '.xls']:
            # Excelæ–‡ä»¶ï¼šå¿«é€Ÿè¯»å–ï¼ˆåªè¯»æ–‡æœ¬å†…å®¹ï¼Œä¸è§£ææ ¼å¼ï¼‰
            try:
                import openpyxl
                wb = openpyxl.load_workbook(file_path, read_only=True, data_only=True)
                text_parts = []
                for sheet in wb.worksheets[:5]:  # åªè¯»å‰5ä¸ªsheet
                    for row in sheet.iter_rows(max_row=1000, values_only=True):  # æ¯ä¸ªsheetæœ€å¤š1000è¡Œ
                        row_text = ' '.join([str(cell) for cell in row if cell is not None])
                        if row_text.strip():
                            text_parts.append(row_text)
                wb.close()
                text = '\n'.join(text_parts)
                # [ä¿®æ”¹] æ³¨å…¥ base_metadata å¹¶æ˜¾å¼è®¾ç½® doc_id
                docs = [Document(text=text, metadata=base_metadata, id_=str(uuid.uuid4()))]
                read_mode = 'fast'
            except:
                # å¤±è´¥åˆ™ç”¨æ…¢é€Ÿæ¨¡å¼
                from llama_index.core import SimpleDirectoryReader
                docs = SimpleDirectoryReader(input_files=[file_path]).load_data()
                # [ä¿®æ”¹] æ³¨å…¥ base_metadata å¹¶ç¡®ä¿ ID
                for d in docs: 
                    d.metadata.update(base_metadata)
                    if not d.doc_id: d.doc_id = str(uuid.uuid4())
                read_mode = 'slow'
        elif ext == '.pdf':
            # ğŸ“„ PDFæ–‡ä»¶ï¼šä½¿ç”¨æ”¯æŒé¡µç çš„è¯»å–å™¨
            try:
                from src.utils.pdf_page_reader import read_pdf_with_pages
                docs = read_pdf_with_pages(file_path, base_metadata)
                # ç¡®ä¿æ¯ä¸ªæ–‡æ¡£éƒ½æœ‰ID
                for d in docs:
                    if not d.doc_id: d.doc_id = str(uuid.uuid4())
                read_mode = 'pdf_with_pages'
            except Exception as e:
                # å›é€€åˆ°æ ‡å‡†è¯»å–å™¨
                from llama_index.core import SimpleDirectoryReader
                docs = SimpleDirectoryReader(input_files=[file_path]).load_data()
                # [ä¿®æ”¹] æ³¨å…¥ base_metadata å¹¶ç¡®ä¿ ID
                for d in docs: 
                    d.metadata.update(base_metadata)
                    if not d.doc_id: d.doc_id = str(uuid.uuid4())
                read_mode = 'slow'
        elif ext in ['.pptx', '.ppt']:
            # PowerPointæ–‡ä»¶ï¼šè¯»å–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            try:
                from pptx import Presentation
                prs = Presentation(file_path)
                text_parts = []
                for slide_idx, slide in enumerate(prs.slides):
                    text_parts.append(f"--- å¹»ç¯ç‰‡ {slide_idx + 1} ---")
                    for shape in slide.shapes:
                        if hasattr(shape, "text") and shape.text.strip():
                            text_parts.append(shape.text)
                text = '\n'.join(text_parts)
                # [ä¿®æ”¹] æ³¨å…¥ base_metadata å¹¶æ˜¾å¼è®¾ç½® doc_id
                docs = [Document(text=text, metadata=base_metadata, id_=str(uuid.uuid4()))]
                read_mode = 'fast'
            except Exception as e:
                return None, file_name, 'failed', f"PPTXè§£æå¤±è´¥: {str(e)[:50]}", 'slow'
        else:
            # å…¶ä»–æ ¼å¼ï¼šä½¿ç”¨ SimpleDirectoryReaderï¼ˆæ…¢é€Ÿæ¨¡å¼ï¼‰
            from llama_index.core import SimpleDirectoryReader
            docs = SimpleDirectoryReader(input_files=[file_path]).load_data()
            # [ä¿®æ”¹] æ³¨å…¥ base_metadata å¹¶ç¡®ä¿ ID
            for d in docs: 
                d.metadata.update(base_metadata)
                if not d.doc_id: d.doc_id = str(uuid.uuid4())
            read_mode = 'slow'

        # å¦‚æœæ˜¯PDFä¸”å†…å®¹ä¸ºç©ºï¼Œå°è¯• OCRï¼ˆæ‰«æç‰ˆPDFï¼‰
        needs_ocr = False
        if ext == '.pdf' and docs:
            if not docs[0].text or len(docs[0].text.strip()) == 0:
                needs_ocr = True
        
        if needs_ocr:
                # æ£€æŸ¥OCRè®¾ç½®ï¼šä¼˜å…ˆæ£€æŸ¥ä¼ å…¥å‚æ•°ï¼Œå…¶æ¬¡æ£€æŸ¥ç¯å¢ƒå˜é‡
                skip_ocr_env = os.environ.get('SKIP_OCR', 'false').lower() == 'true'
                
                # å¦‚æœå‰å°ç¦ç”¨OCRæˆ–ç¯å¢ƒå˜é‡è®¾ç½®è·³è¿‡ï¼Œåˆ™è·³è¿‡OCR
                if not use_ocr or skip_ocr_env:
                    source = "å‰å°è®¾ç½®" if not use_ocr else "ç¯å¢ƒå˜é‡"
                    print(f"   âš¡ è·³è¿‡OCRå¤„ç†ï¼ˆ{source}æ§åˆ¶ï¼‰")
                    return "æ­¤PDFä¸ºæ‰«æç‰ˆï¼Œå·²è·³è¿‡OCRå¤„ç†ã€‚å¦‚éœ€OCRè¯†åˆ«ï¼Œè¯·åœ¨å‰å°å‹¾é€‰'å¯ç”¨OCRè¯†åˆ«'"
                
                try:
                    from pdf2image import convert_from_path
                    from src.utils.enhanced_ocr_optimizer import enhanced_ocr_optimizer
                    
                    print(f"   ğŸ” æ£€æµ‹åˆ°æ‰«æç‰ˆPDFï¼Œå¯ç”¨å¢å¼ºOCRå¤„ç†...")
                    
                    # è½¬æ¢PDFä¸ºå›¾ç‰‡
                    images = convert_from_path(file_path, dpi=200)
                    
                    # ä½¿ç”¨å¢å¼ºOCRä¼˜åŒ–å™¨å¤„ç†
                    ocr_results = enhanced_ocr_optimizer.process_pdf_pages(file_path, images)
                    
                    # åˆå¹¶OCRç»“æœ
                    full_text = '\n\n'.join([
                        f"=== ç¬¬ {i+1} é¡µ ===\n{text}" 
                        for i, text in enumerate(ocr_results) if text.strip()
                    ])
                    
                    if full_text.strip():
                        print(f"   âœ… OCRå¤„ç†å®Œæˆ: {len(images)} é¡µï¼Œæå– {len(full_text)} å­—ç¬¦")
                        # OCRç›´æ¥è¿”å›æ–‡æœ¬ï¼Œä¸Šå±‚é€»è¾‘ä¼šå¤„ç†ï¼Œä½†è¿™é‡Œæˆ‘ä»¬éœ€è¦ç¡®ä¿è¿”å›çš„å…ƒæ•°æ®ä¸€è‡´æ€§
                        # åŸé€»è¾‘ç›´æ¥è¿”å›æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œè°ƒç”¨æ–¹ _process_batch ä¼¼ä¹èƒ½å¤„ç†ï¼Ÿ
                        # æ£€æŸ¥ _process_batch -> å®ƒæ˜¯ç›´æ¥è¿”å› _load_single_file çš„ç»“æœã€‚
                        # åŸé€»è¾‘: return full_text (è¿™çœ‹èµ·æ¥æ˜¯ä¸ªBugï¼Œå› ä¸ºå…¶ä»–è·¯å¾„è¿”å›tuple)
                        # ç­‰ç­‰ï¼ŒåŸä»£ç : return full_text ç¡®å®å­˜åœ¨ã€‚è¿™ä¼šå¯¼è‡´ _process_batch æ‹¿åˆ°å­—ç¬¦ä¸²è€Œä¸æ˜¯tupleã€‚
                        # è®©æˆ‘ä»¬ä¿®æ­£è¿™ä¸ªæ½œåœ¨Bugï¼Œè¿”å›æ ‡å‡†tupleæ ¼å¼
                        # å®é™…ä¸ŠåŸä»£ç ä¸­:
                        # return f"__BATCH_OCR__{task_id}", fname, 'pending_ocr', len(images), 'batch_ocr'
                        # æ˜¯æ­£ç¡®è¿”å›ã€‚ä½† full_text çš„è¿”å›ä¼¼ä¹ä¸å¯¹ã€‚
                        # ä¿®æ­£ä¸ºï¼šè¿”å›å•æ–‡æ¡£åˆ—è¡¨
                        return [Document(text=full_text, metadata=base_metadata)], file_name, 'success', (size, 1), 'ocr'
                    else:
                        print(f"   âš ï¸  OCRæœªæå–åˆ°æ–‡æœ¬å†…å®¹")
                        return None, file_name, 'failed', "æ­¤PDFä¸ºæ‰«æç‰ˆï¼ŒOCRå¤„ç†æœªèƒ½æå–åˆ°æ–‡æœ¬å†…å®¹ã€‚", 'ocr'
                    
                    # åŸä»£ç ä¸­è¿˜æœ‰ batch_ocr é€»è¾‘ï¼Œä½†è¿™é‡Œè¢«ä¸Šé¢çš„ return è¦†ç›–äº†ï¼Ÿ
                    # çœ‹æ¥åŸä»£ç é€»è¾‘æ˜¯ï¼šå¦‚æœèƒ½ç«‹å³å¤„ç†å®Œå°±è¿”å›æ–‡æœ¬ï¼Œå¦åˆ™æ‰”è¿›é˜Ÿåˆ—ã€‚
                    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘è¿™é‡Œå‡è®¾ enhanced_ocr_optimizer æ˜¯åŒæ­¥çš„ï¼ˆæ ¹æ®åŸä»£ç çœ‹ä¼¼å¦‚æ­¤ï¼‰
                    
                except Exception as e:
                    return None, file_name, 'failed', f"OCRå‡†å¤‡å¤±è´¥: {str(e)[:50]}", 'ocr'
        
        if docs:
            # è¿‡æ»¤æ‰ç©ºæ–‡æ¡£
            docs = [d for d in docs if d.text and d.text.strip()]
            if docs:
                return docs, file_name, 'success', (size, len(docs)), read_mode
            else:
                return None, file_name, 'failed', "æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼ˆæ‰€æœ‰æ–‡æ¡£éƒ½æ˜¯ç©ºçš„ï¼‰", read_mode
        else:
            return None, file_name, 'failed', "æ–‡ä»¶å†…å®¹ä¸ºç©º", read_mode
    
    except Exception as e:
        error_msg = str(e)
        # ç®€åŒ–å¸¸è§é”™è¯¯ä¿¡æ¯
        if "trailer cannot be read" in error_msg or "invalid literal" in error_msg:
            error_msg = "PDFæ–‡ä»¶æŸå"
        elif "not a zip file" in error_msg.lower():
            error_msg = "DOCXæ–‡ä»¶æŸå"
        elif "RetryError" in error_msg or "AttributeError" in error_msg:
            error_msg = "æ–‡ä»¶è§£æå¤±è´¥"
        elif "Unsupported" in error_msg:
            error_msg = "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"
        return None, file_name, 'failed', error_msg[:100]


# æ‰¹é‡å¤„ç†å‡½æ•°ï¼ˆæ¨¡å—çº§åˆ«ï¼Œç”¨äºå¤šè¿›ç¨‹ï¼‰
def _process_batch(args):
    """æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼ˆåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œï¼‰"""
    # è§£åŒ…å‚æ•°
    if isinstance(args, tuple) and len(args) == 2:
        batch_files, use_ocr = args
    else:
        batch_files = args
        use_ocr = True  # é»˜è®¤å€¼
        
    # å®‰å…¨çš„CPUå¯†é›†å‹è®¡ç®—ï¼Œå¼ºåˆ¶æ¿€æ´»CPUæ ¸å¿ƒ
    import math
    import os
    import time
    
    # è·å–è¿›ç¨‹ID
    pid = os.getpid()
    
    # åŸæœ‰çš„æ–‡æ¡£å¤„ç†
    batch_results = []
    for file_info in batch_files:
        result = _load_single_file(file_info, use_ocr=use_ocr)
        batch_results.append(result)
    return batch_results


def scan_directory_safe(input_dir: str, use_ocr: bool = True) -> Tuple[List, 'FileProcessResult']:
    """
    å®‰å…¨æ‰«æç›®å½•ï¼Œè¿”å›æˆåŠŸåŠ è½½çš„æ–‡æ¡£å’Œå¤„ç†ç»“æœï¼ˆå¤šçº¿ç¨‹å¹¶è¡Œï¼‰
    
    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        use_ocr: æ˜¯å¦å¯ç”¨OCRè¯†åˆ«
    
    Returns:
        (documents, result) - æ–‡æ¡£åˆ—è¡¨å’Œå¤„ç†ç»“æœ
    """
    from llama_index.core import SimpleDirectoryReader
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    result = FileProcessResult()
    all_docs = []
    
    # ç¬¬ä¸€æ­¥ï¼šå¹¶è¡Œæ‰«ææ‰€æœ‰æ–‡ä»¶ï¼ˆä¼˜åŒ–ï¼šå¤šçº¿ç¨‹åŠ é€Ÿï¼‰
    print(f"ğŸ“ [ç¬¬ 2 æ­¥] å¹¶è¡Œæ‰«æç›®å½•: {input_dir}")
    file_list = []
    
    # è·å–æ‰€æœ‰å­ç›®å½•
    subdirs = [input_dir]
    try:
        subdirs.extend([os.path.join(input_dir, d) for d in os.listdir(input_dir) 
                       if os.path.isdir(os.path.join(input_dir, d)) and not d.startswith('.')])
    except:
        pass
    
    # å¹¶è¡Œæ‰«æå‡½æ•°
    def scan_dir(directory):
        local_files = []
        try:
            for root, _, filenames in os.walk(directory):
                for f in filenames:
                    if not f.startswith('.'):
                        fp = os.path.join(root, f)
                        ext = Path(f).suffix.lower()
                        local_files.append((fp, f, ext))
        except Exception as e:
            print(f"   æ‰«æå¤±è´¥ {directory}: {e}")
        return local_files
    
    # å¤šçº¿ç¨‹å¹¶è¡Œæ‰«æï¼ˆå®‰å…¨é…ç½®ï¼š32 çº¿ç¨‹ï¼‰
    if len(subdirs) > 1:
        from concurrent.futures import ThreadPoolExecutor, as_completed
        scan_workers = min(32, len(subdirs))  # 32 çº¿ç¨‹
        print(f"âš¡ [ç¬¬ 2 æ­¥] å®‰å…¨æ¨¡å¼ï¼š{scan_workers} çº¿ç¨‹å¹¶è¡Œæ‰«æ {len(subdirs)} ä¸ªç›®å½•")
        
        with ThreadPoolExecutor(max_workers=scan_workers) as executor:
            futures = [executor.submit(scan_dir, d) for d in subdirs]
            for future in as_completed(futures):
                file_list.extend(future.result())
    else:
        # å•ç›®å½•ç›´æ¥æ‰«æ
        file_list = scan_dir(input_dir)
    
    print(f"âœ… [ç¬¬ 2 æ­¥] æ‰«æå®Œæˆ: å‘ç° {len(file_list)} ä¸ªæ–‡ä»¶")
    
    # ç¬¬äºŒæ­¥ï¼šå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼ˆåŠ¨æ€è°ƒåº¦ï¼Œä¿æŒèµ„æº < 80%ï¼‰
    import psutil
    import time as time_module
    from queue import Queue
    from threading import Semaphore
    
    # åˆå§‹çº¿ç¨‹æ•°ï¼ˆæé™é…ç½®ï¼š250 çº¿ç¨‹ï¼Œå†²åˆº 80%ï¼‰
    # åŠ¨æ€è®¡ç®—æœ€ä¼˜é…ç½®
    fast_formats = {'.txt', '.md', '.py', '.js', '.json', '.xml', '.html', '.css', '.yaml', '.yml', '.sh', '.sql',
                   '.log', '.ini', '.conf', '.cfg', '.csv', '.tsv', '.properties', '.env', '.rst', '.toml',
                   '.xlsx', '.xls'}  # Excelä¹ŸåŠ å…¥å¿«é€Ÿè¯»å–
    fast_count = sum(1 for _, _, ext in file_list if ext in fast_formats)
    fast_ratio = fast_count / len(file_list) if len(file_list) > 0 else 0
    
    # ç¨³å®šç­–ç•¥ï¼šä½¿ç”¨åˆç†çš„æ ¸å¿ƒæ•°ï¼Œé¿å…æ­»æœº
    cpu_cores = mp.cpu_count()
    # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°ï¼Œä¿ç•™éƒ¨åˆ†æ ¸å¿ƒç»™ç³»ç»Ÿ
    max_workers = min(cpu_cores, 12)
    batch_size = 10
    mode_name = "ç¨³å®šå¹¶è¡Œ"
    
    max_workers = int(max_workers)
    
    if max_workers > 1 and len(file_list) > 10:
        # batch_sizeå·²åœ¨ä¸Šé¢åŠ¨æ€è®¡ç®—ï¼Œè¿™é‡Œä¸å†é‡å¤å®šä¹‰
        
        print(f"ğŸš€ [ç¬¬ 3 æ­¥] {mode_name}æ¨¡å¼: {max_workers} è¿›ç¨‹ | æ–‡æœ¬å æ¯”: {fast_ratio*100:.1f}%")
        print(f"ğŸ“¦ [ç¬¬ 3 æ­¥] æ‰¹é‡å¤§å°: {batch_size} ä¸ª/æ‰¹")
        
        # ä½¿ç”¨å¤šè¿›ç¨‹ï¼ˆçªç ´GILé™åˆ¶ï¼‰
        import time as time_module
        
        # ç§»é™¤å¼ºåˆ¶ set_start_method('fork')ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
        
        actual_workers = max_workers
        print(f"ğŸ’» [ç¬¬ 3 æ­¥] å¯åŠ¨å¤„ç†ï¼šä½¿ç”¨ {actual_workers} ä¸ªè¿›ç¨‹")
        
        # ç»Ÿè®¡å¿«é€Ÿ/æ…¢é€Ÿè¯»å–
        fast_count = 0
        slow_count = 0
        
        # å°†æ–‡ä»¶åˆ—è¡¨åˆ†æ‰¹ (æ‰“åŒ… use_ocr å‚æ•°)
        batches = [(file_list[i:i + batch_size], use_ocr) for i in range(0, len(file_list), batch_size)]
        print(f"ğŸ“Š [ç¬¬ 3 æ­¥] æ€»è®¡ {len(file_list)} ä¸ªæ–‡ä»¶ï¼Œåˆ†æˆ {len(batches)} æ‰¹")
        
        start_time = time_module.time()
        completed = 0
        
        # ä½¿ç”¨æ›´å¤šè¿›ç¨‹ï¼Œå°æ‰¹æ¬¡ï¼Œå¼ºåˆ¶åˆ†å¸ƒåˆ°æ‰€æœ‰æ ¸å¿ƒ
        with mp.Pool(processes=actual_workers) as pool:
            print(f"ğŸ¯ å¯åŠ¨ {actual_workers} ä¸ªè¿›ç¨‹ï¼Œå°æ‰¹æ¬¡å¤„ç†å¼ºåˆ¶ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ")
            # ä½¿ç”¨imap_unorderedè·å–ç»“æœ
            for batch_results in pool.imap_unordered(_process_batch, batches, chunksize=1):
                try:
                    for file_result in batch_results:
                        # æ›´å®‰å…¨çš„è§£åŒ…å¤„ç†
                        if len(file_result) == 5:
                            docs, fname, status, info, read_mode = file_result
                        elif len(file_result) == 4:
                            docs, fname, status, info = file_result
                            read_mode = 'unknown'
                        else:
                            # å¤„ç†å¼‚å¸¸æƒ…å†µ
                            print(f"âš ï¸ å¼‚å¸¸è¿”å›å€¼: {file_result}")
                            continue
                            
                        completed += 1
                        
                        # ç»Ÿè®¡è¯»å–æ¨¡å¼
                        if read_mode == 'fast':
                            fast_count += 1
                        elif read_mode == 'slow':
                            slow_count += 1
                        
                        if status == 'success':
                            all_docs.extend(docs)
                            size, doc_count = info
                            result.add_success(fname, size, doc_count)
                        elif status == 'skipped':
                            result.add_skipped(fname, info)
                        else:  # failed
                            result.add_failed(fname, info)
                    
                    # æ˜¾ç¤ºè¿›åº¦å’Œç»Ÿè®¡ï¼ˆæ¯200ä¸ªæ–‡ä»¶ï¼‰
                    if completed % 200 == 0:
                        elapsed = time_module.time() - start_time
                        speed = completed / elapsed if elapsed > 0 else 0
                        remaining = len(file_list) - completed
                        eta_seconds = remaining / speed if speed > 0 else 0
                        eta_minutes = eta_seconds / 60
                        
                        progress_pct = completed / len(file_list) * 100
                        print(f"ğŸ“Š [ç¬¬ 3 æ­¥] {completed}/{len(file_list)} ({progress_pct:.1f}%) | è¿›ç¨‹: {actual_workers}")
                        print(f"   âš¡ å¿«é€Ÿ: {fast_count} | ğŸŒ æ…¢é€Ÿ: {slow_count} | é€Ÿåº¦: {speed:.1f} æ–‡ä»¶/ç§’ | â±ï¸  é¢„è®¡å‰©ä½™: {eta_minutes:.1f} åˆ†é’Ÿ")
                    
                    # æ¯å¤„ç†50ä¸ªæ–‡ä»¶æ‰“å°ç®€å•è¿›åº¦
                    if completed % 50 == 0:
                        print(f"   å·²è¯»å–: {completed}/{len(file_list)}")
                
                except Exception as e:
                    print(f"   æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
        
        
        # æœ€ç»ˆç»Ÿè®¡
        print(f"\nâœ… [ç¬¬ 3 æ­¥] æ–‡ä»¶è¯»å–å®Œæˆ:")
        print(f"   âš¡ å¿«é€Ÿè¯»å– (ç›´æ¥): {fast_count} ä¸ªæ–‡ä»¶")
        print(f"   ğŸŒ æ…¢é€Ÿè¯»å– (è§£æ): {slow_count} ä¸ªæ–‡ä»¶")
        if fast_count + slow_count > 0:
            print(f"   ğŸ“ˆ å¿«é€Ÿå æ¯”: {fast_count/(fast_count+slow_count)*100:.1f}%")
    
    else:
        # å•æ ¸æ¨¡å¼ï¼ˆæ–‡ä»¶å°‘æ—¶ï¼‰
        for file_info in file_list:
            fp, fname, ext = file_info
            try:
                # ç»Ÿä¸€ä½¿ç”¨ _load_single_file å¤„ç†
                # è§£åŒ…è¿”å›å€¼: docs, fname, status, info, read_mode
                result_tuple = _load_single_file(file_info, use_ocr=use_ocr)
                
                if result_tuple[0]: # docs is not None
                    if len(result_tuple) == 5:
                        docs, _, status, info, _ = result_tuple
                    else:
                        docs, _, status, info = result_tuple
                        
                    if status == 'success':
                        all_docs.extend(docs)
                        size, doc_count = info
                        result.add_success(fname, size, doc_count)
                    elif status == 'skipped':
                        result.add_skipped(fname, info)
                    else:
                        result.add_failed(fname, info)
                else:
                    # Handle failure/skip where docs is None
                    _, fname, status, info, _ = result_tuple
                    if status == 'skipped':
                        result.add_skipped(fname, info)
                    else:
                        result.add_failed(fname, info)

            except Exception as e:
                result.add_failed(fname, str(e)[:100])
    
    # æ‰¹é‡OCRå¤„ç†ï¼ˆåœ¨æ‰€æœ‰æ–‡ä»¶æ‰«æå®Œæˆåç»Ÿä¸€å¤„ç†ï¼‰
    from src.utils.batch_ocr_processor import batch_ocr_processor
    
    if batch_ocr_processor.ocr_tasks:
        print(f"\nğŸš€ [ç¬¬ 4 æ­¥] æ‰¹é‡OCRå¤„ç†å¼€å§‹...")
        
        # ç»Ÿä¸€å¤„ç†æ‰€æœ‰OCRä»»åŠ¡
        ocr_results = batch_ocr_processor.process_all_ocr_tasks()
        
        # å¤„ç†OCRç»“æœï¼Œå°†å¾…å¤„ç†çš„æ–‡æ¡£è½¬æ¢ä¸ºçœŸå®æ–‡æ¡£
        pending_docs = []
        for doc in all_docs:
            if hasattr(doc, 'text') and doc.text.startswith('__BATCH_OCR__'):
                task_id = doc.text.replace('__BATCH_OCR__', '')
                
                # è·å–OCRç»“æœ
                ocr_texts = batch_ocr_processor.get_file_result(task_id)
                
                if ocr_texts:
                    # åˆ›å»ºæ–°çš„æ–‡æ¡£å¯¹è±¡
                    from llama_index.core import Document
                    full_text = "\n\n".join(ocr_texts)
                    new_doc = Document(text=full_text, metadata=doc.metadata)
                    pending_docs.append(new_doc)
                    print(f"   âœ… OCRå®Œæˆ: {doc.metadata.get('file_name', 'unknown')} ({len(ocr_texts)} é¡µ)")
                else:
                    # OCRå¤±è´¥ï¼Œè®°å½•åˆ°å¤±è´¥åˆ—è¡¨
                    fname = doc.metadata.get('file_name', 'unknown')
                    result.add_failed(fname, "OCRæœªè¯†åˆ«åˆ°æ–‡å­—")
                    print(f"   âŒ OCRå¤±è´¥: {fname}")
        
        # æ›¿æ¢å¾…å¤„ç†çš„æ–‡æ¡£
        all_docs = [doc for doc in all_docs if not (hasattr(doc, 'text') and doc.text.startswith('__BATCH_OCR__'))]
        all_docs.extend(pending_docs)
        
        print(f"âœ… [ç¬¬ 4 æ­¥] æ‰¹é‡OCRå¤„ç†å®Œæˆ")
    
    return all_docs, result


