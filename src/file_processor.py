"æ–‡ä»¶å¤„ç†ç»“æœè¿½è¸ª"
import os
from pathlib import Path
from typing import Dict, List, Tuple

SUPPORTED_FORMATS = {'.pdf', '.txt', '.docx', '.md', '.xlsx', '.csv', '.json', '.pptx', '.ppt'}

class FileProcessResult:
    def __init__(self):
        self.success: List[Dict] = []  # [{'file': name, 'size': bytes, 'docs': count}]
        self.failed: List[Dict] = []   # [{'file': name, 'reason': error_msg}]
        self.skipped: List[Dict] = []  # [{'file': name, 'reason': why}]
    
    def add_success(self, filename: str, size: int, doc_count: int):
        self.success.append({'file': filename, 'size': size, 'docs': doc_count})
    
    def add_failed(self, filename: str, reason: str):
        self.failed.append({'file': filename, 'reason': reason})
    
    def add_skipped(self, filename: str, reason: str):
        self.skipped.append({'file': filename, 'reason': reason})
    
    def get_summary(self) -> Dict:
        return {
            'total': len(self.success) + len(self.failed) + len(self.skipped),
            'success': len(self.success),
            'failed': len(self.failed),
            'skipped': len(self.skipped),
            'total_docs': sum(f['docs'] for f in self.success),
            'total_size': sum(f['size'] for f in self.success),
        }
    
    def get_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        summary = self.get_summary()
        report = f"\nğŸ“Š æ–‡ä»¶å¤„ç†æŠ¥å‘Š\n"
        report += f"{ '='*50}\n"
        report += f"âœ… æˆåŠŸ: {summary['success']} ä¸ªæ–‡ä»¶ ({summary['total_docs']} ä¸ªæ–‡æ¡£)\n"
        report += f"âŒ å¤±è´¥: {summary['failed']} ä¸ªæ–‡ä»¶\n"
        report += f"â­ï¸  è·³è¿‡: {summary['skipped']} ä¸ªæ–‡ä»¶\n"
        report += f"{ '='*50}\n"
        
        if self.failed:
            report += f"\nâŒ å¤±è´¥æ–‡ä»¶è¯¦æƒ…:\n"
            for item in self.failed[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                report += f"  â€¢ {item['file']}: {item['reason']}\n"
            if len(self.failed) > 10:
                report += f"  ... è¿˜æœ‰ {len(self.failed) - 10} ä¸ªå¤±è´¥æ–‡ä»¶\n"
        
        if self.skipped:
            report += f"\nâ­ï¸  è·³è¿‡æ–‡ä»¶è¯¦æƒ…:\n"
            for item in self.skipped[:10]:
                report += f"  â€¢ {item['file']}: {item['reason']}\n"
            if len(self.skipped) > 10:
                report += f"  ... è¿˜æœ‰ {len(self.skipped) - 10} ä¸ªè·³è¿‡æ–‡ä»¶\n"
        
        return report


from typing import List, Tuple
from pathlib import Path
import os

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
SUPPORTED_FORMATS = {'.pdf', '.txt', '.docx', '.md', '.xlsx', '.xls', '.csv', '.json'}

# å°†æ–‡ä»¶åŠ è½½å‡½æ•°ç§»åˆ°æ¨¡å—çº§åˆ«ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰
def _load_single_file(file_info):
    """å•ä¸ªæ–‡ä»¶åŠ è½½å‡½æ•°ï¼ˆä¼˜åŒ–ï¼šç›´æ¥è¯»å–æ–‡ä»¶å†…å®¹ï¼Œé¿å… SimpleDirectoryReader å¼€é”€ï¼‰"""
    # å±è”½å­è¿›ç¨‹ä¸­çš„è­¦å‘Šå’Œæ—¥å¿—
    import warnings
    import logging
    warnings.filterwarnings('ignore')
    logging.getLogger('streamlit').setLevel(logging.ERROR)
    logging.getLogger('pypdf').setLevel(logging.ERROR)
    logging.getLogger('pdfminer').setLevel(logging.ERROR)
    
    fp, fname, ext = file_info
    try:
        size = os.path.getsize(fp)
        
        # æ£€æŸ¥æ ¼å¼æ”¯æŒ
        if ext not in SUPPORTED_FORMATS:
            return None, fname, 'skipped', f"ä¸æ”¯æŒçš„æ ¼å¼: {ext}", 'skip'
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if size > 100 * 1024 * 1024:  # 100MB
            return None, fname, 'skipped', "æ–‡ä»¶è¿‡å¤§ (>100MB)", 'skip'
        
        # ä¼˜åŒ–ï¼šç›´æ¥è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå‡å°‘ SimpleDirectoryReader å¼€é”€
        from llama_index.core import Document
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¿«é€Ÿè¯»å–
        if ext in ['.txt', '.md', '.py', '.js', '.json', '.xml', '.html', '.css', '.yaml', '.yml', '.sh', '.sql', 
                   '.log', '.ini', '.conf', '.cfg', '.csv', '.tsv', '.properties', '.env', '.rst', '.toml']:
            # æ–‡æœ¬æ–‡ä»¶ï¼šç›´æ¥è¯»å–ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
            with open(fp, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
            docs = [Document(text=text, metadata={'file_name': fname, 'file_path': fp})]
            read_mode = 'fast'
        
        elif ext in ['.xlsx', '.xls']:
            # Excelæ–‡ä»¶ï¼šå¿«é€Ÿè¯»å–ï¼ˆåªè¯»æ–‡æœ¬å†…å®¹ï¼Œä¸è§£ææ ¼å¼ï¼‰
            try:
                import openpyxl
                wb = openpyxl.load_workbook(fp, read_only=True, data_only=True)
                text_parts = []
                for sheet in wb.worksheets[:5]:  # åªè¯»å‰5ä¸ªsheet
                    for row in sheet.iter_rows(max_row=1000, values_only=True):  # æ¯ä¸ªsheetæœ€å¤š1000è¡Œ
                        row_text = ' '.join([str(cell) for cell in row if cell is not None])
                        if row_text.strip():
                            text_parts.append(row_text)
                wb.close()
                text = '\n'.join(text_parts)
                docs = [Document(text=text, metadata={'file_name': fname, 'file_path': fp})]
                read_mode = 'fast'
            except:
                # å¤±è´¥åˆ™ç”¨æ…¢é€Ÿæ¨¡å¼
                from llama_index.core import SimpleDirectoryReader
                docs = SimpleDirectoryReader(input_files=[fp]).load_data()
                read_mode = 'slow'
        elif ext in ['.pptx', '.ppt']:
            # PowerPointæ–‡ä»¶ï¼šè¯»å–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            try:
                from pptx import Presentation
                prs = Presentation(fp)
                text_parts = []
                for slide_idx, slide in enumerate(prs.slides):
                    text_parts.append(f"--- å¹»ç¯ç‰‡ {slide_idx + 1} ---")
                    for shape in slide.shapes:
                        if hasattr(shape, "text") and shape.text.strip():
                            text_parts.append(shape.text)
                text = '\n'.join(text_parts)
                docs = [Document(text=text, metadata={'file_name': fname, 'file_path': fp})]
                read_mode = 'fast'
            except Exception as e:
                return None, fname, 'failed', f"PPTXè§£æå¤±è´¥: {str(e)[:50]}", 'slow'
        else:
            # å…¶ä»–æ ¼å¼ï¼šä½¿ç”¨ SimpleDirectoryReaderï¼ˆæ…¢é€Ÿæ¨¡å¼ï¼‰
            from llama_index.core import SimpleDirectoryReader
            docs = SimpleDirectoryReader(input_files=[fp]).load_data()
            read_mode = 'slow'

            # å¦‚æœæ˜¯PDFä¸”å†…å®¹ä¸ºç©ºï¼Œå°è¯• OCRï¼ˆæ‰«æç‰ˆPDFï¼‰
            needs_ocr = False
            if ext == '.pdf' and docs:
                if not docs[0].text or len(docs[0].text.strip()) == 0:
                    needs_ocr = True
            
            if needs_ocr:
                try:
                    from pdf2image import convert_from_path
                    import pytesseract
                    import multiprocessing as mp
                    from concurrent.futures import ProcessPoolExecutor
                    
                    # é™åˆ¶æœ€å¤šå¤„ç†50é¡µï¼ˆå¤šè¿›ç¨‹å¯ä»¥å¤„ç†æ›´å¤šï¼‰
                    max_pages = 50
                    print(f"   ğŸ” æ£€æµ‹åˆ°æ‰«æç‰ˆPDFï¼Œå¯åŠ¨å¤šè¿›ç¨‹OCRè¯†åˆ«ï¼ˆæœ€å¤š{max_pages}é¡µï¼‰...")
                    
                    images = convert_from_path(fp, first_page=1, last_page=max_pages, dpi=200)
                    print(f"   ğŸ“„ å…± {len(images)} é¡µï¼Œä½¿ç”¨ {mp.cpu_count()} è¿›ç¨‹å¹¶è¡ŒOCR...")
                    
                    # å¤šè¿›ç¨‹OCRå‡½æ•°
                    def ocr_page(args):
                        idx, img = args
                        try:
                            text = pytesseract.image_to_string(img, lang='chi_sim+eng')
                            return idx, text.strip() if text else ""
                        except:
                            return idx, ""
                    
                    # å¹¶è¡Œå¤„ç†
                    all_text = [""] * len(images)
                    with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:
                        results = executor.map(ocr_page, enumerate(images, 1))
                        for idx, text in results:
                            if text:
                                all_text[idx-1] = f"--- ç¬¬{idx}é¡µ ---\n{text}"
                    
                    # è¿‡æ»¤ç©ºé¡µ
                    all_text = [t for t in all_text if t]
                    
                    if all_text:
                        full_text = "\n\n".join(all_text)
                        docs = [Document(text=full_text, metadata={'file_name': fname, 'file_path': fp})]
                        read_mode = 'ocr'
                        print(f"   âœ… OCRå®Œæˆ: è¯†åˆ«äº† {len(all_text)}/{len(images)} é¡µ")
                    else:
                        return None, fname, 'failed', f"OCRæœªè¯†åˆ«åˆ°æ–‡å­—ï¼ˆå…±{len(images)}é¡µï¼‰", 'ocr'
                
                except Exception as e:
                    return None, fname, 'failed', f"OCRå¤±è´¥: {str(e)[:50]}", 'ocr'
        
        if docs:
            # è¿‡æ»¤æ‰ç©ºæ–‡æ¡£
            docs = [d for d in docs if d.text and d.text.strip()]
            if docs:
                return docs, fname, 'success', (size, len(docs)), read_mode
            else:
                return None, fname, 'failed', "æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼ˆæ‰€æœ‰æ–‡æ¡£éƒ½æ˜¯ç©ºçš„ï¼‰", read_mode
        else:
            return None, fname, 'failed', "æ–‡ä»¶å†…å®¹ä¸ºç©º", read_mode
    
    except Exception as e:
        error_msg = str(e)
        # ç®€åŒ–å¸¸è§é”™è¯¯ä¿¡æ¯
        if "trailer cannot be read" in error_msg or "invalid literal" in error_msg:
            error_msg = "PDFæ–‡ä»¶æŸå"
        elif "not a zip file" in error_msg.lower():
            error_msg = "DOCXæ–‡ä»¶æŸå"
        elif "RetryError" in error_msg or "AttributeError" in error_msg:
            error_msg = "æ–‡ä»¶è§£æå¤±è´¥"
        elif "Unsupported" in error_msg:
            error_msg = "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"
        return None, fname, 'failed', error_msg[:100]


# æ‰¹é‡å¤„ç†å‡½æ•°ï¼ˆæ¨¡å—çº§åˆ«ï¼Œç”¨äºå¤šè¿›ç¨‹ï¼‰
def _process_batch(batch_files):
    """æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼ˆåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œï¼‰"""
    batch_results = []
    for file_info in batch_files:
        result = _load_single_file(file_info)
        batch_results.append(result)
    return batch_results


def scan_directory_safe(input_dir: str) -> Tuple[List, 'FileProcessResult']:
    """
    å®‰å…¨æ‰«æç›®å½•ï¼Œè¿”å›æˆåŠŸåŠ è½½çš„æ–‡æ¡£å’Œå¤„ç†ç»“æœï¼ˆå¤šçº¿ç¨‹å¹¶è¡Œï¼‰
    
    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
    
    Returns:
        (documents, result) - æ–‡æ¡£åˆ—è¡¨å’Œå¤„ç†ç»“æœ
    """
    from llama_index.core import SimpleDirectoryReader
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import multiprocessing as mp
    
    result = FileProcessResult()
    all_docs = []
    
    # ç¬¬ä¸€æ­¥ï¼šå¹¶è¡Œæ‰«ææ‰€æœ‰æ–‡ä»¶ï¼ˆä¼˜åŒ–ï¼šå¤šçº¿ç¨‹åŠ é€Ÿï¼‰
    print(f"ğŸ“ [ç¬¬ 2 æ­¥] å¹¶è¡Œæ‰«æç›®å½•: {input_dir}")
    file_list = []
    
    # è·å–æ‰€æœ‰å­ç›®å½•
    subdirs = [input_dir]
    try:
        subdirs.extend([os.path.join(input_dir, d) for d in os.listdir(input_dir) 
                       if os.path.isdir(os.path.join(input_dir, d)) and not d.startswith('.')])
    except:
        pass
    
    # å¹¶è¡Œæ‰«æå‡½æ•°
    def scan_dir(directory):
        local_files = []
        try:
            for root, _, filenames in os.walk(directory):
                for f in filenames:
                    if not f.startswith('.'):
                        fp = os.path.join(root, f)
                        ext = Path(f).suffix.lower()
                        local_files.append((fp, f, ext))
        except Exception as e:
            print(f"   æ‰«æå¤±è´¥ {directory}: {e}")
        return local_files
    
    # å¤šçº¿ç¨‹å¹¶è¡Œæ‰«æï¼ˆæé™é…ç½®ï¼š250 çº¿ç¨‹ï¼Œå†²åˆº 80% èµ„æºï¼‰
    if len(subdirs) > 1:
        from concurrent.futures import ThreadPoolExecutor, as_completed
        scan_workers = min(250, len(subdirs))  # 250 çº¿ç¨‹
        print(f"âš¡ [ç¬¬ 2 æ­¥] æé™æ¨¡å¼ï¼š{scan_workers} çº¿ç¨‹å¹¶è¡Œæ‰«æ {len(subdirs)} ä¸ªç›®å½•ï¼ˆå†²åˆº 80% èµ„æºï¼‰")
        
        with ThreadPoolExecutor(max_workers=scan_workers) as executor:
            futures = [executor.submit(scan_dir, d) for d in subdirs]
            for future in as_completed(futures):
                file_list.extend(future.result())
    else:
        # å•ç›®å½•ç›´æ¥æ‰«æ
        file_list = scan_dir(input_dir)
    
    print(f"âœ… [ç¬¬ 2 æ­¥] æ‰«æå®Œæˆ: å‘ç° {len(file_list)} ä¸ªæ–‡ä»¶")
    
    # ç¬¬äºŒæ­¥ï¼šå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼ˆåŠ¨æ€è°ƒåº¦ï¼Œä¿æŒèµ„æº < 80%ï¼‰
    import psutil
    import time as time_module
    from queue import Queue
    from threading import Semaphore
    
    # åˆå§‹çº¿ç¨‹æ•°ï¼ˆæé™é…ç½®ï¼š250 çº¿ç¨‹ï¼Œå†²åˆº 80%ï¼‰
    # åŠ¨æ€è®¡ç®—æœ€ä¼˜é…ç½®
    fast_formats = {'.txt', '.md', '.py', '.js', '.json', '.xml', '.html', '.css', '.yaml', '.yml', '.sh', '.sql',
                   '.log', '.ini', '.conf', '.cfg', '.csv', '.tsv', '.properties', '.env', '.rst', '.toml',
                   '.xlsx', '.xls'}  # Excelä¹ŸåŠ å…¥å¿«é€Ÿè¯»å–
    fast_count = sum(1 for _, _, ext in file_list if ext in fast_formats)
    fast_ratio = fast_count / len(file_list) if len(file_list) > 0 else 0
    
    # æ ¹æ®æ–‡ä»¶ç±»å‹ä¼˜åŒ–é…ç½®
    if fast_ratio > 0.5:
        # æ–‡æœ¬æ–‡ä»¶å¤šï¼šé«˜å¹¶å‘ï¼Œå¤§æ‰¹é‡
        max_workers = min(500, mp.cpu_count() * 50, len(file_list))
        batch_size = 75  # å¹³è¡¡æ‰¹é‡
        mode_name = "é«˜å¹¶å‘"
    elif fast_ratio > 0.3:
        # æ··åˆæ–‡ä»¶ï¼šå¹³è¡¡æ¨¡å¼
        max_workers = min(300, mp.cpu_count() * 30, len(file_list))
        batch_size = 40
        mode_name = "å¹³è¡¡"
    else:
        # PDF/DOCXå¤šï¼šä¿å®ˆæ¨¡å¼
        max_workers = min(250, mp.cpu_count() * 25, len(file_list))
        batch_size = 25
        mode_name = "é‡æ–‡ä»¶ä¼˜åŒ–"
    
    max_workers = int(max_workers)
    
    if max_workers > 1 and len(file_list) > 10:
        # batch_sizeå·²åœ¨ä¸Šé¢åŠ¨æ€è®¡ç®—ï¼Œè¿™é‡Œä¸å†é‡å¤å®šä¹‰
        
        print(f"ğŸš€ [ç¬¬ 3 æ­¥] {mode_name}æ¨¡å¼: {max_workers} è¿›ç¨‹ | æ–‡æœ¬å æ¯”: {fast_ratio*100:.1f}%")
        print(f"ğŸ“¦ [ç¬¬ 3 æ­¥] æ‰¹é‡å¤§å°: {batch_size} ä¸ª/æ‰¹ | å¤šè¿›ç¨‹çªç ´GIL")
        
        # ä½¿ç”¨å¤šè¿›ç¨‹ï¼ˆçªç ´GILé™åˆ¶ï¼‰
        import multiprocessing as mp
        import time as time_module
        
        # è®¾ç½®å¯åŠ¨æ–¹æ³•ä¸ºforkï¼ˆmacOSé»˜è®¤ï¼Œé¿å…é‡æ–°å¯¼å…¥ï¼‰
        try:
             mp.set_start_method('fork', force=True)
        except RuntimeError:
             pass
        
        # é™åˆ¶è¿›ç¨‹æ•°
        actual_workers = min(max_workers, int(mp.cpu_count() * 1.2))
        print(f"ğŸ’» [ç¬¬ 3 æ­¥] ä½¿ç”¨ {actual_workers} ä¸ªè¿›ç¨‹ï¼ˆCPU: {mp.cpu_count()}æ ¸ï¼Œç›®æ ‡<80%ï¼‰")
        
        # ç»Ÿè®¡å¿«é€Ÿ/æ…¢é€Ÿè¯»å–
        fast_count = 0
        slow_count = 0
        
        # å°†æ–‡ä»¶åˆ—è¡¨åˆ†æ‰¹
        batches = [file_list[i:i + batch_size] for i in range(0, len(file_list), batch_size)]
        print(f"ğŸ“Š [ç¬¬ 3 æ­¥] æ€»è®¡ {len(file_list)} ä¸ªæ–‡ä»¶ï¼Œåˆ†æˆ {len(batches)} æ‰¹")
        
        start_time = time_module.time()
        completed = 0
        
        # ä½¿ç”¨è¿›ç¨‹æ± 
        with mp.Pool(processes=actual_workers) as pool:
            # ä½¿ç”¨imap_unorderedè·å–ç»“æœ
            for batch_results in pool.imap_unordered(_process_batch, batches, chunksize=1):
                try:
                    for file_result in batch_results:
                        docs, fname, status, info, read_mode = file_result if len(file_result) == 5 else (*file_result, 'unknown')
                        completed += 1
                        
                        # ç»Ÿè®¡è¯»å–æ¨¡å¼
                        if read_mode == 'fast':
                            fast_count += 1
                        elif read_mode == 'slow':
                            slow_count += 1
                        
                        if status == 'success':
                            all_docs.extend(docs)
                            size, doc_count = info
                            result.add_success(fname, size, doc_count)
                        elif status == 'skipped':
                            result.add_skipped(fname, info)
                        else:  # failed
                            result.add_failed(fname, info)
                    
                    # æ˜¾ç¤ºè¿›åº¦å’Œç»Ÿè®¡ï¼ˆæ¯200ä¸ªæ–‡ä»¶ï¼‰
                    if completed % 200 == 0:
                        elapsed = time_module.time() - start_time
                        speed = completed / elapsed if elapsed > 0 else 0
                        remaining = len(file_list) - completed
                        eta_seconds = remaining / speed if speed > 0 else 0
                        eta_minutes = eta_seconds / 60
                        
                        progress_pct = completed / len(file_list) * 100
                        print(f"ğŸ“Š [ç¬¬ 3 æ­¥] {completed}/{len(file_list)} ({progress_pct:.1f}%) | è¿›ç¨‹: {actual_workers}")
                        print(f"   âš¡ å¿«é€Ÿ: {fast_count} | ğŸŒ æ…¢é€Ÿ: {slow_count} | é€Ÿåº¦: {speed:.1f} æ–‡ä»¶/ç§’ | â±ï¸  é¢„è®¡å‰©ä½™: {eta_minutes:.1f} åˆ†é’Ÿ")
                    
                    # æ¯å¤„ç†50ä¸ªæ–‡ä»¶æ‰“å°ç®€å•è¿›åº¦
                    if completed % 50 == 0:
                        print(f"   å·²è¯»å–: {completed}/{len(file_list)}")
                
                except Exception as e:
                    print(f"   æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
        
        
        # æœ€ç»ˆç»Ÿè®¡
        print(f"\nâœ… [ç¬¬ 3 æ­¥] æ–‡ä»¶è¯»å–å®Œæˆ:")
        print(f"   âš¡ å¿«é€Ÿè¯»å– (ç›´æ¥): {fast_count} ä¸ªæ–‡ä»¶")
        print(f"   ğŸŒ æ…¢é€Ÿè¯»å– (è§£æ): {slow_count} ä¸ªæ–‡ä»¶")
        if fast_count + slow_count > 0:
            print(f"   ğŸ“ˆ å¿«é€Ÿå æ¯”: {fast_count/(fast_count+slow_count)*100:.1f}%")
    
    else:
        # å•æ ¸æ¨¡å¼ï¼ˆæ–‡ä»¶å°‘æ—¶ï¼‰
        for fp, fname, ext in file_list:
            try:
                size = os.path.getsize(fp)
                
                if ext not in SUPPORTED_FORMATS:
                    result.add_skipped(fname, f"ä¸æ”¯æŒçš„æ ¼å¼: {ext}")
                    continue
                
                if size > 100 * 1024 * 1024:
                    result.add_skipped(fname, "æ–‡ä»¶è¿‡å¤§ (>100MB)")
                    continue
                
                docs = SimpleDirectoryReader(input_files=[fp]).load_data()
                if docs:
                    all_docs.extend(docs)
                    result.add_success(fname, size, len(docs))
                else:
                    result.add_failed(fname, "æ–‡ä»¶å†…å®¹ä¸ºç©º")
            
            except Exception as e:
                result.add_failed(fname, str(e)[:100])
    
    return all_docs, result

# ==========================================
# å¤šè¿›ç¨‹å¤„ç†å‡½æ•° (ä» apppro.py ç§»åŠ¨è‡³æ­¤)
# ==========================================

def _parse_single_doc(doc_text):
    """å•ä¸ªæ–‡æ¡£è§£æï¼ˆå¤šè¿›ç¨‹å®‰å…¨ï¼‰- è¿”å›å­—å…¸è€Œéå¯¹è±¡"""
    import warnings
    warnings.filterwarnings('ignore')
    
    # æ–‡æœ¬åˆ†å‰² + åŸºç¡€å¤„ç†ï¼ˆä¼˜åŒ–ï¼šå¢å¤§ chunk_size å‡å°‘èŠ‚ç‚¹æ•°ï¼‰
    chunk_size = 1024  # ä» 512 å¢åŠ åˆ° 1024
    chunk_overlap = 100  # ç›¸åº”å¢åŠ  overlap
    chunks = []
    
    # é¢„å¤„ç†ï¼šæ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬
    doc_text = doc_text.strip()
    lines = doc_text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            line = ' '.join(line.split())
            cleaned_lines.append(line)
    
    cleaned_text = '\n'.join(cleaned_lines)
    
    # åˆ†å—å¤„ç†
    for i in range(0, len(cleaned_text), chunk_size - chunk_overlap):
        chunk = cleaned_text[i:i + chunk_size]
        if chunk.strip():
            word_count = len(chunk.split())
            char_count = len(chunk)
            
            chunks.append({
                'text': chunk,
                'start_idx': i,
                'word_count': word_count,
                'char_count': char_count
            })
    
    return chunks

def _parse_batch_docs(doc_texts_batch):
    """æ‰¹é‡å¤„ç†æ–‡æ¡£ï¼ˆå‡å°‘è¿›ç¨‹é—´é€šä¿¡ï¼‰"""
    all_chunks = []
    for doc_text in doc_texts_batch:
        chunks = _parse_single_doc(doc_text)
        all_chunks.extend(chunks)
    return all_chunks


# ==========================================
# å‘é‡åŒ–å¤„ç†å‡½æ•° (å¤šè¿›ç¨‹)
# ==========================================

def _generate_embeddings_worker(task):
    """
    ç”Ÿæˆ embeddings çš„å·¥ä½œè¿›ç¨‹
    task: (model_name, texts_batch, device)
    """
    import os
    # å±è”½æ—¥å¿—
    import logging
    logging.getLogger('sentence_transformers').setLevel(logging.ERROR)
    
    model_name, texts, device = task
    
    try:
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
        
        # ä¼˜åŒ–ï¼šä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œé¿å…é‡å¤ä¸‹è½½
        embed_model = HuggingFaceEmbedding(
            model_name=model_name,
            cache_folder="./hf_cache",
            device=device,
            embed_batch_size=256  # å†…éƒ¨æ‰¹å¤„ç†
        )
        
        # è·å– embeddings
        embeddings = []
        for text in texts:
            embeddings.append(embed_model.get_text_embedding(text))
            
        return embeddings
    except Exception as e:
        return [None] * len(texts)  # è¿”å›ç©ºä»¥æ ‡è®°å¤±è´¥
