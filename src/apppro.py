import os
import sys

# åœ¨å¯¼å…¥ä»»ä½•å…¶ä»–æ¨¡å—ä¹‹å‰è®¾ç½®ç¦»çº¿æ¨¡å¼
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # é¿å…å¤šè¿›ç¨‹forkè­¦å‘Š

# å½»åº•å±è”½æ‰€æœ‰è­¦å‘Šå’Œæ—¥å¿—
import warnings
import logging

# å±è”½æ‰€æœ‰è­¦å‘Š
warnings.filterwarnings('ignore')

# å±è”½æ‰€æœ‰ Streamlit ç›¸å…³æ—¥å¿—
for logger_name in ['streamlit', 'streamlit.runtime', 'streamlit.runtime.scriptrunner_utils', 
                     'streamlit.runtime.scriptrunner_utils.script_run_context']:
    logging.getLogger(logger_name).setLevel(logging.ERROR)
    logging.getLogger(logger_name).propagate = False

# é‡å®šå‘ stderr ä¸­çš„è­¦å‘Šï¼ˆæœ€å½»åº•çš„æ–¹å¼ï¼‰
class SuppressWarnings:
    def write(self, text):
        if 'ScriptRunContext' not in text and 'WARNING' not in text:
            sys.__stderr__.write(text)
    def flush(self):
        sys.__stderr__.flush()

sys.stderr = SuppressWarnings()

# LlamaIndex ç‰ˆæœ¬å…¼å®¹æ€§è¡¥ä¸ï¼ˆåœ¨å¯¼å…¥å‰ï¼‰
import llama_index.core.schema as schema_module
original_textnode = schema_module.TextNode

class PatchedTextNode(original_textnode):
    def get_doc_id(self):
        return self.ref_doc_id or self.node_id

schema_module.TextNode = PatchedTextNode

import streamlit as st
import shutil
import time
import requests
import ollama
import re
import json
import zipfile
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import multiprocessing as mp

# å¼•å…¥ LlamaIndex æ ¸å¿ƒ
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext, load_index_from_storage
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.ollama import Ollama
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.schema import Document

# å¯¼å…¥è‡ªå®šä¹‰åµŒå…¥
from src.custom_embeddings import create_custom_embedding

# å¼•å…¥æ—¥å¿—æ¨¡å—
from src.logger import logger
from src.terminal_logger import terminal_logger
from src.chat_utils_improved import generate_follow_up_questions_safe as generate_follow_up_questions

# å¼•å…¥å…ƒæ•°æ®ç®¡ç†
from src.metadata_manager import MetadataManager

# å¼•å…¥å·¥å…·æ¨¡å—
from src.utils.memory import cleanup_memory, get_memory_stats
from src.utils.model_manager import (
    load_embedding_model,
    load_llm_model,
    set_global_embedding_model,
    set_global_llm_model
)
from src.utils.document_processor import (
    sanitize_filename,
    get_file_size_str,
    get_file_type,
    get_file_info,
    get_relevance_label,
    load_pptx_file
)
from src.utils.config_manager import (
    load_config,
    save_config,
    load_manifest,
    update_manifest,
    get_manifest_path
)
from src.utils.chat_manager import (
    load_chat_history,
    save_chat_history,
    clear_chat_history
)
from src.utils.kb_manager import (
    rename_kb,
    get_existing_kbs,
    delete_kb,
    auto_save_kb_info,
    get_kb_info
)

# å¼•å…¥ RAG å¼•æ“
from src.rag_engine import RAGEngine

# å¼•å…¥èµ„æºç›‘æ§å’Œæ¨¡å‹å·¥å…·
from src.utils.resource_monitor import check_resource_usage, get_system_stats
from src.utils.model_utils import (
    check_ollama_status,
    fetch_remote_models,
    check_hf_model_exists,
    get_kb_embedding_dim,
    auto_switch_model,
    get_model_dimension
)

# å¼•å…¥ UI å±•ç¤ºç»„ä»¶ (Stage 3.1)
from src.ui.display_components import (
    render_message_stats,
    render_source_references,
    get_relevance_label
)

# å¼•å…¥ UI æ¨¡å‹é€‰æ‹©å™¨ (Stage 3.2.1)
from src.ui.model_selectors import (
    render_ollama_model_selector,
    render_openai_model_selector,
    render_hf_embedding_selector
)

# âš ï¸ å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œé¿å… OpenAI é»˜è®¤
# ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®© LlamaIndex ä½¿ç”¨æœ¬åœ°æ¨¡å‹
os.environ['LLAMA_INDEX_EMBED_MODEL'] = 'local'

# å…¼å®¹æ—§ä»£ç çš„åŒ…è£…å‡½æ•°
def get_embed(provider, model, key, url):
    """å…¼å®¹æ—§ä»£ç çš„åŒ…è£…å‡½æ•°"""
    return load_embedding_model(provider, model, key, url)

def get_llm(provider, model, key, url, temp):
    """å…¼å®¹æ—§ä»£ç çš„åŒ…è£…å‡½æ•°"""
    return load_llm_model(provider, model, key, url, temp)

def _process_node_worker(args):
    """å¤šè¿›ç¨‹å¤„ç†å•ä¸ªèŠ‚ç‚¹"""
    node_data, kb_name = args
    try:
        metadata = node_data.get('metadata', {})
        file_name = metadata.get('file_name', 'Unknown')
        score = node_data.get('score', 0.0)
        text = node_data.get('text', '')
        
        return {
            "file": file_name, 
            "score": score, 
            "text": text[:150].replace("\n", " ") + "..."
        }
    except:
        return None

# å¼•å…¥æ–‡ä»¶å¤„ç†æ¨¡å—
from src.file_processor import scan_directory_safe

# å¤šè¿›ç¨‹å‡½æ•°ï¼šå…ƒæ•°æ®æå–ï¼ˆç§»åˆ°æ¨¡å—çº§åˆ«ï¼‰
def _extract_metadata_task(task):
    """å•ä¸ªæ–‡ä»¶çš„å…ƒæ•°æ®æå–ä»»åŠ¡ï¼ˆå¤šè¿›ç¨‹å®‰å…¨ï¼‰"""
    fp, fname, doc_ids, text_sample, persist_dir = task
    temp_mgr = MetadataManager(persist_dir)
    return fname, temp_mgr.add_file_metadata(fp, doc_ids, text_sample)

# å¤šè¿›ç¨‹å‡½æ•°ï¼šæ–‡æ¡£åˆ†å—è§£æï¼ˆç§»åˆ°æ¨¡å—çº§åˆ«ï¼‰
def _parse_single_doc(doc_text):
    """å•ä¸ªæ–‡æ¡£è§£æï¼ˆå¤šè¿›ç¨‹å®‰å…¨ï¼‰- è¿”å›å­—å…¸è€Œéå¯¹è±¡"""
    import warnings
    warnings.filterwarnings('ignore')
    
    # æ–‡æœ¬åˆ†å‰² + åŸºç¡€å¤„ç†ï¼ˆä¼˜åŒ–ï¼šå¢å¤§ chunk_size å‡å°‘èŠ‚ç‚¹æ•°ï¼‰
    chunk_size = 1024  # ä» 512 å¢åŠ åˆ° 1024
    chunk_overlap = 100  # ç›¸åº”å¢åŠ  overlap
    chunks = []
    
    # é¢„å¤„ç†ï¼šæ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬
    doc_text = doc_text.strip()
    lines = doc_text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            line = ' '.join(line.split())
            cleaned_lines.append(line)
    
    cleaned_text = '\n'.join(cleaned_lines)
    
    # åˆ†å—å¤„ç†
    for i in range(0, len(cleaned_text), chunk_size - chunk_overlap):
        chunk = cleaned_text[i:i + chunk_size]
        if chunk.strip():
            word_count = len(chunk.split())
            char_count = len(chunk)
            
            chunks.append({
                'text': chunk,
                'start_idx': i,
                'word_count': word_count,
                'char_count': char_count
            })
    
    return chunks

def _parse_batch_docs(doc_texts_batch):
    """æ‰¹é‡å¤„ç†æ–‡æ¡£ï¼ˆå‡å°‘è¿›ç¨‹é—´é€šä¿¡ï¼‰"""
    all_chunks = []
    for doc_text in doc_texts_batch:
        chunks = _parse_single_doc(doc_text)
        all_chunks.extend(chunks)
    return all_chunks

# ==========================================
# 1. é¡µé¢é…ç½®ä¸æ ·å¼
# ==========================================
st.set_page_config(
    page_title="RAG Pro Max (æ——èˆ°ç‰ˆ)", 
    page_icon="ğŸ›¡ï¸", 
    layout="wide", 
    initial_sidebar_state="expanded"
)

# æ³¨å…¥ CSS
st.markdown("""
<style>
    /* ç¦ç”¨ spinner é®ç½©å±‚ */
    .stSpinner > div {
        border: none !important;
        background-color: transparent !important;
    }
    div[data-testid="stStatusWidget"] {
        background-color: transparent !important;
    }
    
    /* ä¾§è¾¹æ é¡¶éƒ¨å®Œå…¨æ— ç©ºç™½ - æ¿€è¿›ç‰ˆæœ¬ */
    section[data-testid="stSidebar"] {
        padding-top: 0rem !important;
    }
    section[data-testid="stSidebar"] > div {
        padding-top: 0rem !important;
        margin-top: 0rem !important;
    }
    section[data-testid="stSidebar"] .block-container {
        padding-top: 0rem !important;
        margin-top: 0rem !important;
    }
    section[data-testid="stSidebar"] [data-testid="stVerticalBlock"] {
        padding-top: 0rem !important;
        margin-top: 0rem !important;
        gap: 0.5rem !important;
    }
    section[data-testid="stSidebar"] [data-testid="stVerticalBlock"] > div:first-child {
        padding-top: 0rem !important;
        margin-top: 0rem !important;
    }
    
    /* æœ€å°åŒ–é¡¶éƒ¨ç©ºç™½ */
    .block-container {
        padding-top: 1rem !important;
        padding-bottom: 0.5rem !important;
    }
    
    /* ç´§å‡‘æ ‡é¢˜ */
    h3, h4 {
        margin-top: 0 !important;
        margin-bottom: 0.5rem !important;
        padding-top: 0 !important;
        line-height: 1.2 !important;
    }
    
    /* è¶…ç´§å‡‘æŒ‡æ ‡å¡ç‰‡ */
    [data-testid="stMetricValue"] {
        font-size: 1.1rem !important;
    }
    [data-testid="stMetricLabel"] {
        font-size: 0.75rem !important;
        margin-bottom: 0 !important;
    }
    [data-testid="metric-container"] {
        padding: 0.3rem 0 !important;
    }
    
    /* æŒ‰é’®æ ·å¼ä¼˜åŒ– */
    div.stButton > button {
        background-color: transparent !important;
        border: 1px solid rgba(128, 128, 128, 0.5) !important;
        color: inherit !important;
        border-radius: 6px !important;
        padding: 0.3rem 0.6rem !important;
        transition: all 0.3s ease;
        line-height: 1.2;
        text-align: center;
        white-space: nowrap !important;
    }
    div.stButton > button:hover {
        border-color: #FF4B4B !important;
        color: #FF4B4B !important;
        background-color: rgba(255, 75, 75, 0.05) !important;
    }
    
    /* è¾“å…¥æ¡†å’Œä¸‹æ‹‰æ¡† - ç¡®ä¿æ–‡å­—å®Œæ•´æ˜¾ç¤º */
    .stTextInput > div > div > input,
    .stSelectbox > div > div > div {
        border-radius: 6px;
        padding: 0.4rem 0.8rem !important;
        font-size: 0.9rem !important;
        white-space: nowrap !important;
        overflow: visible !important;
    }
    
    /* ä¸‹æ‹‰æ¡†é€‰é¡¹å®Œæ•´æ˜¾ç¤º */
    .stSelectbox label {
        font-size: 0.9rem !important;
        font-weight: 500 !important;
    }
    
    /* å‡å°‘åˆ—é—´è·ä½†ä¿æŒå¯è¯»æ€§ */
    [data-testid="column"] {
        padding: 0 0.4rem !important;
    }
    
    /* ä¾§è¾¹æ æ–‡ä»¶åˆ—è¡¨ */
    .file-item {
        font-size: 12px; 
        padding: 5px 8px; 
        background: rgba(128,128,128,0.1); 
        border-radius: 6px; 
        margin-bottom: 3px;
        display: flex;
        justify-content: space-between;
        align-items: center;
    }
    .file-name { font-weight: 500; max-width: 70%; overflow: hidden; text-overflow: ellipsis; white-space: nowrap; }
    .file-meta { font-size: 10px; opacity: 0.7; }
    
    /* æ¬¢è¿é¡µå¡ç‰‡ */
    .welcome-box {
        padding: 20px;
        border-radius: 10px;
        background-color: rgba(128, 128, 128, 0.05);
        border: 1px solid rgba(128, 128, 128, 0.2);
        margin-bottom: 15px;
        text-align: center;
    }
    
    /* å‡å°‘expanderé—´è· */
    .streamlit-expanderHeader {
        padding: 0.4rem 0.8rem !important;
    }
    
    /* å‡å°‘captioné—´è· */
    .stCaption {
        margin-top: 0.2rem !important;
        margin-bottom: 0.2rem !important;
    }
</style>
""", unsafe_allow_html=True)

# åº”ç”¨å¯åŠ¨æ—¥å¿—
if 'app_initialized' not in st.session_state:
    terminal_logger.separator("RAG Pro Max å¯åŠ¨")
    terminal_logger.info("åº”ç”¨åˆå§‹åŒ–ä¸­...")
    st.session_state.app_initialized = True
    terminal_logger.success("åº”ç”¨åˆå§‹åŒ–å®Œæˆ")

# ==========================================
# 2. æœ¬åœ°æŒä¹…åŒ–ä¸å·¥å…·å‡½æ•°
# ==========================================
CONFIG_FILE = "rag_config.json"
HISTORY_DIR = "chat_histories"
UPLOAD_DIR = "temp_uploads" # ä¸´æ—¶ä¸Šä¼ ç›®å½•

# ç¡®ä¿ç›®å½•å­˜åœ¨
for d in [HISTORY_DIR, UPLOAD_DIR]:
    if not os.path.exists(d): os.makedirs(d)

defaults = load_config()

def fetch_remote_models(base_url, api_key):
    if not base_url: return None, "è¯·å¡«å†™ Base URL"
    clean_url = base_url.rstrip('/')
    endpoints = [f"{clean_url}/models", f"{clean_url}/v1/models"]
    headers = {"Authorization": f"Bearer {api_key}" if api_key else "Bearer EMPTY"}
    for url in endpoints:
        try:
            resp = requests.get(url, headers=headers, timeout=5)
            if resp.status_code == 200:
                data = resp.json()
                if "data" in data and isinstance(data['data'], list):
                    return [item['id'] for item in data['data']], None
        except Exception as e: 
            return None, f"è¿æ¥å¤±è´¥æˆ–APIé”™è¯¯: {e}"
    return None, "æœªæ‰¾åˆ°æ¨¡å‹åˆ—è¡¨æˆ–è·¯å¾„é”™è¯¯"

# --- 3. æ ¸å¿ƒåˆå§‹åŒ– (å¸¦ç¼“å­˜) ---
def check_hf_model_exists(model_name):
    """æ£€æŸ¥ HuggingFace æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½åˆ°æœ¬åœ°"""
    cache_dir = "./hf_cache"
    
    # æ–¹å¼1: ç›´æ¥ç›®å½•æ ¼å¼ (BAAI--bge-large-zh-v1.5)
    model_dir1 = os.path.join(cache_dir, model_name.replace('/', '--'))
    if os.path.exists(os.path.join(model_dir1, "config.json")):
        return True
    
    # æ–¹å¼2: HF Hub ç¼“å­˜æ ¼å¼ (models--BAAI--bge-small-zh-v1.5)
    model_dir2 = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
    if os.path.exists(model_dir2):
        return True
    
    return False

def get_kb_embedding_dim(db_path):
    """æ£€æµ‹çŸ¥è¯†åº“çš„å‘é‡ç»´åº¦ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    # 1. å°è¯•ä»ç¼“å­˜è·å–
    if 'kb_dimensions' not in st.session_state:
        st.session_state.kb_dimensions = {}
    
    # ä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä½œä¸ºç¼“å­˜é”®çš„ä¸€éƒ¨åˆ†ï¼Œç¡®ä¿çŸ¥è¯†åº“æ›´æ–°åç¼“å­˜å¤±æ•ˆ
    kb_cache_key = f"{os.path.basename(db_path)}_dim"
    try:
        kb_info_file = os.path.join(db_path, ".kb_info.json")
        if os.path.exists(kb_info_file):
            mtime = os.path.getmtime(kb_info_file)
            kb_cache_key = f"{os.path.basename(db_path)}_dim_{mtime}"
            
            # æ¸…ç†æ—§ç¼“å­˜
            keys_to_remove = [k for k in st.session_state.kb_dimensions if k.startswith(f"{os.path.basename(db_path)}_dim") and k != kb_cache_key]
            for k in keys_to_remove:
                del st.session_state.kb_dimensions[k]
    except:
        pass

    if kb_cache_key in st.session_state.kb_dimensions:
        return st.session_state.kb_dimensions[kb_cache_key]

    print(f"ğŸ” å¼€å§‹æ£€æµ‹ç»´åº¦: {db_path}")
    
    try:
        # æ–¹æ³•0: å…ˆæ£€æŸ¥ä¿å­˜çš„ KB ä¿¡æ¯
        import json
        kb_info_file = os.path.join(db_path, ".kb_info.json")
        if os.path.exists(kb_info_file):
            try:
                with open(kb_info_file, 'r') as f:
                    kb_info = json.load(f)
                    if 'embedding_dim' in kb_info:
                        dim = kb_info['embedding_dim']
                        model = kb_info.get('embedding_model', 'unknown')
                        print(f"âœ… ä» KB ä¿¡æ¯è¯»å–ç»´åº¦: {dim}D (æ¨¡å‹: {model})")
                        st.session_state.kb_dimensions[kb_cache_key] = dim
                        return dim
            except Exception as e:
                print(f"âš ï¸ è¯»å– KB ä¿¡æ¯å¤±è´¥: {e}")
        
        # æ–¹æ³•1: ç›´æ¥ä» ChromaDB è¯»å–ç»´åº¦
        import chromadb
        try:
            client = chromadb.PersistentClient(path=db_path)
            collections = client.list_collections()
            print(f"ğŸ“¦ æ‰¾åˆ° {len(collections)} ä¸ªé›†åˆ")
            
            if collections:
                col = client.get_collection(collections[0].name)
                data = col.get(limit=1, include=['embeddings'])
                if data['embeddings'] and len(data['embeddings']) > 0:
                    dim = len(data['embeddings'][0])
                    print(f"âœ… ChromaDB æ£€æµ‹åˆ°ç»´åº¦: {dim}D")
                    st.session_state.kb_dimensions[kb_cache_key] = dim
                    return dim
        except Exception as e:
            print(f"âš ï¸ ChromaDB æ£€æµ‹å¤±è´¥: {e}")
        
        # æ–¹æ³•2: æ£€æŸ¥ vector_store.json
        vector_store_path = os.path.join(db_path, "vector_store.json")
        if os.path.exists(vector_store_path):
            print(f"ğŸ“„ æ£€æŸ¥ vector_store.json...")
            with open(vector_store_path, 'r') as f:
                data = json.load(f)
                if 'embedding_dict' in data and data['embedding_dict']:
                    first_embedding = next(iter(data['embedding_dict'].values()))
                    if isinstance(first_embedding, list):
                        dim = len(first_embedding)
                        print(f"âœ… JSON æ£€æµ‹åˆ°ç»´åº¦: {dim}D")
                        st.session_state.kb_dimensions[kb_cache_key] = dim
                        return dim
        else:
            print(f"âŒ vector_store.json ä¸å­˜åœ¨")
        
    except Exception as e:
        print(f"âŒ ç»´åº¦æ£€æµ‹å¼‚å¸¸: {e}")
    
    print(f"âŒ æ— æ³•æ£€æµ‹ç»´åº¦")
    return None



def generate_doc_summary(doc_text, filename):
    """
    ç”Ÿæˆå•ä¸ªæ–‡æ¡£çš„æ‘˜è¦ï¼Œä½¿ç”¨å½“å‰çš„ LLM è®¾ç½®ã€‚
    """
    # å±è”½å¤šçº¿ç¨‹è­¦å‘Š
    import warnings
    import logging
    warnings.filterwarnings('ignore')
    logging.getLogger('streamlit').setLevel(logging.ERROR)
    
    if not hasattr(Settings, 'llm'): return "æ€»ç»“å¤±è´¥: LLMæœªåˆå§‹åŒ–"
    try:
        llm = Settings.llm
        summary_prompt = (
            f"ä»¥ä¸‹æ˜¯æ–‡æ¡£ '{filename}' çš„ä¸€ä¸ªç‰‡æ®µå†…å®¹ï¼Œè¯·ç”¨ä¸€æ®µç®€çŸ­çš„ä¸­æ–‡è¯æ€»ç»“å…¶æ ¸å¿ƒå†…å®¹ (ä¸è¶…è¿‡ 80 å­—)ï¼Œç”¨äºæ–‡ä»¶æ¸…å•é¢„è§ˆã€‚å†…å®¹:\n---\n{doc_text[:2000]}..."
        )
        response = llm.complete(summary_prompt)
        return response.text.strip().replace('\n', ' ')\
                             .replace('æ€»ç»“:', '').replace('æ€»ç»“æ˜¯ï¼š', '').strip()
        
    except Exception as e:
        return f"æ€»ç»“å¤±è´¥: {str(e)}"

with st.sidebar:
    # P0æ”¹è¿›1: å¿«é€Ÿå¼€å§‹æ¨¡å¼
    st.markdown("### âš¡ å¿«é€Ÿå¼€å§‹")
    
    if st.button("âš¡ ä¸€é”®é…ç½®ï¼ˆæ¨èæ–°æ‰‹ï¼‰", type="primary", use_container_width=True, help="è‡ªåŠ¨é…ç½®é»˜è®¤è®¾ç½®ï¼Œ1åˆ†é’Ÿå¼€å§‹ä½¿ç”¨"):
        # è‡ªåŠ¨é…ç½® Ollama
        config = load_config()
        config["llm_provider"] = "Ollama"
        config["llm_url_ollama"] = "http://localhost:11434"
        config["llm_model_ollama"] = "qwen2.5:7b"
        
        # è‡ªåŠ¨é…ç½®åµŒå…¥æ¨¡å‹
        config["embed_provider_idx"] = 0  # HuggingFace
        config["embed_model_hf"] = "BAAI/bge-small-zh-v1.5"
        
        save_config(config)
        st.success("âœ… å·²ä½¿ç”¨é»˜è®¤é…ç½®ï¼\n\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼šåˆ›å»ºçŸ¥è¯†åº“ â†’ ä¸Šä¼ æ–‡æ¡£ â†’ å¼€å§‹å¯¹è¯")
        time.sleep(2)
        st.rerun()
    
    st.caption("ğŸ’¡ æˆ–æ‰‹åŠ¨é…ç½®ï¼ˆé«˜çº§ç”¨æˆ·ï¼‰")
    
    st.markdown("---")
    
    # P0æ”¹è¿›3: ä¾§è¾¹æ åˆ†ç»„ - åŸºç¡€é…ç½®ï¼ˆé»˜è®¤æŠ˜å ï¼‰
    with st.expander("âš™ï¸ åŸºç¡€é…ç½®", expanded=False):
        st.markdown("**LLM å¯¹è¯æ¨¡å‹**")
        
        # LLMé…ç½®å†…å®¹ç§»åˆ°è¿™é‡Œï¼ˆç¨åå¤„ç†ï¼‰
        llm_provider_choice = st.radio("ä¾›åº”å•†", ["Ollama (æœ¬åœ°)", "OpenAI-Compatible (äº‘ç«¯)"], horizontal=True, label_visibility="collapsed")
        
        if llm_provider_choice.startswith("Ollama"):
            llm_provider = "Ollama"
            llm_url = st.text_input("Ollama URL", defaults.get("llm_url_ollama", "http://localhost:11434"))
            
            # æ£€æµ‹ Ollama çŠ¶æ€
            ollama_ok = check_ollama_status(llm_url)
            
            col_status, _ = st.columns([3, 1])
            with col_status:
                if ollama_ok:
                    st.success("âœ… Ollama å·²è¿æ¥")
                else:
                    st.warning("âš ï¸ Ollama æœªè¿è¡Œ")
            
            # æ¨¡å‹é€‰æ‹©/è¾“å…¥ - ä½¿ç”¨æ–°ç»„ä»¶ (Stage 3.2.1)
            saved_model = defaults.get("llm_model_ollama", "qwen2.5:7b")
            llm_model, save_as_default = render_ollama_model_selector(llm_url, saved_model, ollama_ok)
            
            # å¤„ç†"è®¾ä¸ºé»˜è®¤"æŒ‰é’®
            if save_as_default:
                config = load_config()
                config["llm_model_ollama"] = llm_model
                save_config(config)
                st.success(f"âœ… å·²è®¾ä¸ºé»˜è®¤: {llm_model}")
                time.sleep(1)
                st.rerun()
            
            llm_key = ""
        else:
            llm_provider = "OpenAI-Compatible"
            llm_url = st.text_input("Base URL", defaults.get("llm_url_openai", "https://api.deepseek.com"))
            
            # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å– Key
            env_key = os.getenv('OPENAI_API_KEY', "")
            default_key = defaults.get("llm_key", "") or env_key
            
            llm_key = st.text_input("API Key", value=default_key, type="password", help="å¯ä»ç¯å¢ƒå˜é‡ OPENAI_API_KEY è‡ªåŠ¨åŠ è½½")
            if st.button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", use_container_width=True):
                with st.spinner("æ­£åœ¨è¿æ¥æ¨¡å‹åˆ—è¡¨..."):
                    mods, err = fetch_remote_models(llm_url, llm_key)
                    if mods: st.session_state.model_list = mods
                    else: st.error(err)
            
            if st.session_state.model_list:
                saved_model = defaults.get("llm_model_openai", "deepseek-chat")
                idx = st.session_state.model_list.index(saved_model) if saved_model in st.session_state.model_list else 0
                llm_model = st.selectbox("é€‰æ‹©æ¨¡å‹", st.session_state.model_list, index=idx)
            else:
                llm_model = st.text_input("è¾“å…¥æ¨¡å‹å", defaults.get("llm_model_openai", "deepseek-chat"), key="llm_openai_1")

        st.markdown("---")
        st.markdown("**Embedding å‘é‡æ¨¡å‹**")
        st.caption("ğŸ’¡ ç”¨äºç†è§£æ–‡æ¡£è¯­ä¹‰")
        
        embed_idx = defaults.get("embed_provider_idx", 0)
        if embed_idx > 2: embed_idx = 0
        embed_provider = st.selectbox("ä¾›åº”å•†", ["HuggingFace (æœ¬åœ°/æé€Ÿ)", "OpenAI-Compatible", "Ollama"], index=embed_idx, key="embed_provider_1")
        
        if embed_provider.startswith("HuggingFace"):
            # é¢„è®¾ä¼˜ç§€æ¨¡å‹åˆ—è¡¨
            preset_models = [
                "BAAI/bge-small-zh-v1.5",      # å°å‹ï¼Œå¿«é€Ÿ
                "BAAI/bge-large-zh-v1.5",      # å¤§å‹ï¼Œå‡†ç¡®
                "BAAI/bge-m3",                 # å¤šè¯­è¨€æœ€å¼º
                "BAAI/bge-base-zh-v1.5",       # ä¸­å‹ï¼Œå¹³è¡¡
                "moka-ai/m3e-base",            # M3E ä¸­æ–‡ä¼˜åŒ–
                "shibing624/text2vec-base-chinese",  # Text2Vec ä¸­æ–‡
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # å¤šè¯­è¨€è½»é‡
                "è‡ªå®šä¹‰æ¨¡å‹..."
            ]
            
            model_descriptions = {
                "BAAI/bge-small-zh-v1.5": "ğŸš€ å°å‹å¿«é€Ÿç‰ˆ | 90MB | é€‚åˆå®æ—¶åº”ç”¨ã€èµ„æºå—é™åœºæ™¯",
                "BAAI/bge-large-zh-v1.5": "ğŸ¯ ä¸­æ–‡æœ€å¼ºç‰ˆ | 1.3GB | æœ€é«˜å‡†ç¡®åº¦ï¼Œæ¨èç”¨äºç²¾å‡†æ£€ç´¢",
                "BAAI/bge-m3": "ğŸŒ å¤šè¯­è¨€æœ€å¼º | 2GB | æ”¯æŒ100+è¯­è¨€ï¼Œè·¨è¯­è¨€æ£€ç´¢æœ€ä½³",
                "BAAI/bge-base-zh-v1.5": "âš–ï¸ å¹³è¡¡ç‰ˆæœ¬ | 400MB | é€Ÿåº¦ä¸å‡†ç¡®åº¦çš„å®Œç¾å¹³è¡¡",
                "moka-ai/m3e-base": "ğŸ”¤ M3Eä¸­æ–‡ä¼˜åŒ– | 400MB | ä¸­æ–‡è¯­ä¹‰ç†è§£ä¼˜åŒ–",
                "shibing624/text2vec-base-chinese": "ğŸ“ Text2Vecä¸­æ–‡ | 400MB | ä¸­æ–‡æ–‡æœ¬å‘é‡åŒ–ä¸“å®¶",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "ğŸ’¡ è½»é‡å¤šè¯­è¨€ | 400MB | èµ„æºå—é™æ—¶çš„å¤šè¯­è¨€æ–¹æ¡ˆ"
            }
            
            # ä»é…ç½®è¯»å–é»˜è®¤æ¨¡å‹
            saved_model = defaults.get("embed_model_hf", "BAAI/bge-small-zh-v1.5")
            try:
                default_idx = preset_models.index(saved_model) if saved_model in preset_models else 0
            except:
                default_idx = 0
            
            col1, col2 = st.columns([5, 1])
            with col1:
                selected = st.selectbox(
                    "HF æ¨¡å‹",
                    options=preset_models,
                    index=default_idx,
                    help=model_descriptions.get(preset_models[default_idx], ""),
                    label_visibility="collapsed"
                )
            
            # å¦‚æœé€‰æ‹©è‡ªå®šä¹‰ï¼Œæ˜¾ç¤ºè¾“å…¥æ¡†
            if selected == "è‡ªå®šä¹‰æ¨¡å‹...":
                embed_model = st.text_input(
                    "è¾“å…¥æ¨¡å‹åç§°",
                    placeholder="ä¾‹å¦‚: sentence-transformers/all-MiniLM-L6-v2",
                    help="è¾“å…¥ä»»æ„ HuggingFace æ¨¡å‹ ID"
                )
                if not embed_model:
                    embed_model = "BAAI/bge-small-zh-v1.5"  # é»˜è®¤å€¼
            else:
                embed_model = selected
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨å¹¶æ˜¾ç¤ºçŠ¶æ€
            model_exists = check_hf_model_exists(embed_model)
            
            with col2:
                button_label = "âœ… â­" if model_exists else "â­"
                if st.button(button_label, key="set_default_embed", use_container_width=True, help="è®¾ä¸ºé»˜è®¤æ¨¡å‹"):
                    config = load_config()
                    config["embed_model_hf"] = embed_model
                    save_config(config)
                    st.success(f"âœ… å·²è®¾ä¸ºé»˜è®¤")
                    time.sleep(1)
                    st.rerun()
            
            if not model_exists:
                st.warning("âš ï¸ æ¨¡å‹æœªä¸‹è½½")
                if st.button("ğŸ“¥ ä¸‹è½½æ¨¡å‹", key="download_hf_model", type="primary", use_container_width=True):
                    with st.spinner(f"æ­£åœ¨ä¸‹è½½ {embed_model}..."):
                        try:
                            import subprocess
                            download_script = f"""
import os
os.environ['HF_HUB_OFFLINE'] = '0'
os.environ['TRANSFORMERS_OFFLINE'] = '0'
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="{embed_model}",
    cache_dir="./hf_cache",
    local_dir="./hf_cache/{embed_model.replace('/', '--')}",
    local_dir_use_symlinks=False
)
print("SUCCESS")
"""
                            result = subprocess.run(
                                [sys.executable, "-c", download_script],
                                capture_output=True,
                                text=True,
                                timeout=600
                            )
                            
                            if result.returncode == 0 and "SUCCESS" in result.stdout:
                                st.success(f"âœ… ä¸‹è½½å®Œæˆ: {embed_model}")
                                time.sleep(1)
                                st.rerun()
                            else:
                                st.error(f"ä¸‹è½½å¤±è´¥: {result.stderr}")
                        except Exception as e:
                            st.error(f"ä¸‹è½½å¤±è´¥: {e}")
            else:
                st.success("âœ… æ¨¡å‹å·²å°±ç»ª")
            
            embed_url = ""
            embed_key = ""
        elif embed_provider.startswith("OpenAI"):
            embed_model = st.text_input("æ¨¡å‹å", defaults.get("embed_model_openai", "text-embedding-3-small"))
            embed_url = st.text_input("Base URL", defaults.get("embed_url_openai", "https://api.openai.com/v1"))
            embed_key = st.text_input("API Key", defaults.get("embed_key", ""), type="password")
        else:  # Ollama
            embed_model = st.text_input("æ¨¡å‹å", defaults.get("embed_model_ollama", "nomic-embed-text"))
            embed_url = st.text_input("URL", defaults.get("embed_url_ollama", "http://localhost:11434"))
            embed_key = ""
    
    # P0æ”¹è¿›3: é«˜çº§åŠŸèƒ½ï¼ˆé»˜è®¤æŠ˜å ï¼‰
    with st.expander("ğŸ¯ é«˜çº§åŠŸèƒ½", expanded=False):
        # P0æ”¹è¿›2: ä¸“ä¸šæœ¯è¯­é€šä¿—åŒ–
        st.markdown("**æ™ºèƒ½é‡æ’åº (Re-ranking)**")
        enable_rerank = st.checkbox(
            "å¼€å¯æ™ºèƒ½é‡æ’åº",
            value=False,
            key="enable_rerank",
            help="ğŸ’¡ **é€šä¿—è§£é‡Š**ï¼šå°±åƒæœç´¢å¼•æ“çš„ç¬¬äºŒæ¬¡ç­›é€‰ï¼ŒæŠŠæœ€ç›¸å…³çš„ç»“æœæ’åœ¨å‰é¢\n\n"
                 "ğŸ”§ **æŠ€æœ¯åç§°**ï¼šRe-ranking (Cross-Encoder)\n"
                 "ğŸ“ˆ **æ•ˆæœæå‡**ï¼šå‡†ç¡®ç‡ +10~20%\n"
                 "â±ï¸ **é€Ÿåº¦å½±å“**ï¼šæŸ¥è¯¢å»¶è¿Ÿ +0.5~1ç§’"
        )
        
        if enable_rerank:
            st.caption("ğŸ“Š **å·¥ä½œåŸç†**ï¼šå…ˆæ£€ç´¢10ä¸ªå€™é€‰ â†’ æ™ºèƒ½é‡æ’åº â†’ è¿”å›æœ€ç›¸å…³çš„3ä¸ª")
            
            rerank_model = st.selectbox(
                "æ¨¡å‹é€‰æ‹©",
                ["BAAI/bge-reranker-baseï¼ˆæ¨èï¼‰", "BAAI/bge-reranker-v2-m3ï¼ˆæ›´å¼ºï¼‰"],
                key="rerank_model_display",
                help="é¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 1GBï¼‰"
            )
            
            # ä¿å­˜å®é™…æ¨¡å‹å
            if "æ¨è" in rerank_model:
                st.session_state.rerank_model = "BAAI/bge-reranker-base"
            else:
                st.session_state.rerank_model = "BAAI/bge-reranker-v2-m3"
        
        st.markdown("---")
        
        # P0æ”¹è¿›2: BM25é€šä¿—åŒ–
        st.markdown("**å…³é”®è¯å¢å¼º (BM25)**")
        enable_bm25 = st.checkbox(
            "å¼€å¯å…³é”®è¯å¢å¼º",
            value=False,
            key="enable_bm25",
            help="ğŸ’¡ **é€šä¿—è§£é‡Š**ï¼šé™¤äº†ç†è§£è¯­ä¹‰ï¼Œè¿˜èƒ½ç²¾ç¡®åŒ¹é…å…³é”®è¯ï¼ˆå¦‚ç‰ˆæœ¬å·ã€ä»£ç ã€ä¸“æœ‰åè¯ï¼‰\n\n"
                 "ğŸ”§ **æŠ€æœ¯åç§°**ï¼šBM25 æ··åˆæ£€ç´¢\n"
                 "ğŸ“ˆ **æ•ˆæœæå‡**ï¼šå‡†ç¡®ç‡å† +5~10%\n"
                 "â±ï¸ **é€Ÿåº¦å½±å“**ï¼šæŸ¥è¯¢å»¶è¿Ÿ +0.2~0.5ç§’"
        )
        
        if enable_bm25:
            st.caption("ğŸ“Š **å·¥ä½œåŸç†**ï¼šè¯­ä¹‰æ£€ç´¢ + å…³é”®è¯åŒ¹é… â†’ æ™ºèƒ½èåˆ â†’ è¿”å›æœ€ä½³ç»“æœ")
            st.caption("âœ¨ **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦ç²¾ç¡®åŒ¹é…ç‰ˆæœ¬å·ã€ä»£ç ç‰‡æ®µã€ä¸“æœ‰åè¯æ—¶")
    
    # P0æ”¹è¿›3: ç³»ç»Ÿå·¥å…·ï¼ˆé»˜è®¤æŠ˜å ï¼‰
    with st.expander("ğŸ› ï¸ ç³»ç»Ÿå·¥å…·", expanded=False):
        # ç³»ç»Ÿç›‘æ§
        auto_refresh = st.checkbox("ğŸ”„ è‡ªåŠ¨åˆ·æ–° (2ç§’)", value=False, key="monitor_auto_refresh")
        
        monitor_placeholder = st.empty()
        
        import psutil
        import subprocess
        cpu_percent = psutil.cpu_percent(interval=0.1)
        mem = psutil.virtual_memory()
        disk = psutil.disk_usage('/System/Volumes/Data')
        
        gpu_active = False
        try:
            result = subprocess.run(['ioreg', '-r', '-d', '1', '-w', '0', '-c', 'IOAccelerator'],
                                  capture_output=True, text=True, timeout=1)
            if 'PerformanceStatistics' in result.stdout:
                gpu_active = True
        except:
            pass
        
        with monitor_placeholder.container():
            col1, col2 = st.columns([3, 1])
            with col1:
                st.metric("CPU ä½¿ç”¨ç‡", f"{cpu_percent:.1f}%")
            with col2:
                st.caption(f"{psutil.cpu_count()} æ ¸")
            st.progress(cpu_percent / 100)
            
            col1, col2 = st.columns([3, 1])
            with col1:
                st.metric("GPU çŠ¶æ€", "æ´»è·ƒ" if gpu_active else "ç©ºé—²")
            with col2:
                st.caption("32 æ ¸")
            if gpu_active:
                st.progress(0.5)
            else:
                st.progress(0.0)
            
            col1, col2 = st.columns([3, 1])
            with col1:
                st.metric("å†…å­˜ä½¿ç”¨", f"{mem.percent:.1f}%")
            with col2:
                st.caption(f"{mem.used/1024**3:.1f}GB")
            st.progress(mem.percent / 100)
            
            col1, col2 = st.columns([3, 1])
            with col1:
                st.metric("ç£ç›˜ä½¿ç”¨", f"{disk.percent:.1f}%")
            with col2:
                st.caption(f"{disk.used/1024**3:.0f}GB")
            st.progress(disk.percent / 100)
            
            current_proc = psutil.Process()
            proc_mem = current_proc.memory_info().rss / 1024**3
            st.caption(f"ğŸ” è¿›ç¨‹: {proc_mem:.1f}GB | {current_proc.num_threads()} çº¿ç¨‹")
            st.caption("ğŸ’¡ GPU è¯¦ç»†ä¿¡æ¯éœ€è¦: `sudo python3 system_monitor.py`")
        
        if auto_refresh:
            import time
            time.sleep(2)
            st.rerun()
    
    st.markdown("---")
    st.markdown("### ğŸ’  çŸ¥è¯†åº“æ§åˆ¶å°")
    if "model_list" not in st.session_state: st.session_state.model_list = []

    # ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ä¸‹çš„ vector_db_storage
    default_output_path = os.path.join(os.getcwd(), "vector_db_storage")
    output_base = st.text_input("å­˜å‚¨æ ¹ç›®å½•", value=default_output_path)
    existing_kbs = get_existing_kbs(output_base)

    # --- æ ¸å¿ƒå¯¼èˆª ---
    st.markdown("#### ğŸ“š çŸ¥è¯†åº“ç®¡ç†")
    
    # çŸ¥è¯†åº“æœç´¢/è¿‡æ»¤
    if len(existing_kbs) > 5:
        search_kb = st.text_input(
            "ğŸ” æœç´¢çŸ¥è¯†åº“",
            placeholder="è¾“å…¥å…³é”®è¯è¿‡æ»¤...",
            key="search_kb",
            label_visibility="collapsed"
        )
        if search_kb:
            filtered_kbs = [kb for kb in existing_kbs if search_kb.lower() in kb.lower()]
            st.caption(f"æ‰¾åˆ° {len(filtered_kbs)} ä¸ªåŒ¹é…çš„çŸ¥è¯†åº“")
        else:
            filtered_kbs = existing_kbs
    else:
        filtered_kbs = existing_kbs
    
    nav_options = ["â• æ–°å»ºçŸ¥è¯†åº“..."] + [f"ğŸ“‚ {kb}" for kb in filtered_kbs]
    
    # é»˜è®¤é€‰æ‹©"æ–°å»ºçŸ¥è¯†åº“"ï¼Œé¿å…è‡ªåŠ¨åŠ è½½å¤§çŸ¥è¯†åº“
    default_idx = 0
    if "current_nav" in st.session_state and st.session_state.current_nav in nav_options:
        default_idx = nav_options.index(st.session_state.current_nav)
    # æ³¨é‡Šæ‰è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªçŸ¥è¯†åº“çš„é€»è¾‘
    # elif len(nav_options) > 1:
    #     default_idx = 1 
        
    selected_nav = st.selectbox("é€‰æ‹©å½“å‰çŸ¥è¯†åº“", nav_options, index=default_idx, label_visibility="collapsed")
    
    # å¸è½½çŸ¥è¯†åº“æŒ‰é’®ï¼ˆé‡Šæ”¾å†…å­˜ï¼‰
    if not (selected_nav == "â• æ–°å»ºçŸ¥è¯†åº“...") and st.session_state.get('chat_engine') is not None:
        if st.button("ğŸ”“ å¸è½½çŸ¥è¯†åº“ï¼ˆé‡Šæ”¾å†…å­˜ï¼‰", use_container_width=True, help="é‡Šæ”¾å½“å‰çŸ¥è¯†åº“å ç”¨çš„å†…å­˜èµ„æº"):
            st.session_state.chat_engine = None
            st.session_state.current_kb_id = None
            cleanup_memory()
            st.toast("âœ… çŸ¥è¯†åº“å·²å¸è½½ï¼Œå†…å­˜å·²é‡Šæ”¾")
            st.rerun()
    
    if selected_nav != st.session_state.get('current_nav'):
        st.session_state.pop('suggestions_history', None) 
        
    st.session_state.current_nav = selected_nav
    
    is_create_mode = (selected_nav == "â• æ–°å»ºçŸ¥è¯†åº“...")
    current_kb_name = selected_nav.replace("ğŸ“‚ ", "") if not is_create_mode else None

    # --- æ•°æ®æºé…ç½®åŒº ---
    if is_create_mode:
        st.caption("ğŸ› ï¸ åˆ›å»ºæ–°çŸ¥è¯†åº“")
    else:
        st.caption(f"ğŸ› ï¸ ç®¡ç†: {current_kb_name}")

    with st.container(border=True):
        if is_create_mode:
            action_mode = "NEW"
        else:
            action_mode = st.radio("æ“ä½œæ¨¡å¼", ["â• è¿½åŠ ", "ğŸ”„ è¦†ç›–"], horizontal=True, label_visibility="collapsed")
            action_mode = "APPEND" if "è¿½åŠ " in action_mode else "NEW"

        st.markdown("**æ•°æ®æº**")
        
        if "path_val" not in st.session_state: 
            st.session_state.path_val = os.path.abspath(defaults.get("target_path", ""))

        if 'path_input' not in st.session_state:
            st.session_state.path_input = ""
        
        # å¦‚æœæœ‰ä¸Šä¼ è·¯å¾„ä¸”è¾“å…¥æ¡†ä¸ºç©ºï¼Œè‡ªåŠ¨å¡«å……
        if st.session_state.get('uploaded_path') and not st.session_state.path_input:
            st.session_state.path_input = st.session_state.uploaded_path
        
        # ä¼˜åŒ–è·¯å¾„æ˜¾ç¤º
        path_col1, path_col2 = st.columns([5, 1])
        with path_col1:
            target_path = st.text_input(
                "æ–‡ä»¶/æ–‡ä»¶å¤¹è·¯å¾„", 
                value=st.session_state.path_input,
                placeholder="ğŸ“ /Users/username/docs æˆ–ä¸Šä¼ åè‡ªåŠ¨ç”Ÿæˆ",
                key="path_input_display",
                label_visibility="collapsed"
            )
            # åŒæ­¥åˆ° path_input
            if target_path != st.session_state.path_input:
                st.session_state.path_input = target_path
        with path_col2:
            if st.button("ğŸ“‚", help="åœ¨Finderä¸­æ‰“å¼€", use_container_width=True):
                if target_path and os.path.exists(target_path):
                    # macOS: åœ¨Finderä¸­æ‰“å¼€
                    import webbrowser
                    import urllib.parse
                    try:
                        file_url = 'file://' + urllib.parse.quote(os.path.abspath(target_path))
                        webbrowser.open(file_url)
                        st.toast("âœ… å·²åœ¨Finderä¸­æ‰“å¼€")
                    except Exception as e:
                        st.error(f"æ‰“å¼€å¤±è´¥: {e}")
                else:
                    st.warning("ğŸ’¡ è¯·å…ˆè¾“å…¥æœ‰æ•ˆè·¯å¾„ï¼Œæˆ–ä½¿ç”¨ä¸‹æ–¹ä¸Šä¼ åŠŸèƒ½")
        
        
        uploaded_files = st.file_uploader(
            "â¬†ï¸ æˆ–æ‹–å…¥æ–‡ä»¶/ZIP", 
            accept_multiple_files=True, 
            key="uploader",
            label_visibility="collapsed"
        )
        
        # å¤„ç†ä¸Šä¼ 
        if uploaded_files:
            if 'last_uploaded_names' not in st.session_state:
                st.session_state.last_uploaded_names = []
            
            current_names = [f.name for f in uploaded_files]
            
            # åªåœ¨æ–‡ä»¶åˆ—è¡¨å˜åŒ–æ—¶å¤„ç†
            if set(current_names) != set(st.session_state.last_uploaded_names):
                batch_dir = os.path.join(UPLOAD_DIR, f"batch_{int(time.time())}")
                os.makedirs(batch_dir, exist_ok=True)
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # æ–‡ä»¶éªŒè¯é…ç½®
                MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
                ALLOWED_EXTENSIONS = {'.pdf', '.txt', '.docx', '.md', '.xlsx', '.csv', '.pptx', '.html', '.json', '.zip'}
                
                success_count = 0
                skipped_count = 0
                skip_reasons = []
                
                for idx, f in enumerate(uploaded_files):
                    try:
                        status_text.text(f"éªŒè¯ä¸­: {f.name} ({idx+1}/{len(uploaded_files)})")
                        
                        # 1. æ£€æŸ¥æ–‡ä»¶å¤§å°
                        if f.size > MAX_FILE_SIZE:
                            skipped_count += 1
                            skip_reasons.append(f"{f.name}: è¶…è¿‡100MB")
                            continue
                            
                        # 2. æ£€æŸ¥æ‰©å±•å
                        ext = os.path.splitext(f.name)[1].lower()
                        if ext not in ALLOWED_EXTENSIONS:
                            skipped_count += 1
                            skip_reasons.append(f"{f.name}: ç±»å‹ä¸æ”¯æŒ ({ext})")
                            continue
                            
                        p = os.path.join(batch_dir, f.name)
                        
                        with open(p, "wb") as w: 
                            w.write(f.getbuffer())
                        
                        # å¤„ç† ZIP (å¸¦å®‰å…¨æ£€æŸ¥)
                        if f.name.endswith('.zip'):
                            try:
                                with zipfile.ZipFile(p, 'r') as z: 
                                    # 2.1 ZIPç‚¸å¼¹æ£€æŸ¥
                                    total_size = sum(info.file_size for info in z.infolist())
                                    if total_size > 500 * 1024 * 1024: # è§£å‹åè¶…è¿‡500MB
                                        skipped_count += 1
                                        skip_reasons.append(f"{f.name}: ZIPè§£å‹åè¿‡å¤§(>500MB)")
                                        os.remove(p)
                                        continue
                                    
                                    # 2.2 è·¯å¾„éå†æ£€æŸ¥
                                    is_safe = True
                                    for info in z.infolist():
                                        if info.filename.startswith('/') or '..' in info.filename:
                                            is_safe = False
                                            break
                                    
                                    if not is_safe:
                                        skipped_count += 1
                                        skip_reasons.append(f"{f.name}: ZIPåŒ…å«éæ³•è·¯å¾„")
                                        os.remove(p)
                                        continue
                                        
                                    z.extractall(batch_dir)
                                os.remove(p)
                            except Exception as e:
                                skipped_count += 1
                                skip_reasons.append(f"{f.name}: ZIPè§£å‹å¤±è´¥ {str(e)}")
                                if os.path.exists(p): os.remove(p)
                                continue
                        
                        logger.log_file_upload(f.name, "success")
                        success_count += 1
                        
                        progress_bar.progress((idx + 1) / len(uploaded_files))
                    except Exception as e:
                        logger.log_file_upload(f.name, "error", str(e))
                        skipped_count += 1
                        skip_reasons.append(f"{f.name}: ç³»ç»Ÿé”™è¯¯")
                
                progress_bar.empty()
                status_text.empty()
                
                st.session_state.last_uploaded_names = current_names
                st.session_state.uploaded_path = os.path.abspath(batch_dir)
                
                # æ˜¾ç¤ºä¸Šä¼ ç»“æœ
                if success_count > 0:
                    st.success(f"âœ… æˆåŠŸä¸Šä¼  {success_count} ä¸ªæ–‡ä»¶")
                
                if skipped_count > 0:
                    st.warning(f"âš ï¸ è·³è¿‡ {skipped_count} ä¸ªæ–‡ä»¶")
                    with st.expander("æŸ¥çœ‹è·³è¿‡è¯¦æƒ…", expanded=False):
                        for reason in skip_reasons:
                            st.text(f"â€¢ {reason}")
                
                time.sleep(1)
                if success_count > 0:
                    st.rerun()


        # ä½¿ç”¨ä¸Šä¼ è·¯å¾„æˆ–æ‰‹åŠ¨è¾“å…¥çš„è·¯å¾„
        target_path = st.session_state.get('uploaded_path') or target_path
        
        auto_name = ""
        if target_path:
            if os.path.exists(target_path):
                # ç»Ÿè®¡æ–‡ä»¶ä¿¡æ¯
                all_files = [f for r,d,fs in os.walk(target_path) for f in fs if not f.startswith('.')]
                cnt = len(all_files)
                
                # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
                file_types = {}
                total_size = 0
                for root, dirs, files in os.walk(target_path):
                    for f in files:
                        if not f.startswith('.'):
                            ext = os.path.splitext(f)[1].upper() or 'OTHER'
                            file_types[ext] = file_types.get(ext, 0) + 1
                            try:
                                total_size += os.path.getsize(os.path.join(root, f))
                            except:
                                pass
                
                # ç¾åŒ–æ˜¾ç¤º
                size_mb = total_size / (1024 * 1024)
                folder_name = os.path.basename(target_path.rstrip('/'))
                
                st.success(f"âœ… **æœ‰æ•ˆæ•°æ®æº**: `{folder_name}`")
                
                # ä¸‰åˆ—ç»Ÿè®¡å¡ç‰‡
                stat_col1, stat_col2, stat_col3 = st.columns(3)
                stat_col1.metric("ğŸ“„ æ–‡ä»¶æ•°", f"{cnt}")
                stat_col2.metric("ğŸ’¾ æ€»å¤§å°", f"{size_mb:.1f}MB" if size_mb > 1 else f"{total_size/1024:.0f}KB")
                stat_col3.metric("ğŸ“‚ ç±»å‹", f"{len(file_types)} ç§")
                
                # ç±»å‹åˆ†å¸ƒï¼ˆåªæ˜¾ç¤ºå‰5ç§ï¼‰
                if file_types:
                    st.caption("**æ–‡ä»¶ç±»å‹åˆ†å¸ƒ**")
                    sorted_types = sorted(file_types.items(), key=lambda x: x[1], reverse=True)[:5]
                    type_text = " Â· ".join([f"{ext.replace('.', '')}: {count}" for ext, count in sorted_types])
                    if len(file_types) > 5:
                        type_text += f" Â· å…¶ä»–: {sum(c for _, c in sorted(file_types.items(), key=lambda x: x[1], reverse=True)[5:])}"
                    st.caption(type_text)
                
                auto_name = folder_name
            else:
                st.error("âŒ è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")

        # final_kb_name å¿…é¡»åœ¨ if/else ä¸­è¢«å®šä¹‰ï¼Œä»¥ç¡®ä¿å…¶åœ¨æ¨¡å—ä½œç”¨åŸŸå†…
        st.write("")
        if is_create_mode:
            st.markdown("**çŸ¥è¯†åº“åç§°**")
            final_kb_name = st.text_input(
                "çŸ¥è¯†åº“åç§°", 
                value=sanitize_filename(auto_name), 
                placeholder="ä¾‹å¦‚: Project_Alpha, æŠ€æœ¯æ–‡æ¡£åº“",
                label_visibility="collapsed",
                help="å»ºè®®ä½¿ç”¨è‹±æ–‡ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦"
            )
        else:
            final_kb_name = current_kb_name

        # é«˜çº§é€‰é¡¹
        with st.expander("ğŸ”§ é«˜çº§é€‰é¡¹", expanded=False):
            force_reindex = st.checkbox("ğŸ”„ å¼ºåˆ¶é‡å»ºç´¢å¼•", False, help="åˆ é™¤ç°æœ‰ç´¢å¼•ï¼Œé‡æ–°æ„å»ºï¼ˆç”¨äºä¿®å¤æŸåçš„ç´¢å¼•ï¼‰")
            st.caption("âš ï¸ å¼ºåˆ¶é‡å»ºä¼šåˆ é™¤ç°æœ‰çš„å‘é‡ç´¢å¼•å’Œæ–‡æ¡£ç‰‡æ®µï¼Œé‡æ–°è§£ææ‰€æœ‰æ–‡æ¡£")
        
        st.write("")
        
        btn_label = "ğŸš€ ç«‹å³åˆ›å»º" if is_create_mode else ("â• æ‰§è¡Œè¿½åŠ " if action_mode=="APPEND" else "ğŸ”„ æ‰§è¡Œè¦†ç›–")
        btn_start = st.button(btn_label, type="primary", use_container_width=True)

    # --- ç°æœ‰åº“çš„ç®¡ç† ---
    if not is_create_mode:
        st.write("")
        st.divider()
        
        # èŠå¤©æ§åˆ¶ (P2 ä¼˜åŒ– - æ’¤é”€åŠŸèƒ½)
        st.caption("ğŸ› ï¸ èŠå¤©æ§åˆ¶")
        col1, col2 = st.columns(2)
        
        # æ’¤é”€æŒ‰é’®
        if col1.button("â†©ï¸ æ’¤é”€æé—®", use_container_width=True, disabled=len(st.session_state.messages) < 2, help="æ’¤é”€æœ€åä¸€ç»„é—®ç­”"):
            if len(st.session_state.messages) >= 2:
                # å¼¹å‡ºæœ€åä¸¤æ¡æ¶ˆæ¯ (User + Assistant)
                st.session_state.messages.pop()
                st.session_state.messages.pop()
                # ä¿å­˜æ›´æ–°åçš„å†å²
                if current_kb_name:
                    save_chat_history(current_kb_name, st.session_state.messages)
                st.toast("âœ… å·²æ’¤é”€ä¸Šä¸€æ¡æ¶ˆæ¯")
                time.sleep(0.5)
                st.rerun()
        
        # æ¸…ç©ºæŒ‰é’®
        if col2.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯", use_container_width=True, disabled=len(st.session_state.messages) == 0):
            st.session_state.messages = []
            st.session_state.suggestions_history = []
            if current_kb_name:
                save_chat_history(current_kb_name, [])
            st.toast("âœ… å¯¹è¯å·²æ¸…ç©º")
            time.sleep(0.5)
            st.rerun()
        
        # å¯¹è¯å†å²ç®¡ç†
        if len(st.session_state.messages) > 0:
            col3, col4 = st.columns(2)
            
            # å¯¼å‡ºå¯¹è¯
            if col3.button("ğŸ“¥ å¯¼å‡ºå¯¹è¯", use_container_width=True, help="å¯¼å‡ºä¸º Markdown æ–‡ä»¶"):
                export_content = f"# å¯¹è¯è®°å½• - {current_kb_name}\n\n"
                export_content += f"**å¯¼å‡ºæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                export_content += "---\n\n"
                
                for i, msg in enumerate(st.session_state.messages, 1):
                    role = "ğŸ‘¤ ç”¨æˆ·" if msg["role"] == "user" else "ğŸ¤– åŠ©æ‰‹"
                    export_content += f"## {role} ({i})\n\n{msg['content']}\n\n"
                
                st.download_button(
                    "ğŸ’¾ ä¸‹è½½ Markdown",
                    export_content,
                    file_name=f"chat_{current_kb_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                    mime="text/markdown",
                    use_container_width=True
                )
            
            # å¯¹è¯ç»Ÿè®¡
            if col4.button("ğŸ“Š å¯¹è¯ç»Ÿè®¡", use_container_width=True):
                qa_count = len(st.session_state.messages) // 2
                total_chars = sum(len(msg["content"]) for msg in st.session_state.messages)
                st.info(f"ğŸ’¬ é—®ç­”è½®æ¬¡: {qa_count}\n\nğŸ“ æ€»å­—ç¬¦æ•°: {total_chars}")
            
        # å¹¶è¡Œå¯¹è¯ (P2 ä¼˜åŒ– - å“åº”ç”¨æˆ·å¤šçº¿ç¨‹éœ€æ±‚)
        st.write("")
        st.link_button("ğŸ”€ æ–°å¼€çª—å£ (å¹¶è¡Œå¯¹è¯)", "http://localhost:8501", help="Streamlit é™åˆ¶å•é¡µé¢æ— æ³•å¹¶è¡Œç”Ÿæˆã€‚ç‚¹å‡»æ­¤æŒ‰é’®æ‰“å¼€æ–°çª—å£ï¼Œå³å¯å®ç°ä¸€è¾¹ç”Ÿæˆã€ä¸€è¾¹æé—®ã€‚", use_container_width=True)

        st.write("")
        st.caption("âš ï¸ å±é™©æ“ä½œ")
        
        if 'confirm_delete' not in st.session_state:
            st.session_state.confirm_delete = False
        
        if not st.session_state.confirm_delete:
            if st.button("ğŸ—‘ï¸ åˆ é™¤æ­¤çŸ¥è¯†åº“", use_container_width=True, type="secondary"):
                st.session_state.confirm_delete = True
        else:
            st.warning(f"âš ï¸ ç¡®è®¤åˆ é™¤ **{current_kb_name}**ï¼Ÿ\n\næ­¤æ“ä½œä¸å¯æ¢å¤ï¼Œå°†åˆ é™¤æ‰€æœ‰æ–‡æ¡£å’Œå¯¹è¯å†å²ã€‚")
            col1, col2 = st.columns(2)
            if col1.button("âœ… ç¡®è®¤åˆ é™¤", use_container_width=True, type="primary"):
                try:
                    with st.spinner(f"æ­£åœ¨åˆ é™¤ {current_kb_name}..."):
                        shutil.rmtree(os.path.join(output_base, current_kb_name), ignore_errors=True)
                        hist_path = os.path.join(HISTORY_DIR, f"{current_kb_name}.json")
                        if os.path.exists(hist_path):
                            os.remove(hist_path)
                    st.success("âœ… åˆ é™¤æˆåŠŸ")
                    st.session_state.current_nav = "â• æ–°å»ºçŸ¥è¯†åº“..."
                    st.session_state.confirm_delete = False
                    st.session_state.pop('suggestions_history', None)
                    time.sleep(0.5)
                    st.rerun()
                except Exception as e:
                    st.error(f"åˆ é™¤å¤±è´¥: {e}")
                    st.session_state.confirm_delete = False
            if col2.button("âŒ å–æ¶ˆ", use_container_width=True):
                st.session_state.confirm_delete = False

    # --- å¿«é€Ÿå¼€å§‹æ¨¡å¼ ---
    st.write("")
    if st.button("âš¡ å¿«é€Ÿå¼€å§‹ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰", use_container_width=True, type="primary", help="è‡ªåŠ¨é…ç½® Ollama + é»˜è®¤åµŒå…¥æ¨¡å‹ï¼Œ1 åˆ†é’Ÿå¼€å§‹ä½¿ç”¨"):
        # ä¿å­˜å¿«é€Ÿé…ç½®
        quick_config = {
            "llm_type_idx": 0,
            "llm_url_ollama": "http://127.0.0.1:11434",
            "llm_model_ollama": "qwen2.5:7b",
            "embed_provider_index": 0,
            "embed_model_hf": "BAAI/bge-small-zh-v1.5"
        }
        
        # åˆå¹¶åˆ°ç°æœ‰é…ç½®
        defaults.update(quick_config)
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(defaults, f, indent=4, ensure_ascii=False)
        
        st.success("âœ… å·²ä½¿ç”¨é»˜è®¤é…ç½®ï¼\n\nğŸ“ LLM: Ollama (qwen2.5:7b)\nğŸ“ åµŒå…¥: BAAI/bge-small-zh-v1.5\n\nç°åœ¨å¯ä»¥ç›´æ¥åˆ›å»ºçŸ¥è¯†åº“äº†ï¼")
        terminal_logger.success("å¿«é€Ÿå¼€å§‹æ¨¡å¼ï¼šå·²é…ç½®é»˜è®¤å€¼")
        time.sleep(1.5)
        st.rerun()
    
    st.caption("ğŸ’¡ æç¤ºï¼šå¿«é€Ÿå¼€å§‹ä¼šä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹ï¼Œéœ€è¦å…ˆå®‰è£… Ollama")
    
    # --- æ¨¡å‹é…ç½®åŒºåŸŸ (æŠ˜å æ”¶çº³) ---
    st.write("")
# ==========================================
# 5. æ ¸å¿ƒé€»è¾‘ (RAG & Indexing)
# ==========================================

def process_knowledge_base_logic():
    persist_dir = os.path.join(output_base, final_kb_name) 
    index = None
    docs = []
    file_infos = []
    start_time = time.time()

    # âš ï¸ å…³é”®ä¿®å¤ï¼šåœ¨å¤„ç†å¼€å§‹æ—¶å°±è®¾ç½®åµŒå…¥æ¨¡å‹
    terminal_logger.info(f"ğŸ”§ è®¾ç½®åµŒå…¥æ¨¡å‹: {embed_model} (provider: {embed_provider})")
    embed = get_embed(embed_provider, embed_model, embed_key, embed_url)
    if embed:
        Settings.embed_model = embed
        try:
            actual_dim = len(embed._get_text_embedding("test"))
            terminal_logger.success(f"âœ… åµŒå…¥æ¨¡å‹å·²è®¾ç½®: {embed_model} ({actual_dim}ç»´)")
        except:
            terminal_logger.success(f"âœ… åµŒå…¥æ¨¡å‹å·²è®¾ç½®: {embed_model}")
    else:
        terminal_logger.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {embed_model}")
        raise ValueError(f"æ— æ³•åŠ è½½åµŒå…¥æ¨¡å‹: {embed_model}")

    logger.log_kb_start(kb_name=final_kb_name)
    
    status_container = st.status(f"ğŸš€ å¤„ç†çŸ¥è¯†åº“: {final_kb_name}", expanded=True)
    prog_bar = status_container.progress(0)
    status_container.write(f"â±ï¸ å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")

    # æ­¥éª¤ 1: æ£€æŸ¥ç°æœ‰ç´¢å¼•
    terminal_logger.separator(f"çŸ¥è¯†åº“å¤„ç†: {final_kb_name}")
    terminal_logger.info(f"ğŸ“‚ [æ­¥éª¤ 1/6] æ£€æŸ¥ç°æœ‰ç´¢å¼•...")
    if not force_reindex and os.path.exists(persist_dir) and action_mode != "NEW":
        try:
            logger.log_kb_load_index(final_kb_name)
            status_container.write("ğŸ“‚ [æ­¥éª¤1/6] æ£€æŸ¥ç°æœ‰ç´¢å¼•...")
            
            # è®¾ç½® embedding æ¨¡å‹ç¡®ä¿å…¼å®¹æ€§
            terminal_logger.info(f"ğŸ”§ åˆ›å»ºçŸ¥è¯†åº“ä½¿ç”¨æ¨¡å‹: {embed_model} (provider: {embed_provider})")
            embed = get_embed(embed_provider, embed_model, embed_key, embed_url)
            if embed:
                Settings.embed_model = embed
                actual_dim = len(embed._get_text_embedding("test"))
                terminal_logger.info(f"âœ… æ¨¡å‹ç»´åº¦: {actual_dim}")
            else:
                terminal_logger.error("âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥ï¼")
            
            storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
            index = load_index_from_storage(storage_context)
            status_container.write("âœ… ç°æœ‰ç´¢å¼•åŠ è½½æˆåŠŸï¼Œå°†è¿½åŠ æ–°æ–‡æ¡£")
            terminal_logger.success("âœ… [æ­¥éª¤ 1/6] ç°æœ‰ç´¢å¼•åŠ è½½æˆåŠŸï¼Œå°†è¿½åŠ æ–°æ–‡æ¡£")
            prog_bar.progress(10)
        except Exception as e:
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç»´åº¦ä¸åŒ¹é…é”™è¯¯
            if "shapes" in error_msg and "not aligned" in error_msg:
                status_container.write(f"âš ï¸  å‘é‡ç»´åº¦ä¸åŒ¹é…ï¼Œæ¸…ç†æ—§ç´¢å¼•")
                terminal_logger.warning(f"âš ï¸  [æ­¥éª¤ 1/6] å‘é‡ç»´åº¦ä¸åŒ¹é…ï¼Œè½¬ä¸ºæ–°å»ºæ¨¡å¼")
            else:
                status_container.write(f"âš ï¸  ç´¢å¼•æŸåï¼Œè½¬ä¸ºæ–°å»ºæ¨¡å¼")
                terminal_logger.warning(f"âš ï¸  [æ­¥éª¤ 1/6] ç´¢å¼•æŸåï¼Œè½¬ä¸ºæ–°å»ºæ¨¡å¼")
            shutil.rmtree(persist_dir, ignore_errors=True)
            index = None

    current_target_path = st.session_state.get('uploaded_path') or st.session_state.path_input
    
    if not current_target_path or not os.path.exists(current_target_path):
        status_container.update(label="âŒ è·¯å¾„æ— æ•ˆ", state="error")
        terminal_logger.error(f"âŒ è·¯å¾„æ— æ•ˆ: {current_target_path}")
        raise ValueError(f"è·¯å¾„æ— æ•ˆ: {current_target_path}")

    # æ­¥éª¤ 2: æ‰«ææ–‡ä»¶
    terminal_logger.info(f"ğŸ“ [æ­¥éª¤ 2/6] æ‰«ææ–‡ä»¶å¤¹: {os.path.basename(current_target_path)}")
    logger.log_kb_scan_path(current_target_path, kb_name=final_kb_name)
    status_container.write(f"ğŸ“ [æ­¥éª¤2/6] æ‰«ææ–‡ä»¶å¤¹: {os.path.basename(current_target_path)}")
    
    # å…ˆå¿«é€Ÿç»Ÿè®¡æ–‡ä»¶æ•°é‡
    all_files = []
    for root, _, filenames in os.walk(current_target_path):
        for f in filenames:
            if not f.startswith('.'):
                all_files.append(os.path.join(root, f))
    
    total_files = len(all_files)
    status_container.write(f"   ğŸ“Š å‘ç° {total_files} ä¸ªæ–‡ä»¶")
    terminal_logger.success(f"âœ… [æ­¥éª¤ 2/6] æ‰«æå®Œæˆ: å‘ç° {total_files} ä¸ªæ–‡ä»¶")
    prog_bar.progress(20)
    
    # æ­¥éª¤ 3: è¯»å–æ–‡æ¡£
    terminal_logger.info(f"ğŸ“– [æ­¥éª¤ 3/6] è¯»å–æ–‡æ¡£å†…å®¹ (å…± {total_files} ä¸ªæ–‡ä»¶)")
    status_container.write(f"ğŸ“– [æ­¥éª¤3/6] è¯»å–æ–‡æ¡£å†…å®¹ (å…± {total_files} ä¸ªæ–‡ä»¶)")
    if total_files > 10:
        status_container.write(f"   ğŸš€ 250 çº¿ç¨‹å¹¶è¡Œè¯»å– | æ‰¹é‡ 5 ä¸ªæ–‡ä»¶ | ç›®æ ‡ < 80% èµ„æº")
        terminal_logger.info(f"   ğŸš€ å¯ç”¨ 250 çº¿ç¨‹å¹¶è¡Œè¯»å–")
    
    # åˆ›å»ºä¸€ä¸ªå ä½ç¬¦ç”¨äºå®æ—¶æ›´æ–°
    progress_placeholder = status_container.empty()
    
    docs, process_result = scan_directory_safe(current_target_path)
    summary = process_result.get_summary()
    
    if summary['success'] == 0:
        status_container.update(label="âŒ æ²¡æœ‰å¯å¤„ç†çš„æ–‡ä»¶", state="error")
        raise ValueError(f"æ²¡æœ‰æˆåŠŸè¯»å–çš„æ–‡ä»¶ã€‚{process_result.get_report()}")
    
    # è®¡ç®—æ€»æ•°å’ŒæˆåŠŸç‡
    total_files = summary['success'] + summary['failed'] + summary['skipped']
    success_rate = (summary['success'] / total_files * 100) if total_files > 0 else 0
    
    status_container.write(f"âœ… è¯»å–å®Œæˆ: {summary['success']}/{total_files} ä¸ªæ–‡ä»¶ ({success_rate:.1f}%)ï¼Œ{summary['total_docs']} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    terminal_logger.success(f"âœ… [æ­¥éª¤ 3/6] è¯»å–å®Œæˆ: {summary['success']}/{total_files} ä¸ªæ–‡ä»¶ï¼Œ{summary['total_docs']} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    if summary['failed'] > 0:
        status_container.write(f"   âš ï¸  å¤±è´¥: {summary['failed']} ä¸ªæ–‡ä»¶ ({summary['failed']/total_files*100:.1f}%)")
        terminal_logger.warning(f"   âš ï¸  å¤±è´¥: {summary['failed']} ä¸ªæ–‡ä»¶")
    if summary['skipped'] > 0:
        status_container.write(f"   â­ï¸  è·³è¿‡: {summary['skipped']} ä¸ªæ–‡ä»¶ ({summary['skipped']/total_files*100:.1f}%)")
        terminal_logger.info(f"   â­ï¸  è·³è¿‡: {summary['skipped']} ä¸ªæ–‡ä»¶")
    prog_bar.progress(40)
    
    # æ­¥éª¤ 4: æ„å»ºæ–‡ä»¶æ¸…å•
    terminal_logger.info(f"ğŸ“‹ [æ­¥éª¤ 4/6] æ„å»ºæ–‡ä»¶æ¸…å•...")
    status_container.write(f"ğŸ“‹ [æ­¥éª¤4/6] æ„å»ºæ–‡ä»¶æ¸…å•...")
    
    # åˆå§‹åŒ–å…ƒæ•°æ®ç®¡ç†å™¨
    metadata_mgr = MetadataManager(persist_dir)
    
    temp_file_map = {}
    for root, _, filenames in os.walk(current_target_path):
        for f in filenames:
            if not f.startswith('.'):
                fp = os.path.join(root, f)
                info = get_file_info(fp, metadata_mgr); info['doc_ids'] = []
                temp_file_map[f] = info
    
    file_count = len(temp_file_map)
    logger.log_kb_read_success(len(docs), file_count=file_count, kb_name=final_kb_name)
    status_container.write(f"âœ… æ¸…å•å®Œæˆ: {file_count} ä¸ªæ–‡ä»¶å·²ç™»è®°")
    terminal_logger.success(f"âœ… [æ­¥éª¤ 4/6] æ¸…å•å®Œæˆ: {file_count} ä¸ªæ–‡ä»¶å·²ç™»è®°")
    logger.log_kb_manifest(file_count, kb_name=final_kb_name)
    prog_bar.progress(50)
    
    # æ­¥éª¤ 5: è§£ææ–‡æ¡£ç‰‡æ®µï¼ˆå¿«é€Ÿæ¨¡å¼ + åå°æ‘˜è¦ + å…ƒæ•°æ®æå–ï¼‰
    terminal_logger.info(f"ğŸ” [æ­¥éª¤ 5/6] è§£ææ–‡æ¡£ç‰‡æ®µ (å…± {len(docs)} ä¸ª)")
    terminal_logger.info(f"   ğŸ“‹ ä»»åŠ¡: æ˜ å°„æ–‡æ¡£ID â†’ æ–‡ä»¶æ¸…å• + å…ƒæ•°æ®æå–")
    status_container.write(f"ğŸ” [æ­¥éª¤5/6] è§£ææ–‡æ¡£ç‰‡æ®µ (å…± {len(docs)} ä¸ª)")
    
    step5_start = time.time()
    # å¿«é€Ÿæ˜ å°„æ–‡æ¡£ID + æå–å…ƒæ•°æ®
    file_text_samples = {}  # æ”¶é›†æ¯ä¸ªæ–‡ä»¶çš„æ–‡æœ¬æ ·æœ¬
    for d in docs:
        fname = d.metadata.get('file_name')
        if fname and fname in temp_file_map:
            temp_file_map[fname]['doc_ids'].append(d.doc_id)
            # æ”¶é›†æ–‡æœ¬æ ·æœ¬ç”¨äºå…ƒæ•°æ®æå–
            if fname not in file_text_samples and d.text.strip():
                file_text_samples[fname] = d.text[:1000]  # å‰1000å­—ç”¨äºåˆ†æ
    
    # æ‰¹é‡å¤„ç†å…ƒæ•°æ®ï¼ˆå¤šè¿›ç¨‹åŠ é€Ÿï¼‰
    status_container.write(f"   ğŸ”– æå–å…ƒæ•°æ®: å“ˆå¸Œ/å…³é”®è¯/åˆ†ç±»... ({len(file_text_samples)} ä¸ªæ–‡ä»¶)")
    terminal_logger.info(f"   ğŸ”– æå–å…ƒæ•°æ®: {len(file_text_samples)} ä¸ªæ–‡ä»¶")
    
    if len(file_text_samples) > 100:
        # å¤§é‡æ–‡ä»¶ï¼Œä½¿ç”¨å¤šè¿›ç¨‹
        import multiprocessing as mp
        
        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for fname, text_sample in file_text_samples.items():
            if fname in temp_file_map:
                fp = os.path.join(current_target_path, fname)
                if os.path.exists(fp):
                    doc_ids = temp_file_map[fname]['doc_ids']
                    tasks.append((fp, fname, doc_ids, text_sample, persist_dir))
        
        # å¤šè¿›ç¨‹å¤„ç†
        num_workers = min(mp.cpu_count(), 12)  # æœ€å¤š12è¿›ç¨‹
        status_container.write(f"   âš¡ ä½¿ç”¨ {num_workers} è¿›ç¨‹å¹¶è¡Œæå–...")
        terminal_logger.info(f"   âš¡ ä½¿ç”¨ {num_workers} è¿›ç¨‹å¹¶è¡Œæå–å…ƒæ•°æ®")
        
        with mp.Pool(processes=num_workers) as pool:
            results = pool.map(_extract_metadata_task, tasks, chunksize=50)
        
        # æ›´æ–°ç»“æœ
        metadata_count = 0
        for fname, meta in results:
            if fname in temp_file_map:
                temp_file_map[fname].update({
                    'file_hash': meta.get('file_hash', ''),
                    'keywords': meta.get('keywords', []),
                    'language': meta.get('language', 'unknown'),
                    'category': meta.get('category', 'å…¶ä»–æ–‡æ¡£')
                })
                metadata_count += 1
        
        terminal_logger.success(f"   âœ… å…ƒæ•°æ®æå–å®Œæˆ: {metadata_count} ä¸ªæ–‡ä»¶")
    else:
        # å°‘é‡æ–‡ä»¶ï¼Œå•çº¿ç¨‹å¤„ç†
        terminal_logger.info(f"   ğŸ“ å•çº¿ç¨‹å¤„ç† {len(file_text_samples)} ä¸ªæ–‡ä»¶")
        metadata_count = 0
        for fname, text_sample in file_text_samples.items():
            if fname in temp_file_map:
                fp = os.path.join(current_target_path, fname)
                if os.path.exists(fp):
                    doc_ids = temp_file_map[fname]['doc_ids']
                    meta = metadata_mgr.add_file_metadata(fp, doc_ids, text_sample)
                    temp_file_map[fname].update({
                        'file_hash': meta.get('file_hash', ''),
                        'keywords': meta.get('keywords', []),
                        'language': meta.get('language', 'unknown'),
                        'category': meta.get('category', 'å…¶ä»–æ–‡æ¡£')
                    })
                    metadata_count += 1
    
    if metadata_count > 0:
        status_container.write(f"   âœ… å…ƒæ•°æ®æå–å®Œæˆ: {metadata_count} ä¸ªæ–‡ä»¶")
        terminal_logger.success(f"   âœ… å…ƒæ•°æ®æå–å®Œæˆ: {metadata_count} ä¸ªæ–‡ä»¶")
    
    # æ”¶é›†éœ€è¦ç”Ÿæˆæ‘˜è¦çš„æ–‡æ¡£
    summary_tasks = []
    for d in docs:
        fname = d.metadata.get('file_name')
        if fname and fname in temp_file_map and d.text.strip() and not temp_file_map[fname].get('summary'):
            summary_tasks.append((fname, d.text[:2000]))  # åªä¿å­˜å‰2000å­—
    
    if summary_tasks:
        status_container.write(f"   ğŸ’¡ æ‘˜è¦ç”Ÿæˆå·²åŠ å…¥åå°é˜Ÿåˆ— ({len(summary_tasks)} ä¸ªæ–‡ä»¶)")
        status_container.write(f"   âš¡ çŸ¥è¯†åº“å°†ç«‹å³å®Œæˆï¼Œæ‘˜è¦åœ¨åå°ç”Ÿæˆ")
        
        # ä¿å­˜æ‘˜è¦ä»»åŠ¡åˆ°æ–‡ä»¶ï¼Œä¾›åå°å¤„ç†
        summary_queue_file = os.path.join(persist_dir, "summary_queue.json")
        os.makedirs(persist_dir, exist_ok=True)
        
        # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        def clean_text(text):
            try:
                # ç§»é™¤ä»£ç†å¯¹å­—ç¬¦ï¼ˆsurrogate pairsï¼‰
                return text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')
            except:
                return ""
        
        cleaned_tasks = [(fname, clean_text(text)) for fname, text in summary_tasks]
        
        with open(summary_queue_file, 'w', encoding='utf-8', errors='ignore') as f:
            json.dump({
                'tasks': cleaned_tasks,
                'total': len(cleaned_tasks),
                'completed': 0
            }, f, ensure_ascii=False)
    
    file_infos = list(temp_file_map.values())
    valid_docs = [d for d in docs if d.text and d.text.strip()]
    status_container.write(f"âœ… è§£æå®Œæˆ: {len(valid_docs)} ä¸ªæœ‰æ•ˆç‰‡æ®µ")
    prog_bar.progress(70)
    
    logger.log_kb_parse_complete(valid_count=len(valid_docs), kb_name=final_kb_name)
    
    if not valid_docs:
        status_container.update(label="âŒ æ–‡æ¡£å†…å®¹ä¸ºç©º", state="error")
        raise ValueError("è·¯å¾„ä¸‹æ–‡æ¡£å†…å®¹ä¸ºç©º")
    
    if not valid_docs:
        status_container.update(label="âŒ æ–‡æ¡£å†…å®¹ä¸ºç©º", state="error")
        raise ValueError("è·¯å¾„ä¸‹æ–‡æ¡£å†…å®¹ä¸ºç©º")

    # æ­¥éª¤ 6: å‘é‡åŒ–å’Œç´¢å¼•æ„å»º
    terminal_logger.info(f"âš¡ï¸ [æ­¥éª¤ 6/6] å‘é‡åŒ–å’Œç´¢å¼•æ„å»º...")
    if index and action_mode == "APPEND":
        logger.log_kb_mode("append", kb_name=final_kb_name)
        terminal_logger.info(f"â• [æ­¥éª¤ 6/6] è¿½åŠ æ¨¡å¼: æ’å…¥æ–°æ–‡æ¡£åˆ°ç°æœ‰ç´¢å¼•")
        status_container.write(f"â• [æ­¥éª¤6/6] è¿½åŠ æ¨¡å¼: æ’å…¥æ–°æ–‡æ¡£åˆ°ç°æœ‰ç´¢å¼•")
        for i, d in enumerate(valid_docs):
            index.insert(d)
            if (i + 1) % 10 == 0:
                prog_bar.progress(70 + int((i + 1) / len(valid_docs) * 20))
    else:
        logger.log_kb_mode("new", kb_name=final_kb_name)
        step6_start = time.time()
        terminal_logger.info(f"âš¡ï¸ [æ­¥éª¤ 6/6] æ–°å»ºæ¨¡å¼: æ„å»ºå‘é‡ç´¢å¼•")
        terminal_logger.info(f"   ğŸ“‹ ä»»åŠ¡æ¸…å•:")
        terminal_logger.info(f"      1ï¸âƒ£  æ–‡æ¡£åˆ†å— ({len(valid_docs)} ä¸ªæ–‡æ¡£)")
        terminal_logger.info(f"      2ï¸âƒ£  å‘é‡åŒ– (GPUåŠ é€Ÿ)")
        terminal_logger.info(f"      3ï¸âƒ£  æ„å»ºç´¢å¼•")
        status_container.write(f"âš¡ï¸ [æ­¥éª¤6/6] æ–°å»ºæ¨¡å¼: æ„å»ºå‘é‡ç´¢å¼•")
        status_container.write(f"   ğŸš€ å¤šæ ¸åŠ é€Ÿå¯åŠ¨ä¸­...")
        if os.path.exists(persist_dir): shutil.rmtree(persist_dir, ignore_errors=True)
        parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)
        
        # å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼ˆé’ˆå¯¹ M4 Max ä¼˜åŒ–ï¼š10æ€§èƒ½æ ¸+4æ•ˆç‡æ ¸ï¼‰
        status_container.write(f"   ğŸ”¥ å¤šçº¿ç¨‹å¹¶è¡Œå·²å¯ç”¨ (å…± {len(valid_docs)} ä¸ªæ–‡æ¡£)")
        terminal_logger.processing(f"ğŸš€ [6.1] å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç† {len(valid_docs)} ä¸ªæ–‡æ¡£...")
        
        # åŠ¨æ€è®¡ç®—çº¿ç¨‹æ•° - æ ¹æ®CPUæ ¸å¿ƒæ•°å’Œå½“å‰è´Ÿè½½
        import psutil
        cpu_count = psutil.cpu_count(logical=True)
        current_cpu = psutil.cpu_percent(interval=0.5)
        current_mem = psutil.virtual_memory().percent
        
        # ç›®æ ‡ï¼šä¿æŒæ€»èµ„æºä½¿ç”¨åœ¨80%ä»¥å†…
        target_usage = 80.0
        available_cpu = max(10, target_usage - current_cpu)  # è‡³å°‘ä¿ç•™10%
        available_mem = max(10, target_usage - current_mem)
        
        # æ ¹æ®å¯ç”¨èµ„æºåŠ¨æ€è°ƒæ•´çº¿ç¨‹æ•°
        if available_cpu > 30 and available_mem > 50:
            # èµ„æºå……è¶³ï¼Œæ¿€è¿›ä½¿ç”¨
            num_workers = min(cpu_count * 6, 80)  # æœ€å¤š80ä¸ªçº¿ç¨‹
        elif available_cpu > 20 and available_mem > 30:
            # èµ„æºé€‚ä¸­
            num_workers = min(cpu_count * 4, 60)
        elif available_cpu > 10 and available_mem > 20:
            # èµ„æºç´§å¼ 
            num_workers = min(cpu_count * 2, 40)
        else:
            # èµ„æºéå¸¸ç´§å¼ 
            num_workers = max(cpu_count, 20)
        
        status_container.write(f"   ğŸ’» {num_workers} ä¸ªçº¿ç¨‹è¿è¡Œä¸­ (åŠ¨æ€è°ƒæ•´: CPUå¯ç”¨{available_cpu:.0f}%, å†…å­˜å¯ç”¨{available_mem:.0f}%)...")
        terminal_logger.info(f"   ğŸ’» å¯ç”¨ {num_workers} ä¸ªå¹¶è¡Œçº¿ç¨‹ï¼ˆåŠ¨æ€è°ƒæ•´ï¼Œç›®æ ‡èµ„æº<80%ï¼‰")
        terminal_logger.info(f"   ğŸ“Š å½“å‰çŠ¶æ€: CPU {current_cpu:.1f}%, å†…å­˜ {current_mem:.1f}%")
        
        terminal_logger.cpu_multicore_start(num_workers)
        parse_start = time.time()
        
        # æå–æ–‡æ¡£æ–‡æœ¬
        doc_texts = [doc.text for doc in valid_docs]
        
        status_container.write(f"   ğŸ“¦ æ­£åœ¨åˆ†å—å¤„ç†...")
        
        # åˆ›å»ºå®æ—¶è¿›åº¦å ä½ç¬¦
        chunk_progress = status_container.empty()
        
        all_chunks = []
        processed_count = 0
        
        # æ‰¹é‡å¤„ç†ï¼šå°æ‰¹æ¬¡é«˜å¹¶å‘
        docs_per_batch = max(10, len(doc_texts) // (num_workers * 8))  # æ¯æ‰¹10-30ä¸ª
        batches = [doc_texts[i:i + docs_per_batch] for i in range(0, len(doc_texts), docs_per_batch)]
        
        chunk_progress.write(f"      ğŸ“¦ åˆ†æˆ {len(batches)} æ‰¹ï¼Œæ¯æ‰¹çº¦ {docs_per_batch} ä¸ªæ–‡æ¡£")
        terminal_logger.info(f"   ğŸ“¦ åˆ†æˆ {len(batches)} æ‰¹å¤„ç†")
        
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(_parse_batch_docs, batch) for batch in batches]
            for i, future in enumerate(as_completed(futures)):
                try:
                    chunks = future.result()
                    all_chunks.extend(chunks)
                    processed_count += len(batches[i])
                    
                    # è®¡ç®—é¢„è®¡å®Œæˆæ—¶é—´
                    elapsed = time.time() - parse_start
                    if processed_count > 0:
                        avg_time_per_doc = elapsed / processed_count
                        remaining_docs = len(doc_texts) - processed_count
                        eta_seconds = avg_time_per_doc * remaining_docs
                        eta_str = f"{int(eta_seconds)}s" if eta_seconds < 60 else f"{int(eta_seconds/60)}m{int(eta_seconds%60)}s"
                    else:
                        eta_str = "è®¡ç®—ä¸­..."
                    
                    # å®æ—¶æ›´æ–°è¿›åº¦
                    percent = int((processed_count / len(doc_texts)) * 100)
                    chunk_progress.write(f"      âš¡ å·²å¤„ç†: {processed_count}/{len(doc_texts)} ({percent}%) | å·²ç”Ÿæˆ {len(all_chunks)} ä¸ªèŠ‚ç‚¹ | é¢„è®¡å‰©ä½™: {eta_str}")
                    
                    prog_bar.progress(70 + int((processed_count / len(doc_texts)) * 10))
                    
                    if i % 5 == 0:
                        terminal_logger.cpu_multicore_status(processed_count, len(doc_texts))
                        terminal_logger.info(f"   â±ï¸  é¢„è®¡å‰©ä½™: {eta_str}")
                        
                        # æ£€æŸ¥èµ„æºä½¿ç”¨ï¼Œè¶…è¿‡90%åˆ™æš‚åœ
                        cpu, mem, gpu, should_throttle = check_resource_usage()
                        if should_throttle:
                            import time as time_module
                            terminal_logger.warning(f"âš ï¸  èµ„æºä½¿ç”¨è¿‡é«˜ (CPU: {cpu:.1f}%, å†…å­˜: {mem:.1f}%, GPU: {gpu:.1f}%)ï¼Œæš‚åœ1ç§’...")
                            time_module.sleep(1)
                            
                except Exception as e:
                    terminal_logger.error(f"æ‰¹æ¬¡è§£æå¤±è´¥: {e}")
        
        parse_elapsed = time.time() - parse_start
        terminal_logger.cpu_multicore_end(len(doc_texts), parse_elapsed)
        
        # è½¬æ¢ä¸º TextNode å¯¹è±¡
        from llama_index.core.schema import TextNode
        nodes = [TextNode(text=chunk['text']) for chunk in all_chunks]
        
        # é‡Šæ”¾å†…å­˜
        del all_chunks
        del doc_texts
        cleanup_memory()
        status_container.write(f"   ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ")
        
        chunk_progress.empty()  # æ¸…é™¤è¿›åº¦å ä½ç¬¦
        status_container.write(f"   âœ… åˆ†å—å®Œæˆ: {len(nodes)} ä¸ªèŠ‚ç‚¹ (è€—æ—¶ {parse_elapsed:.1f}s)")
        prog_bar.progress(80)
        
        # GPU å‘é‡åŒ–ï¼ˆæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡ï¼Œä¸è¶…è¿‡ 90%ï¼‰
        status_container.write(f"   ğŸ® GPU å‘é‡åŒ–å¤„ç†ä¸­...")
        status_container.write(f"      æ­£åœ¨å°† {len(nodes)} ä¸ªèŠ‚ç‚¹è½¬æ¢ä¸ºå‘é‡...")
        terminal_logger.processing(f"ğŸš€ [6.2] GPU æ‰¹é‡æ„å»ºç´¢å¼• (ç›®æ ‡ GPU åˆ©ç”¨ç‡ <90%)...")
        terminal_logger.info(f"   ğŸ“‹ å½“å‰ä»»åŠ¡: å‘é‡åŒ– {len(nodes)} ä¸ªèŠ‚ç‚¹")
        vector_start = time.time()
        
        # åŠ¨æ€æ‰¹æ¬¡å¤§å°ï¼šä¼˜åŒ–GPUåˆ©ç”¨ç‡ï¼ˆæ›´å°çš„batchï¼Œæ›´é¢‘ç¹çš„GPUè°ƒç”¨ï¼‰
        import psutil
        total_mem_gb = psutil.virtual_memory().total / (1024**3)
        available_mem_gb = psutil.virtual_memory().available / (1024**3)
        
        # ä¼˜åŒ–ç­–ç•¥ï¼šè¾ƒå°çš„batch_sizeï¼Œè®©GPUæŒç»­å·¥ä½œ
        if len(nodes) > 500000:  # è¶…å¤§è§„æ¨¡
            batch_size = 50000   # 5ä¸‡ï¼ˆåŸ20ä¸‡ï¼‰
        elif len(nodes) > 200000:  # å¤§è§„æ¨¡
            batch_size = 30000   # 3ä¸‡ï¼ˆåŸ15ä¸‡ï¼‰
        elif len(nodes) > 100000:  # ä¸­å¤§è§„æ¨¡
            batch_size = 20000   # 2ä¸‡ï¼ˆåŸ10ä¸‡ï¼‰
        elif len(nodes) > 50000:  # ä¸­ç­‰è§„æ¨¡
            batch_size = 15000   # 1.5ä¸‡ï¼ˆåŸ8ä¸‡ï¼‰
        else:  # å°è§„æ¨¡
            batch_size = 10000   # 1ä¸‡ï¼ˆåŸ5ä¸‡ï¼‰
        
        # å†…å­˜ä¿æŠ¤ï¼šå¦‚æœå¯ç”¨å†…å­˜ä¸è¶³ï¼Œé™ä½ batch_size
        if available_mem_gb < 3:
            batch_size = min(batch_size, 5000)
        elif available_mem_gb < 8:
            batch_size = min(batch_size, 10000)
        
        # ç¡®ä¿è‡³å°‘åˆ† 5 æ‰¹ï¼ˆè®©GPUæŒç»­å·¥ä½œï¼‰
        if len(nodes) > batch_size and total_batches < 5:
            batch_size = len(nodes) // 5
            
        total_batches = (len(nodes) + batch_size - 1) // batch_size
        
        status_container.write(f"      ğŸ“¦ åˆ† {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} ä¸ªèŠ‚ç‚¹")
        terminal_logger.info(f"   ğŸ“¦ åˆ† {total_batches} æ‰¹å¤„ç† (batch_size={batch_size})")
        terminal_logger.info(f"   ğŸ¯ ç›®æ ‡: æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡ (<90%)")
        vector_progress = status_container.empty()
        
        # åˆ›å»ºç´¢å¼•ï¼ˆç¬¬ä¸€æ‰¹ï¼‰
        first_batch = nodes[:batch_size]
        
        # ä¼°ç®—æ€»æ—¶é—´ï¼ˆåŸºäºç»éªŒå€¼ï¼šçº¦ 0.01-0.02s/èŠ‚ç‚¹ï¼‰
        estimated_total_time = len(nodes) * 0.015
        eta_str = f"{int(estimated_total_time)}s" if estimated_total_time < 60 else f"{int(estimated_total_time/60)}m{int(estimated_total_time%60)}s"
        
        vector_progress.write(f"      âš¡ å¤„ç†ç¬¬ 1/{total_batches} æ‰¹ ({len(first_batch)} ä¸ªèŠ‚ç‚¹) | é¢„è®¡æ€»è€—æ—¶: {eta_str}")
        terminal_logger.info(f"   âš¡ å¤„ç†ç¬¬ 1/{total_batches} æ‰¹ | é¢„è®¡æ€»è€—æ—¶: {eta_str}")
        index = VectorStoreIndex(first_batch, show_progress=False)
        
        # è¿½åŠ å‰©ä½™æ‰¹æ¬¡ï¼ˆåŠ¨æ€è°ƒæ•´ batch_sizeï¼‰
        current_batch_size = batch_size
        for i in range(1, total_batches):
            # è®¡ç®—é¢„è®¡å®Œæˆæ—¶é—´
            elapsed = time.time() - vector_start
            avg_time_per_batch = elapsed / i
            remaining_batches = total_batches - i
            eta_seconds = avg_time_per_batch * remaining_batches
            eta_str = f"{int(eta_seconds)}s" if eta_seconds < 60 else f"{int(eta_seconds/60)}m{int(eta_seconds%60)}s"
            
            # æ£€æŸ¥èµ„æºä½¿ç”¨
            import psutil
            import time as time_module
            mem_percent = psutil.virtual_memory().percent
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            # æ£€æŸ¥ GPU
            gpu_percent = 0.0
            try:
                import torch
                if torch.backends.mps.is_available():
                    gpu_percent = min(90.0, mem_percent * 0.8)
                elif torch.cuda.is_available():
                    gpu_mem = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory * 100
                    gpu_percent = gpu_mem
            except:
                pass
            
            # åŠ¨æ€è°ƒæ•´ç­–ç•¥ï¼šGPU åˆ©ç”¨ç‡ä½ä¸”å†…å­˜å……è¶³ï¼Œå°è¯•å¢å¤§ batch
            if i > 2 and gpu_percent < 60 and mem_percent < 70:
                # GPU åˆ©ç”¨ç‡ä½ï¼Œå¯ä»¥å¢å¤§ batch_size
                if i % 3 == 0:  # æ¯ 3 æ‰¹æ£€æŸ¥ä¸€æ¬¡
                    old_batch = current_batch_size
                    current_batch_size = min(int(current_batch_size * 2), 300000)  # ç¿»å€ï¼Œæœ€å¤§ 30ä¸‡
                    if current_batch_size != old_batch:
                        terminal_logger.info(f"   ğŸ“ˆ åŠ¨æ€è°ƒæ•´: batch_size {old_batch} â†’ {current_batch_size} (GPU åˆ©ç”¨ç‡ä½)")
            
            if mem_percent > 90 or cpu_percent > 90 or gpu_percent > 90:
                vector_progress.write(f"      â¸ï¸  èµ„æºä½¿ç”¨è¿‡é«˜ (CPU: {cpu_percent:.1f}%, å†…å­˜: {mem_percent:.1f}%, GPU: {gpu_percent:.1f}%)ï¼Œç­‰å¾…...")
                terminal_logger.warning(f"   âš ï¸  èµ„æºè¶…è¿‡90%é˜ˆå€¼ï¼Œæš‚åœ2ç§’...")
                time_module.sleep(2)
            
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, len(nodes))
            batch = nodes[start_idx:end_idx]
            
            percent = int((i / total_batches) * 100)
            vector_progress.write(f"      âš¡ å¤„ç†ç¬¬ {i+1}/{total_batches} æ‰¹ ({percent}%) | {len(batch)} ä¸ªèŠ‚ç‚¹ | CPU: {cpu_percent:.1f}% | å†…å­˜: {mem_percent:.1f}% | GPU: {gpu_percent:.1f}% | é¢„è®¡å‰©ä½™: {eta_str}")
            
            if i % 5 == 0:
                terminal_logger.info(f"   ğŸ“Š è¿›åº¦: {i+1}/{total_batches} ({percent}%) | é¢„è®¡å‰©ä½™: {eta_str}")
            
            # æ‰¹é‡æ’å…¥ï¼ˆä½¿ç”¨ insert_nodes è€Œä¸æ˜¯é€ä¸ª insertï¼‰
            index.insert_nodes(batch)
        
        vector_elapsed = time.time() - vector_start
        vector_progress.empty()
        status_container.write(f"   âœ… å‘é‡åŒ–å®Œæˆ: {len(nodes)} ä¸ªèŠ‚ç‚¹ â†’ å‘é‡æ•°æ®åº“ (è€—æ—¶ {vector_elapsed:.1f}s)")
        terminal_logger.success(f"âœ… [6.2] å‘é‡åŒ–å®Œæˆ: è€—æ—¶ {vector_elapsed:.1f}s")
        terminal_logger.success(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ")
    
    prog_bar.progress(90)
    
    # æŒä¹…åŒ–å­˜å‚¨
    terminal_logger.info(f"ğŸ’¾ æŒä¹…åŒ–å­˜å‚¨: {final_kb_name}")
    logger.log_kb_persist("persisting", kb_name=final_kb_name)
    status_container.write(f"ğŸ’¾ ä¿å­˜åˆ°ç£ç›˜...")
    status_container.write(f"   è·¯å¾„: {persist_dir}")
    if not os.path.exists(output_base): os.makedirs(output_base)
    index.storage_context.persist(persist_dir=persist_dir)
    update_manifest(persist_dir, file_infos, is_append=(action_mode == "APPEND"), embed_model=embed_model)
    logger.log_kb_persist("success", kb_name=final_kb_name)
    status_container.write(f"   âœ… ä¿å­˜æˆåŠŸ")
    terminal_logger.success(f"âœ… å­˜å‚¨å®Œæˆ [çŸ¥è¯†åº“: {final_kb_name}]")

    prog_bar.progress(100)
    elapsed = time.time() - start_time
    
    # æ˜¾ç¤ºå®Œæˆæ‘˜è¦
    terminal_logger.separator(f"å¤„ç†å®Œæˆ")
    terminal_logger.success(f"âœ… çŸ¥è¯†åº“å¤„ç†å®Œæˆ: {final_kb_name}")
    
    # è®¡ç®—è¯¦ç»†ç»Ÿè®¡
    end_time_obj = datetime.now()
    start_time_obj = datetime.fromtimestamp(start_time)
    docs_per_sec = len(valid_docs) / elapsed if elapsed > 0 else 0
    
    terminal_logger.data_summary("å¤„ç†ç»Ÿè®¡", {
        "çŸ¥è¯†åº“": final_kb_name,
        "æ–‡ä»¶æ•°": file_count,
        "æ–‡æ¡£ç‰‡æ®µ": len(valid_docs),
        "å‘é‡èŠ‚ç‚¹": len(nodes) if 'nodes' in locals() else 'N/A',
        "æ¨¡å¼": "è¿½åŠ " if action_mode == "APPEND" else "æ–°å»º"
    })
    terminal_logger.data_summary("æ—¶é—´ç»Ÿè®¡", {
        "å¼€å§‹æ—¶é—´": start_time_obj.strftime('%H:%M:%S'),
        "ç»“æŸæ—¶é—´": end_time_obj.strftime('%H:%M:%S'),
        "æ€»è€—æ—¶": f"{elapsed:.2f}s ({elapsed/60:.1f}åˆ†é’Ÿ)",
        "å¤„ç†é€Ÿåº¦": f"{docs_per_sec:.1f} æ–‡æ¡£/ç§’"
    })
    # è®¡ç®—ç»“æŸæ—¶é—´å’Œå„é˜¶æ®µè€—æ—¶
    end_time = datetime.now()
    start_time_obj = datetime.fromtimestamp(start_time)
    
    # è®¡ç®—å¹³å‡é€Ÿåº¦
    docs_per_sec = len(valid_docs) / elapsed if elapsed > 0 else 0
    nodes_per_sec = len(nodes) / elapsed if elapsed > 0 and 'nodes' in locals() else 0
    
    status_container.write(f"")
    status_container.write(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    status_container.write(f"âœ… å¤„ç†å®Œæˆ!")
    status_container.write(f"")
    status_container.write(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    status_container.write(f"   ğŸ“ æ–‡ä»¶æ•°: {file_count}")
    status_container.write(f"   ğŸ“„ æ–‡æ¡£ç‰‡æ®µ: {len(valid_docs)}")
    status_container.write(f"   ğŸ”¢ å‘é‡èŠ‚ç‚¹: {len(nodes) if 'nodes' in locals() else 'N/A'}")
    if 'summary' in locals() and summary.get('failed', 0) > 0:
        status_container.write(f"   âš ï¸  å¤±è´¥: {summary['failed']} ä¸ªæ–‡ä»¶")
    if 'summary' in locals() and summary.get('skipped', 0) > 0:
        status_container.write(f"   â­ï¸  è·³è¿‡: {summary['skipped']} ä¸ªæ–‡ä»¶")
    status_container.write(f"")
    status_container.write(f"â±ï¸  æ—¶é—´ç»Ÿè®¡:")
    status_container.write(f"   ğŸ• å¼€å§‹æ—¶é—´: {start_time_obj.strftime('%H:%M:%S')}")
    status_container.write(f"   ğŸ• ç»“æŸæ—¶é—´: {end_time.strftime('%H:%M:%S')}")
    status_container.write(f"   â±ï¸  æ€»è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ ({elapsed:.0f}ç§’)")
    status_container.write(f"   âš¡ å¤„ç†é€Ÿåº¦: {docs_per_sec:.1f} æ–‡æ¡£/ç§’")
    if 'parse_start' in locals() and 'vector_start' in locals():
        parse_time = vector_start - parse_start if 'vector_start' in locals() else 0
        vector_time = locals().get('vector_elapsed', 0)
        status_container.write(f"")
        status_container.write(f"ğŸ“ˆ é˜¶æ®µè€—æ—¶:")
        status_container.write(f"   ğŸ“¦ æ–‡æ¡£åˆ†å—: {parse_time:.1f}ç§’")
        status_container.write(f"   ğŸ® GPUå‘é‡åŒ–: {vector_time:.1f}ç§’")
    status_container.write(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    logger.log_kb_complete(kb_name=final_kb_name, doc_count=len(valid_docs))
    status_container.update(label=f"âœ… çŸ¥è¯†åº“ '{final_kb_name}' å¤„ç†å®Œæˆ", state="complete", expanded=False)
    
    # æ˜¾ç¤ºè¯¦ç»†å¤„ç†æŠ¥å‘Š
    with st.expander("ğŸ“Š æ–‡ä»¶å¤„ç†è¯¦æƒ…", expanded=False):
        st.markdown(process_result.get_report())
    
    time.sleep(0.5)
    return len(valid_docs)

# ==========================================
# 6. èŠå¤©ç•Œé¢ & æ— é™è¿½é—®åŠŸèƒ½
# ==========================================
st.title("ğŸ›¡ï¸ RAG Pro Max")

# åˆå§‹åŒ–çŠ¶æ€
if "messages" not in st.session_state: st.session_state.messages = []
if "chat_engine" not in st.session_state: st.session_state.chat_engine = None
if "prompt_trigger" not in st.session_state: st.session_state.prompt_trigger = None
if "current_kb_id" not in st.session_state: st.session_state.current_kb_id = None
if "renaming" not in st.session_state: st.session_state.renaming = False
if "suggestions_history" not in st.session_state: st.session_state.suggestions_history = []
if "is_processing" not in st.session_state: st.session_state.is_processing = False 
if "quote_content" not in st.session_state: st.session_state.quote_content = None # å¼•ç”¨å†…å®¹åˆå§‹åŒ–
if "first_time_guide_shown" not in st.session_state: st.session_state.first_time_guide_shown = False

# é¦–æ¬¡ä½¿ç”¨å¼•å¯¼
if not st.session_state.first_time_guide_shown and len(existing_kbs) == 0:
    st.info("""
    ### ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ RAG Pro Maxï¼
    
    **å¿«é€Ÿå¼€å§‹æŒ‡å—ï¼š**
    
    1ï¸âƒ£ **é…ç½® LLM**ï¼ˆå·¦ä¾§è¾¹æ ï¼‰
    - é€‰æ‹© Ollamaï¼ˆæœ¬åœ°ï¼‰æˆ– OpenAIï¼ˆäº‘ç«¯ï¼‰
    - è¾“å…¥ API ä¿¡æ¯
    
    2ï¸âƒ£ **åˆ›å»ºçŸ¥è¯†åº“**
    - ç‚¹å‡» "â• æ–°å»ºçŸ¥è¯†åº“..."
    - è¾“å…¥åç§°ï¼Œä¸Šä¼ æ–‡æ¡£
    
    3ï¸âƒ£ **å¼€å§‹å¯¹è¯**
    - é€‰æ‹©çŸ¥è¯†åº“
    - åœ¨ä¸‹æ–¹è¾“å…¥é—®é¢˜
    
    ğŸ’¡ **æç¤º**ï¼šæ”¯æŒ PDFã€DOCXã€TXTã€MD ç­‰å¤šç§æ ¼å¼
    """)
    
    if st.button("âœ… æˆ‘çŸ¥é“äº†ï¼Œå¼€å§‹ä½¿ç”¨", use_container_width=True):
        st.session_state.first_time_guide_shown = True
        st.rerun()

def click_btn(q):
    st.session_state.prompt_trigger = q
    st.session_state.suggestions_history = []
    st.rerun()

# è®¡ç®—å½“å‰çš„ KB ID (æ ¹æ®ä¾§è¾¹æ é€‰æ‹©)
active_kb_name = current_kb_name if not is_create_mode else None

# è‡ªåŠ¨åŠ è½½é€»è¾‘
if active_kb_name and active_kb_name != st.session_state.current_kb_id:
    st.session_state.current_kb_id = active_kb_name
    st.session_state.chat_engine = None
    with st.spinner("ğŸ“œ æ­£åœ¨åŠ è½½å¯¹è¯å†å²..."):
        st.session_state.messages = load_chat_history(active_kb_name)
    st.session_state.suggestions_history = []

if active_kb_name and st.session_state.chat_engine is None:
    db_path = os.path.join(output_base, active_kb_name)
    if os.path.exists(db_path):
        try:
            logger.log_kb_mount_start(active_kb_name)
            
            # æ£€æµ‹çŸ¥è¯†åº“çš„å‘é‡ç»´åº¦
            kb_dim = get_kb_embedding_dim(db_path)
            if kb_dim:
                # æ ¹æ®ç»´åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹
                model_map = {
                    512: "BAAI/bge-small-zh-v1.5",
                    768: "BAAI/bge-base-zh-v1.5",
                    1024: "BAAI/bge-m3"
                }
                
                if kb_dim in model_map:
                    required_model = model_map[kb_dim]
                    if embed_model != required_model:
                        terminal_logger.warning(f"âš ï¸ çŸ¥è¯†åº“ç»´åº¦: {kb_dim}Dï¼Œè‡ªåŠ¨åˆ‡æ¢æ¨¡å‹: {required_model}")
                        embed_model = required_model
                        # é‡æ–°åŠ è½½ embedding æ¨¡å‹
                        embed = get_embed(embed_provider, embed_model, embed_key, embed_url)
                        if embed:
                            Settings.embed_model = embed
            
            # æ£€æŸ¥çŸ¥è¯†åº“å¤§å°
            import glob
            vector_files = glob.glob(os.path.join(db_path, "**/*.json"), recursive=True)
            total_size = sum(os.path.getsize(f) for f in vector_files) / (1024 * 1024)  # MB
            is_large_kb = len(vector_files) > 100 or total_size > 100
            
            if is_large_kb:
                load_start = time.time()
                terminal_logger.info(f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡: {len(vector_files)} ä¸ªæ–‡ä»¶, {total_size:.1f}MB")
                
                # è¿›åº¦æ¡æ”¾åœ¨å¤–é¢
                progress_placeholder = st.empty()
                progress_bar = progress_placeholder.progress(0, text="â³ å‡†å¤‡åŠ è½½çŸ¥è¯†åº“... 0%")
                
                with st.status(f"ğŸ“š æ­£åœ¨æŒ‚è½½å¤§å‹çŸ¥è¯†åº“: {active_kb_name}ï¼ˆ{len(vector_files)} ä¸ªæ–‡ä»¶, {total_size:.1f}MBï¼‰", expanded=True) as status:
                    # é˜¶æ®µ1: åŠ è½½å‘é‡æ•°æ® (0-40%)
                    status.write("â³ [1/3] æ­£åœ¨åŠ è½½å‘é‡æ•°æ®...")
                    terminal_logger.processing("[1/3] å¼€å§‹åŠ è½½å‘é‡æ•°æ®...")
                    terminal_logger.info(f"ğŸ“‚ åŠ è½½ docstore.json ({total_size:.1f}MB)...")
                    
                    # å®æ—¶è¿›åº¦æ˜¾ç¤º
                    stage1_start = time.time()
                    import threading
                    result = [None]
                    def load_storage():
                        result[0] = StorageContext.from_defaults(persist_dir=db_path)
                    
                    thread = threading.Thread(target=load_storage)
                    thread.start()
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = 5
                    while thread.is_alive():
                        progress = min(progress + 1, 39)
                        elapsed = time.time() - stage1_start
                        progress_bar.progress(progress, text=f"â³ [1/3] åŠ è½½å‘é‡æ•°æ®... {progress}% (å·²ç”¨æ—¶ {elapsed:.0f}s)")
                        time.sleep(0.5)
                    
                    thread.join()
                    storage_context = result[0]
                    stage1_time = time.time() - stage1_start
                    
                    progress_bar.progress(40, text=f"âœ… [1/3] å‘é‡æ•°æ®åŠ è½½å®Œæˆ ({stage1_time:.1f}s) - 40%")
                    status.write(f"âœ… [1/3] å‘é‡æ•°æ®åŠ è½½å®Œæˆ (è€—æ—¶ {stage1_time:.1f}s)")
                    terminal_logger.success(f"[1/3] å‘é‡æ•°æ®åŠ è½½å®Œæˆ ({stage1_time:.1f}s)")
                    
                    # é˜¶æ®µ2: æ„å»ºç´¢å¼• (40-80%)
                    status.write("â³ [2/3] æ­£åœ¨æ„å»ºç´¢å¼•...")
                    terminal_logger.processing("[2/3] å¼€å§‹æ„å»ºç´¢å¼•...")
                    terminal_logger.info(f"ğŸ“Š åŠ è½½ index_store.json...")
                    terminal_logger.info(f"ğŸ”— æ„å»ºå‘é‡ç´¢å¼• (959K èŠ‚ç‚¹)...")
                    
                    stage2_start = time.time()
                    result2 = [None]
                    def load_index():
                        result2[0] = load_index_from_storage(storage_context)
                    
                    thread2 = threading.Thread(target=load_index)
                    thread2.start()
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = 45
                    while thread2.is_alive():
                        progress = min(progress + 1, 79)
                        elapsed = time.time() - stage2_start
                        progress_bar.progress(progress, text=f"â³ [2/3] æ„å»ºç´¢å¼•... {progress}% (å·²ç”¨æ—¶ {elapsed:.0f}s)")
                        time.sleep(0.5)
                    
                    thread2.join()
                    index = result2[0]
                    stage2_time = time.time() - stage2_start
                    
                    progress_bar.progress(80, text=f"âœ… [2/3] ç´¢å¼•æ„å»ºå®Œæˆ ({stage2_time:.1f}s) - 80%")
                    status.write(f"âœ… [2/3] ç´¢å¼•æ„å»ºå®Œæˆ (è€—æ—¶ {stage2_time:.1f}s)")
                    terminal_logger.success(f"[2/3] ç´¢å¼•æ„å»ºå®Œæˆ ({stage2_time:.1f}s)")
                    
                    # é˜¶æ®µ3: åˆå§‹åŒ–é—®ç­”å¼•æ“ (80-100%)
                    status.write("â³ [3/3] æ­£åœ¨åˆå§‹åŒ–é—®ç­”å¼•æ“...")
                    terminal_logger.processing("[3/3] åˆå§‹åŒ–é—®ç­”å¼•æ“...")
                    terminal_logger.info(f"ğŸ¤– é…ç½® chat_engine...")
                    
                    stage3_start = time.time()
                    for i in range(85, 100, 3):
                        progress_bar.progress(i, text=f"â³ [3/3] åˆå§‹åŒ–é—®ç­”å¼•æ“... {i}%")
                        time.sleep(0.1)
                    
                    # Re-ranking é…ç½®
                    node_postprocessors = []
                    similarity_top_k = 5
                    retriever = None
                    
                    # BM25 æ··åˆæ£€ç´¢é…ç½®
                    if st.session_state.get('enable_bm25', False):
                        try:
                            from llama_index.retrievers.bm25 import BM25Retriever
                            from llama_index.core.retrievers import QueryFusionRetriever
                            
                            status.write(f"   ğŸ” æ„å»º BM25 æ··åˆæ£€ç´¢...")
                            terminal_logger.info(f"ğŸ” BM25 æ··åˆæ£€ç´¢å¯ç”¨")
                            
                            # è·å–æ‰€æœ‰èŠ‚ç‚¹
                            nodes = index.docstore.docs.values()
                            
                            # åˆ›å»º BM25 æ£€ç´¢å™¨
                            bm25_retriever = BM25Retriever.from_defaults(
                                nodes=list(nodes),
                                similarity_top_k=5
                            )
                            
                            # åˆ›å»ºå‘é‡æ£€ç´¢å™¨
                            vector_retriever = index.as_retriever(similarity_top_k=5)
                            
                            # èåˆæ£€ç´¢å™¨
                            retriever = QueryFusionRetriever(
                                retrievers=[vector_retriever, bm25_retriever],
                                similarity_top_k=5,
                                num_queries=1,
                                mode="reciprocal_rerank",
                                use_async=False,
                            )
                            
                            status.write(f"   âœ… BM25 æ··åˆæ£€ç´¢æ„å»ºæˆåŠŸ")
                            terminal_logger.success(f"âœ… BM25 æ··åˆæ£€ç´¢æ„å»ºæˆåŠŸ")
                        except ImportError:
                            status.write(f"   âš ï¸ BM25 éœ€è¦å®‰è£…: pip install llama-index-retrievers-bm25")
                            terminal_logger.warning("BM25 ä¾èµ–ç¼ºå¤±")
                        except Exception as e:
                            status.write(f"   âš ï¸ BM25 æ„å»ºå¤±è´¥: {e}")
                            terminal_logger.error(f"BM25 æ„å»ºå¤±è´¥: {e}")
                    
                    if st.session_state.get('enable_rerank', False):
                        try:
                            from llama_index.core.postprocessor import SentenceTransformerRerank
                            
                            rerank_model = st.session_state.get('rerank_model', 'BAAI/bge-reranker-base')
                            status.write(f"   ğŸ¯ åŠ è½½ Re-ranking æ¨¡å‹: {rerank_model}...")
                            terminal_logger.info(f"ğŸ¯ Re-ranking å¯ç”¨: {rerank_model}")
                            
                            reranker = SentenceTransformerRerank(
                                top_n=3,
                                model=rerank_model,
                                keep_retrieval_score=True,
                            )
                            node_postprocessors.append(reranker)
                            similarity_top_k = 10  # Re-ranking æ—¶å…ˆæ£€ç´¢æ›´å¤š
                            
                            status.write(f"   âœ… Re-ranking æ¨¡å‹åŠ è½½æˆåŠŸ")
                            terminal_logger.success(f"âœ… Re-ranking æ¨¡å‹åŠ è½½æˆåŠŸ")
                        except ImportError:
                            status.write(f"   âš ï¸ Re-ranking éœ€è¦å®‰è£…: pip install sentence-transformers")
                            terminal_logger.warning("Re-ranking ä¾èµ–ç¼ºå¤±")
                        except Exception as e:
                            status.write(f"   âš ï¸ Re-ranking åŠ è½½å¤±è´¥: {e}")
                            terminal_logger.error(f"Re-ranking åŠ è½½å¤±è´¥: {e}")
                    
                    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
                    if retriever:
                        st.session_state.chat_engine = index.as_chat_engine(
                            chat_mode="context",
                            retriever=retriever,
                            memory=ChatMemoryBuffer.from_defaults(token_limit=4000),
                            system_prompt="ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œè¯·åŠ¡å¿…ä»…åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚å›ç­”åº”æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šã€‚",
                            node_postprocessors=node_postprocessors if node_postprocessors else None,
                        )
                    else:
                        st.session_state.chat_engine = index.as_chat_engine(
                            chat_mode="context", 
                            memory=ChatMemoryBuffer.from_defaults(token_limit=4000),
                            system_prompt="ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œè¯·åŠ¡å¿…ä»…åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚å›ç­”åº”æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šã€‚",
                            similarity_top_k=similarity_top_k,
                            node_postprocessors=node_postprocessors if node_postprocessors else None,
                        )
                    stage3_time = time.time() - stage3_start
                    load_time = time.time() - load_start
                    
                    progress_bar.progress(100, text=f"âœ… å…¨éƒ¨å®Œæˆï¼æ€»è€—æ—¶: {load_time:.1f}s - 100%")
                    status.write(f"âœ… [3/3] é—®ç­”å¼•æ“åˆå§‹åŒ–å®Œæˆ (è€—æ—¶ {stage3_time:.1f}s)")
                    terminal_logger.success(f"[3/3] é—®ç­”å¼•æ“åˆå§‹åŒ–å®Œæˆ ({stage3_time:.1f}s)")
                    
                    status.update(label=f"âœ… çŸ¥è¯†åº“ '{active_kb_name}' æŒ‚è½½æˆåŠŸï¼æ€»è€—æ—¶: {load_time:.1f}s", state="complete")
                    terminal_logger.info(f"ğŸ“Š æ€»è€—æ—¶: {load_time:.1f}s")
                
                # æ¸…ç†è¿›åº¦æ¡
                time.sleep(1.5)
                progress_placeholder.empty()
            else:
                with st.spinner(f"ğŸ“š æ­£åœ¨æŒ‚è½½çŸ¥è¯†åº“: {active_kb_name}..."):
                    try:
                        # è¯»å–çŸ¥è¯†åº“å®é™…ä½¿ç”¨çš„æ¨¡å‹ï¼ˆè€Œä¸æ˜¯ä¾§è¾¹æ é€‰æ‹©ï¼‰
                        kb_manifest = load_manifest(db_path)
                        kb_embed_model = kb_manifest.get('embed_model', 'BAAI/bge-large-zh-v1.5')
                        
                        terminal_logger.info(f"ğŸ“Š çŸ¥è¯†åº“æ¨¡å‹: {kb_embed_model}")
                        terminal_logger.info(f"ğŸ“Š Embed Provider: {embed_provider}")
                        
                        # ä½¿ç”¨çŸ¥è¯†åº“çš„æ¨¡å‹åŠ è½½
                        embed = get_embed(embed_provider, kb_embed_model, embed_key, embed_url)
                        if embed:
                            Settings.embed_model = embed
                            terminal_logger.success(f"âœ… åµŒå…¥æ¨¡å‹å·²è®¾ç½®: {kb_embed_model}")
                        else:
                            raise ValueError(f"æ— æ³•åŠ è½½åµŒå…¥æ¨¡å‹: {kb_embed_model}")
                    except Exception as e:
                        terminal_logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                        st.error(f"çŸ¥è¯†åº“æŒ‚è½½å¤±è´¥: {e}")
                        raise
                    
                    try:
                        storage_context = StorageContext.from_defaults(persist_dir=db_path)
                        index = load_index_from_storage(storage_context)
                    except Exception as e:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»´åº¦ä¸åŒ¹é…é”™è¯¯
                        if "shapes" in str(e) and "not aligned" in str(e):
                            terminal_logger.warning(f"âš ï¸ å‘é‡ç»´åº¦ä¸åŒ¹é…")
                            terminal_logger.info(f"å½“å‰æ¨¡å‹: {embed_model}")
                            terminal_logger.info(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
                            
                            st.error(f"âŒ å‘é‡ç»´åº¦ä¸åŒ¹é…")
                            st.warning(f"""
**å½“å‰æ¨¡å‹:** {embed_model}

**åŸå› :** çŸ¥è¯†åº“æ˜¯ç”¨å…¶ä»–ç»´åº¦çš„æ¨¡å‹åˆ›å»ºçš„ï¼Œæ— æ³•ç›´æ¥æŸ¥è¯¢ã€‚

**è§£å†³æ–¹æ¡ˆ:**
1. **ä¿ç•™æ—§æ•°æ®** - åˆ‡æ¢å›åŸæ¨¡å‹ï¼ˆbge-small-zh-v1.5ï¼‰
2. **é‡å»ºç´¢å¼•** - ç”¨æ–°æ¨¡å‹é‡æ–°åµŒå…¥æ‰€æœ‰æ–‡æ¡£ï¼ˆè€—æ—¶è¾ƒé•¿ï¼‰

é€‰æ‹©ä¸€ä¸ªæ“ä½œ:
""")
                            col1, col2 = st.columns(2)
                            with col1:
                                if st.button("ğŸ”„ é‡å»ºç´¢å¼•", type="primary", use_container_width=True):
                                    with st.spinner("æ­£åœ¨æ¸…ç†æ—§ç´¢å¼•..."):
                                        import shutil
                                        shutil.rmtree(db_path, ignore_errors=True)
                                        terminal_logger.success(f"âœ… æ—§ç´¢å¼•å·²æ¸…ç†ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡æ¡£")
                                        st.success("âœ… ç´¢å¼•å·²æ¸…ç†ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡æ¡£")
                                        time.sleep(2)
                                        st.rerun()
                            with col2:
                                if st.button("â†©ï¸ åˆ‡æ¢æ¨¡å‹", use_container_width=True):
                                    st.info("è¯·åœ¨ä¾§è¾¹æ é€‰æ‹©åŸæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ bge-small-zh-v1.5ï¼‰")
                            
                            st.session_state.chat_engine = None
                            st.stop()
                        else:
                            raise
                    
                    terminal_logger.processing("åˆå§‹åŒ–é—®ç­”å¼•æ“...")
                    st.session_state.chat_engine = index.as_chat_engine(
                        chat_mode="context", 
                        memory=ChatMemoryBuffer.from_defaults(token_limit=4000),
                        system_prompt="ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œè¯·åŠ¡å¿…ä»…åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚å›ç­”åº”æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šã€‚",
                        similarity_top_k=3,  # å‡å°‘æ£€ç´¢æ•°é‡
                    )
            
            terminal_logger.success("é—®ç­”å¼•æ“å·²å¯ç”¨GPUåŠ é€Ÿ")
            logger.log_kb_mount_success(active_kb_name)
            st.toast(f"âœ… çŸ¥è¯†åº“ '{active_kb_name}' æŒ‚è½½æˆåŠŸï¼")
            
            # é‡Šæ”¾å†…å­˜
            cleanup_memory()
        except Exception as e: 
            logger.log_kb_mount_error(active_kb_name, e)
            st.error(f"çŸ¥è¯†åº“æŒ‚è½½å¤±è´¥ï¼Œè¯·å°è¯•ã€å¼ºåˆ¶é‡å»ºã€‘ï¼š{e}")
            st.session_state.chat_engine = None 

# æŒ‰é’®å¤„ç†
if btn_start:
    config_to_save = {
        "target_path": target_path,
        "output_path": output_base,
        "llm_type_idx": 0 if llm_provider == "Ollama" else 1,
        "llm_url_ollama": llm_url if llm_provider == "Ollama" else "",
        "llm_model_ollama": llm_model if llm_provider == "Ollama" else "",
        "llm_url_openai": llm_url if llm_provider != "Ollama" else "",
        "llm_key": llm_key,
        "llm_model_openai": llm_model if llm_provider != "Ollama" else "",
        "embed_provider_idx": ["HuggingFace (æœ¬åœ°/æé€Ÿ)", "OpenAI-Compatible", "Ollama"].index(embed_provider),
        "embed_model_hf": embed_model if embed_provider.startswith("HuggingFace") else "",
        "embed_url_ollama": embed_url if embed_provider.startswith("Ollama") else "",
        "embed_model_ollama": embed_model if embed_provider.startswith("Ollama") else ""
    }
    save_config(config_to_save)

    if not final_kb_name:
        st.error("è¯·è¾“å…¥çŸ¥è¯†åº“åç§°")
    else:
        try:
            clean_kb_name = sanitize_filename(final_kb_name)
            if not clean_kb_name: raise ValueError("çŸ¥è¯†åº“åç§°åŒ…å«éæ³•å­—ç¬¦æˆ–ä¸ºç©º")
                
            # ä¿®å¤ï¼šç›´æ¥å¯¹æ¨¡å—çº§å˜é‡ final_kb_name èµ‹å€¼ï¼Œä¸å†éœ€è¦ global å…³é”®å­—
            # final_kb_name åœ¨ä¾§è¾¹æ å·²å®šä¹‰
            final_kb_name = clean_kb_name
            
            process_knowledge_base_logic()
            st.session_state.current_nav = f"ğŸ“‚ {final_kb_name}"
            st.session_state.current_kb_id = None 
            
            if action_mode == "NEW" or action_mode == "APPEND":
                st.session_state.messages = []
                st.session_state.suggestions_history = []
                hist_path = os.path.join(HISTORY_DIR, f"{final_kb_name}.json")
                if os.path.exists(hist_path): os.remove(hist_path)
            
            time.sleep(1); st.rerun()
        except Exception as e: st.error(f"æ‰§è¡Œå¤±è´¥: {e}")

# --- ä¸»è§†å›¾æ¸²æŸ“ ---
if active_kb_name:
    db_path = os.path.join(output_base, active_kb_name)
    manifest = load_manifest(db_path)
    file_cnt = len(manifest.get('files', []))
    last_upd = manifest.get('last_updated', 'N/A')[:10]
    kb_model = manifest.get('embed_model', 'Unknown')
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_sz = 0
    total_chunks = 0
    file_types = {}
    oldest_date = None
    newest_date = None
    
    for f in manifest.get('files', []):
        try:
            if 'KB' in f['size']: total_sz += float(f['size'].replace(' KB',''))
            elif 'MB' in f['size']: total_sz += float(f['size'].replace(' MB',''))*1024
        except: pass
        
        total_chunks += len(f.get('doc_ids', []))
        ftype = f.get('type', 'Unknown')
        file_types[ftype] = file_types.get(ftype, 0) + 1
        
        file_date = f.get('added_at', '')
        if file_date:
            if oldest_date is None or file_date < oldest_date:
                oldest_date = file_date
            if newest_date is None or file_date > newest_date:
                newest_date = file_date
    
    # å•è¡Œç´§å‡‘æ ‡é¢˜ + ç»Ÿè®¡
    if st.session_state.renaming:
        def apply_rename():
            n = sanitize_filename(st.session_state.new_name_input)
            if n and n != active_kb_name:
                try:
                    rename_kb(active_kb_name, n, output_base)
                    st.session_state.current_nav = f"ğŸ“‚ {n}"
                    st.toast("âœ… é‡å‘½åæˆåŠŸ")
                except FileExistsError as e:
                    st.error(f"é‡å‘½åå¤±è´¥: {e}")
            st.session_state.renaming = False
        c1, c2 = st.columns([3, 1])
        c1.text_input("æ–°åç§°", value=active_kb_name, key="new_name_input", on_change=apply_rename)
        c2.button("å–æ¶ˆ", on_click=lambda: st.session_state.update({"renaming": False}))
    else:
        col1, col2, col3, col4, col5, col6 = st.columns([2, 1, 1, 1, 1, 0.6])
        col1.markdown(f"### ğŸ’¬ {active_kb_name}")
        col2.metric("ğŸ“„ æ–‡ä»¶", file_cnt)
        col3.metric("ğŸ’¾ å¤§å°", f"{total_sz/1024:.1f}MB" if total_sz > 1024 else f"{int(total_sz)}KB")
        col4.metric("ğŸ“¦ ç‰‡æ®µ", total_chunks)
        col5.metric("ğŸ§¬ æ¨¡å‹", kb_model.split('/')[-1] if '/' in kb_model else kb_model)
        if col6.button("âœï¸", help="é‡å‘½å"): 
            st.session_state.renaming = True
    
    # æ–‡ä»¶ç®¡ç†
    with st.expander("ğŸ“Š çŸ¥è¯†åº“è¯¦æƒ…ä¸ç®¡ç†", expanded=False):
        if not manifest['files']: 
            st.info("æš‚æ— æ–‡ä»¶")
        else:
            # è®¡ç®—å­˜å‚¨å¤§å°
            import os
            db_size = 0
            if os.path.exists(db_path):
                for root, dirs, files in os.walk(db_path):
                    db_size += sum(os.path.getsize(os.path.join(root, f)) for f in files)
            db_size_mb = db_size / (1024 * 1024)
            
            # è®¡ç®—æˆåŠŸç‡
            files_with_chunks = len([f for f in manifest['files'] if len(f.get('doc_ids', [])) > 0])
            success_rate = (files_with_chunks / file_cnt * 100) if file_cnt > 0 else 0
            
            # è®¡ç®—å‹ç¼©æ¯”å’Œå­˜å‚¨æ•ˆç‡ï¼ˆç»Ÿä¸€ä¸ºå­—èŠ‚ï¼‰
            total_sz_bytes = total_sz * 1024  # total_sz æ˜¯ KBï¼Œè½¬æ¢ä¸ºå­—èŠ‚
            compression_ratio = (total_sz_bytes / db_size) if db_size > 0 else 0
            storage_efficiency = f"{compression_ratio:.1f}x" if compression_ratio > 1 else "1.0x" if compression_ratio > 0 else "N/A"
            
            # å•è¡Œç»Ÿè®¡æ‘˜è¦
            time_range = f"{oldest_date[:10]} ~ {newest_date[:10]}" if oldest_date and newest_date else last_upd
            st.markdown(f"**ğŸ“Š ç»Ÿè®¡** Â· {file_cnt} æ–‡ä»¶ Â· {total_chunks} ç‰‡æ®µ Â· ğŸ“ åŸå§‹ {f'{total_sz/1024:.1f}MB' if total_sz > 1024 else f'{int(total_sz)}KB'} Â· ğŸ’¾ å‘é‡åº“ {db_size_mb:.1f}MB ({storage_efficiency}) Â· ğŸ“… {time_range}")
            
            # æ ¸å¿ƒæŒ‡æ ‡ + è´¨é‡åˆ†æï¼ˆ6åˆ—ï¼‰
            metric_col1, metric_col2, metric_col3, metric_col4, metric_col5, metric_col6 = st.columns(6)
            avg_chunks = total_chunks / file_cnt if file_cnt > 0 else 0
            avg_size = (total_sz / file_cnt) if file_cnt > 0 else 0
            
            metric_col1.metric("ğŸ“ˆ å¹³å‡ç‰‡æ®µ", f"{avg_chunks:.1f}")
            metric_col2.metric("ğŸ“Š å¹³å‡å¤§å°", f"{avg_size/1024:.1f}KB" if avg_size > 1024 else f"{int(avg_size)}KB")
            
            # å¥åº·åº¦
            health_icon = "ğŸŸ¢" if success_rate >= 90 else "ğŸŸ¡" if success_rate >= 70 else "ğŸ”´"
            metric_col3.metric("ğŸ’š å¥åº·åº¦", f"{health_icon} {success_rate:.0f}%")
            
            # è´¨é‡åˆ†æ
            low_quality = len([f for f in manifest['files'] if len(f.get('doc_ids', [])) < 2])
            large_files = len([f for f in manifest['files'] if 'MB' in f['size']])
            empty_docs = len([f for f in manifest['files'] if len(f.get('doc_ids', [])) == 0])
            
            quality_status = "âœ… ä¼˜ç§€" if low_quality == 0 and large_files == 0 and empty_docs == 0 else f"âš ï¸ {empty_docs}ç©º {low_quality}ä½è´¨"
            metric_col4.metric("ğŸ” è´¨é‡", quality_status)
            
            # æ–‡ä»¶ç±»å‹æ•°é‡
            type_count = len(file_types)
            metric_col5.metric("ğŸ“‚ ç±»å‹", f"{type_count} ç§")
            
            metric_col6.metric("ğŸ”¤ æ¨¡å‹", kb_model.split('/')[-1][:12] if '/' in kb_model else kb_model[:12])
            
            st.divider()
            
            # å››åˆ—å¸ƒå±€ï¼šç±»å‹åˆ†å¸ƒ + å¤§å°åˆ†å¸ƒ + ç‰‡æ®µåˆ†å¸ƒ + æ•°æ®æ´å¯Ÿ
            type_col, size_col, chunk_col, insight_col = st.columns([2, 2, 2, 2])
            
            with type_col:
                st.markdown("**ğŸ“‚ ç±»å‹åˆ†å¸ƒ**")
                sorted_types = sorted(file_types.items(), key=lambda x: x[1], reverse=True)
                for i, (ftype, count) in enumerate(sorted_types[:5]):  # æ˜¾ç¤ºå‰5ç§
                    pct = (count / file_cnt * 100) if file_cnt > 0 else 0
                    bar = "â–ˆ" * int(pct / 5) + "â–‘" * (20 - int(pct / 5))
                    st.caption(f"{ftype}: {count} ({pct:.0f}%) {bar[:10]}")
                if len(sorted_types) > 5:
                    other_count = sum(c for _, c in sorted_types[5:])
                    other_pct = (other_count / file_cnt * 100) if file_cnt > 0 else 0
                    st.caption(f"å…¶ä»–: {other_count} ({other_pct:.0f}%)")
            
            with size_col:
                st.markdown("**ğŸ“Š å¤§å°åˆ†å¸ƒ**")
                # æŒ‰å¤§å°åˆ†ç±»
                size_ranges = {"<100KB": 0, "100KB-1MB": 0, "1MB-10MB": 0, ">10MB": 0}
                for f in manifest['files']:
                    size_bytes = f.get('size_bytes', 0)
                    if size_bytes < 100 * 1024:
                        size_ranges["<100KB"] += 1
                    elif size_bytes < 1024 * 1024:
                        size_ranges["100KB-1MB"] += 1
                    elif size_bytes < 10 * 1024 * 1024:
                        size_ranges["1MB-10MB"] += 1
                    else:
                        size_ranges[">10MB"] += 1
                
                for range_name, count in size_ranges.items():
                    if count > 0:
                        pct = (count / file_cnt * 100) if file_cnt > 0 else 0
                        st.caption(f"{range_name}: {count} ({pct:.0f}%)")
            
            with chunk_col:
                st.markdown("**ğŸ“¦ ç‰‡æ®µåˆ†å¸ƒ**")
                # æŒ‰ç‰‡æ®µæ•°åˆ†ç±»
                chunk_ranges = {"0ç‰‡æ®µ": 0, "1-5ç‰‡æ®µ": 0, "6-20ç‰‡æ®µ": 0, ">20ç‰‡æ®µ": 0}
                for f in manifest['files']:
                    chunk_count = len(f.get('doc_ids', []))
                    if chunk_count == 0:
                        chunk_ranges["0ç‰‡æ®µ"] += 1
                    elif chunk_count <= 5:
                        chunk_ranges["1-5ç‰‡æ®µ"] += 1
                    elif chunk_count <= 20:
                        chunk_ranges["6-20ç‰‡æ®µ"] += 1
                    else:
                        chunk_ranges[">20ç‰‡æ®µ"] += 1
                
                for range_name, count in chunk_ranges.items():
                    if count > 0:
                        pct = (count / file_cnt * 100) if file_cnt > 0 else 0
                        icon = "âš ï¸" if range_name == "0ç‰‡æ®µ" else "âœ…" if range_name == ">20ç‰‡æ®µ" else ""
                        st.caption(f"{icon}{range_name}: {count} ({pct:.0f}%)")
            
            with insight_col:
                st.markdown("**ğŸ’¡ æ•°æ®æ´å¯Ÿ**")
                if manifest['files']:
                    # çƒ­é—¨æ–‡ä»¶ï¼ˆåŸºäºå‘½ä¸­æ¬¡æ•°ï¼‰
                    hot_files = [(f['name'], f.get('hit_count', 0)) for f in manifest['files'] if f.get('hit_count', 0) > 0]
                    if hot_files:
                        hot_files.sort(key=lambda x: x[1], reverse=True)
                        top_file = hot_files[0]
                        st.caption(f"ğŸ”¥ æœ€çƒ­: {top_file[0][:12]}... ({top_file[1]}æ¬¡)")
                    
                    # æœ€å¤šç‰‡æ®µ
                    chunks_list = [(f['name'], len(f.get('doc_ids', []))) for f in manifest['files']]
                    most_chunks = max(chunks_list, key=lambda x: x[1]) if chunks_list else None
                    if most_chunks and most_chunks[1] > 0:
                        st.caption(f"ğŸ”¢ æœ€å¤šç‰‡æ®µ: {most_chunks[0][:12]}... ({most_chunks[1]})")
                    
                    # ä¸»è¦ç±»å‹
                    if file_types:
                        main_type = max(file_types.items(), key=lambda x: x[1])
                        st.caption(f"ğŸ“‚ ä¸»è¦ç±»å‹: {main_type[0]} ({main_type[1]}ä¸ª)")
                    
                    # æ™ºèƒ½å»ºè®®
                    if empty_docs > file_cnt * 0.1:
                        st.caption(f"âš ï¸ {empty_docs}ä¸ªç©ºæ–‡æ¡£éœ€å¤„ç†")
                    elif low_quality > file_cnt * 0.3:
                        st.caption(f"ğŸ’¡ å»ºè®®ä¼˜åŒ–æ–‡æ¡£è´¨é‡")
                    elif success_rate >= 95:
                        st.caption(f"ğŸ‰ çŸ¥è¯†åº“è´¨é‡ä¼˜ç§€")
                    else:
                        st.caption(f"âœ… çŸ¥è¯†åº“çŠ¶æ€è‰¯å¥½")
            
            st.divider()
            
            # å…ƒæ•°æ®ç»Ÿè®¡ï¼ˆæ–°å¢ï¼‰
            try:
                metadata_mgr = MetadataManager(db_path)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å…ƒæ•°æ®
                if metadata_mgr.metadata or metadata_mgr.stats:
                    with st.expander("ğŸ“Š å…ƒæ•°æ®ç»Ÿè®¡", expanded=False):
                        stat_col1, stat_col2, stat_col3 = st.columns(3)
                        
                        with stat_col1:
                            st.markdown("**ğŸ”¥ çƒ­é—¨æ–‡ä»¶ Top 5**")
                            hot_files = metadata_mgr.get_hot_files(top_k=5)
                            if hot_files:
                                for i, (fname, count) in enumerate(hot_files, 1):
                                    st.caption(f"{i}. {fname[:20]}... ({count})")
                            else:
                                st.caption("æš‚æ— æ•°æ®")
                        
                        with stat_col2:
                            st.markdown("**ğŸ“‚ æ–‡æ¡£åˆ†ç±»**")
                            categories = metadata_mgr.get_all_categories()
                            if categories:
                                for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True)[:5]:
                                    st.caption(f"{cat}: {count}")
                            else:
                                st.caption("æš‚æ— æ•°æ®")
                        
                        with stat_col3:
                            st.markdown("**ğŸ·ï¸ çƒ­é—¨å…³é”®è¯**")
                            keywords = metadata_mgr.get_all_keywords(top_k=8)
                            if keywords:
                                kw_text = " Â· ".join([f"{kw}({cnt})" for kw, cnt in keywords[:8]])
                                st.caption(kw_text)
                            else:
                                st.caption("æš‚æ— æ•°æ®")
                        
                        # é‡å¤æ–‡ä»¶æ£€æµ‹
                        duplicates = metadata_mgr.find_duplicates()
                        if duplicates:
                            st.divider()
                            st.markdown(f"**âš ï¸ å‘ç° {len(duplicates)} ç»„é‡å¤æ–‡ä»¶**")
                            for i, (file_hash, files) in enumerate(list(duplicates.items())[:2], 1):
                                st.caption(f"ç»„{i}: {', '.join([f[:15] for f in files[:3]])}...")
            except:
                pass  # å¦‚æœå…ƒæ•°æ®ä¸å­˜åœ¨ï¼Œé™é»˜è·³è¿‡
            
            st.divider()
            
            # å¿«é€Ÿæ“ä½œåŒº
            st.markdown("**âš¡ å¿«é€Ÿæ“ä½œ**")
            
            # å¿«é€Ÿæ“ä½œæŒ‰é’®ç»„
            quick_col1, quick_col2 = st.columns(2)
            
            # æ‰“å¼€çŸ¥è¯†åº“ç›®å½•
            with quick_col1:
                if st.button("ğŸ“‚ æ‰“å¼€ç›®å½•", use_container_width=True, help="åœ¨Finderä¸­æ‰“å¼€çŸ¥è¯†åº“æ–‡ä»¶å¤¹"):
                    import webbrowser
                    import urllib.parse
                    try:
                        file_url = 'file://' + urllib.parse.quote(os.path.abspath(db_path))
                        webbrowser.open(file_url)
                        st.toast("âœ… å·²åœ¨Finderä¸­æ‰“å¼€")
                    except Exception as e:
                        st.error(f"æ‰“å¼€å¤±è´¥: {e}")
            
            # å¤åˆ¶è·¯å¾„
            with quick_col2:
                if st.button("ğŸ“‹ å¤åˆ¶è·¯å¾„", use_container_width=True, help="å¤åˆ¶çŸ¥è¯†åº“è·¯å¾„åˆ°å‰ªè´´æ¿"):
                    try:
                        import subprocess
                        subprocess.run(["pbcopy"], input=db_path.encode(), check=True)
                        st.toast(f"âœ… å·²å¤åˆ¶: {db_path}")
                    except Exception as e:
                        st.info(f"ğŸ“ è·¯å¾„: {db_path}")
            
            st.write("")
            
            # æ‰¹é‡ç”Ÿæˆæ‘˜è¦
            files_without_summary = [f for f in manifest['files'] if not f.get('summary') and f.get('doc_ids')]
            if files_without_summary:
                if 'selected_for_summary' not in st.session_state:
                    st.session_state.selected_for_summary = set()
                
                selected_count = len(st.session_state.selected_for_summary)
                
                # å§‹ç»ˆæ˜¾ç¤ºæŒ‰é’®ï¼Œä½†æ ¹æ®é€‰ä¸­æ•°é‡å†³å®šæ˜¯å¦ç¦ç”¨
                button_label = f"âœ¨ ç”Ÿæˆæ‘˜è¦ ({selected_count})" if selected_count > 0 else "âœ¨ ç”Ÿæˆæ‘˜è¦ (è¯·å…ˆå‹¾é€‰æ–‡ä»¶)"
                button_disabled = selected_count == 0
                
                if st.button(button_label, use_container_width=True, type="primary", disabled=button_disabled):
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    from llama_index.core import StorageContext, load_index_from_storage as load_idx
                    storage_context = StorageContext.from_defaults(persist_dir=db_path)
                    idx = load_idx(storage_context)
                    retriever = idx.as_retriever(similarity_top_k=3)
                    
                    success_count = 0
                    for i, fname in enumerate(st.session_state.selected_for_summary):
                        status_text.text(f"æ­£åœ¨å¤„ç†: {fname} ({i+1}/{selected_count})")
                        try:
                            file_info = next((f for f in manifest['files'] if f['name'] == fname), None)
                            if file_info and file_info.get('doc_ids'):
                                # ä½¿ç”¨æ£€ç´¢å™¨è·å–æ–‡æ¡£å†…å®¹
                                nodes = retriever.retrieve(fname)
                                
                                doc_text = ""
                                for node in nodes:
                                    if hasattr(node, 'node') and hasattr(node.node, 'text'):
                                        doc_text += node.node.text + "\n"
                                    elif hasattr(node, 'text'):
                                        doc_text += node.text + "\n"
                                    if len(doc_text) > 2000:
                                        break
                                
                                if doc_text.strip():
                                    summary = generate_doc_summary(doc_text, fname)
                                    file_info['summary'] = summary
                                    success_count += 1
                        except Exception as e:
                            st.warning(f"âš ï¸ {fname}: {str(e)}")
                            
                            progress_bar.progress((i + 1) / selected_count)
                        
                        # ä¿å­˜ manifest
                        with open(get_manifest_path(db_path), 'w', encoding='utf-8') as f:
                            json.dump(manifest, f, indent=4, ensure_ascii=False)
                        
                        status_text.empty()
                        progress_bar.empty()
                        st.success(f"âœ… å·²ç”Ÿæˆ {success_count}/{selected_count} ä¸ªæ‘˜è¦")
                        st.session_state.selected_for_summary = set()
                        time.sleep(1)
                
                if st.button("ğŸ“¥ å¯¼å‡ºæ¸…å•", use_container_width=True):
                    export_data = f"çŸ¥è¯†åº“: {active_kb_name}\næ–‡ä»¶æ•°: {file_cnt}\nç‰‡æ®µæ•°: {total_chunks}\n\næ–‡ä»¶åˆ—è¡¨:\n"
                    for f in manifest['files']:
                        export_data += f"- {f['name']} ({f['type']}, {len(f.get('doc_ids', []))} ç‰‡æ®µ)\n"
                    st.download_button("ä¸‹è½½", export_data, f"{active_kb_name}_æ¸…å•.txt", use_container_width=True)
            
            st.divider()
            
            # æœç´¢ç­›é€‰æ’åºï¼ˆå•è¡Œè¶…ç´§å‡‘å¸ƒå±€ï¼‰
            col1, col2, col3, col4, col5, col6, col7 = st.columns([3, 1, 1, 1, 1, 1.2, 0.8])
            search_term = col1.text_input("ğŸ”", "", key="file_search", placeholder="æœç´¢æ–‡ä»¶å...", label_visibility="collapsed")
            filter_type = col2.selectbox("ğŸ“‚", ["å…¨éƒ¨"] + sorted(set(f.get('type', 'Unknown') for f in manifest['files'])), label_visibility="collapsed")
            
            # åˆ†ç±»ç­›é€‰
            all_categories = set(f.get('category', 'å…¶ä»–') for f in manifest['files'] if f.get('category'))
            filter_category = col3.selectbox("ğŸ“‹", ["å…¨éƒ¨"] + sorted(all_categories), label_visibility="collapsed") if all_categories else "å…¨éƒ¨"
            
            # çƒ­åº¦ç­›é€‰
            filter_heat = col4.selectbox("ğŸ”¥", ["å…¨éƒ¨", "é«˜é¢‘", "ä¸­é¢‘", "ä½é¢‘", "æœªç”¨"], label_visibility="collapsed")
            
            # è´¨é‡ç­›é€‰
            filter_quality = col5.selectbox("âœ…", ["å…¨éƒ¨", "ä¼˜ç§€", "æ­£å¸¸", "ä½è´¨", "ç©º"], label_visibility="collapsed")
            
            sort_by = col6.selectbox("æ’åº", ["æ—¶é—´â†“", "æ—¶é—´â†‘", "å¤§å°â†“", "å¤§å°â†‘", "åç§°", "çƒ­åº¦â†“", "ç‰‡æ®µâ†“"], label_visibility="collapsed")
            page_size = col7.selectbox("é¡µ", [10, 20, 50, 100], index=0, label_visibility="collapsed")
            
            # ç­›é€‰æ–‡ä»¶
            filtered_files = manifest['files']
            
            # æœç´¢
            if search_term:
                filtered_files = [f for f in filtered_files if search_term.lower() in f['name'].lower()]
            
            # ç±»å‹ç­›é€‰
            if filter_type != "å…¨éƒ¨":
                filtered_files = [f for f in filtered_files if f.get('type') == filter_type]
            
            # åˆ†ç±»ç­›é€‰
            if filter_category != "å…¨éƒ¨":
                filtered_files = [f for f in filtered_files if f.get('category') == filter_category]
            
            # çƒ­åº¦ç­›é€‰
            if filter_heat == "é«˜é¢‘":
                filtered_files = [f for f in filtered_files if f.get('hit_count', 0) > 10]
            elif filter_heat == "ä¸­é¢‘":
                filtered_files = [f for f in filtered_files if 3 < f.get('hit_count', 0) <= 10]
            elif filter_heat == "ä½é¢‘":
                filtered_files = [f for f in filtered_files if 0 < f.get('hit_count', 0) <= 3]
            elif filter_heat == "æœªç”¨":
                filtered_files = [f for f in filtered_files if f.get('hit_count', 0) == 0]
            
            # è´¨é‡ç­›é€‰
            if filter_quality == "ä¼˜ç§€":
                filtered_files = [f for f in filtered_files if len(f.get('doc_ids', [])) >= 10]
            elif filter_quality == "æ­£å¸¸":
                filtered_files = [f for f in filtered_files if 2 <= len(f.get('doc_ids', [])) < 10]
            elif filter_quality == "ä½è´¨":
                filtered_files = [f for f in filtered_files if 0 < len(f.get('doc_ids', [])) < 2]
            elif filter_quality == "ç©º":
                filtered_files = [f for f in filtered_files if len(f.get('doc_ids', [])) == 0]
            
            # æ’åº
            if sort_by == "æ—¶é—´â†“":
                filtered_files = sorted(filtered_files, key=lambda x: x.get('added_at', ''), reverse=True)
            elif sort_by == "æ—¶é—´â†‘":
                filtered_files = sorted(filtered_files, key=lambda x: x.get('added_at', ''))
            elif sort_by == "å¤§å°â†“":
                filtered_files = sorted(filtered_files, key=lambda x: x.get('size_bytes', 0), reverse=True)
            elif sort_by == "å¤§å°â†‘":
                filtered_files = sorted(filtered_files, key=lambda x: x.get('size_bytes', 0))
            elif sort_by == "åç§°A-Z":
                filtered_files = sorted(filtered_files, key=lambda x: x['name'].lower())
            elif sort_by == "çƒ­åº¦â†“":
                filtered_files = sorted(filtered_files, key=lambda x: x.get('hit_count', 0), reverse=True)
            elif sort_by == "ç‰‡æ®µâ†“":
                filtered_files = sorted(filtered_files, key=lambda x: len(x.get('doc_ids', [])), reverse=True)
            
            # åˆ†é¡µ
            total_files = len(filtered_files)
            total_pages = (total_files + page_size - 1) // page_size if total_files > 0 else 1
            
            if 'file_page' not in st.session_state:
                st.session_state.file_page = 1
            
            # ç¡®ä¿é¡µç åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if st.session_state.file_page > total_pages:
                st.session_state.file_page = 1
            
            # åˆ†é¡µæ§åˆ¶å’Œç»Ÿè®¡
            if total_files == 0:
                st.info("âŒ æ— åŒ¹é…æ–‡ä»¶")
            else:
                # ç®€æ´çš„ç­›é€‰ç»“æœï¼ˆå•è¡Œï¼‰
                filters = []
                if search_term: filters.append(f"'{search_term}'")
                if filter_type != "å…¨éƒ¨": filters.append(filter_type)
                if filter_category != "å…¨éƒ¨": filters.append(filter_category)
                if filter_heat != "å…¨éƒ¨": filters.append(filter_heat)
                if filter_quality != "å…¨éƒ¨": filters.append(filter_quality)
                
                if filters:
                    st.caption(f"**{' Â· '.join(filters)}** â†’ {total_files} ä¸ª")
                
                # åˆ†é¡µæ§åˆ¶
                if total_pages > 1:
                    col1, col2, col3 = st.columns([1, 2, 1])
                    with col2:
                        page_cols = st.columns([1, 3, 1])
                        if page_cols[0].button("â¬…ï¸ ä¸Šä¸€é¡µ", disabled=st.session_state.file_page <= 1):
                            st.session_state.file_page -= 1
                        page_cols[1].markdown(f"<div style='text-align:center'>ç¬¬ {st.session_state.file_page}/{total_pages} é¡µ</div>", unsafe_allow_html=True)
                        if page_cols[2].button("ä¸‹ä¸€é¡µ â¡ï¸", disabled=st.session_state.file_page >= total_pages):
                            st.session_state.file_page += 1
                
                # è®¡ç®—å½“å‰é¡µæ–‡ä»¶èŒƒå›´
                start_idx = (st.session_state.file_page - 1) * page_size
                end_idx = min(start_idx + page_size, total_files)
                
                # è¡¨å¤´
                cols = st.columns([0.5, 2.5, 1, 0.8, 1, 0.8, 1.2, 0.8])
                
                # å…¨é€‰å¤é€‰æ¡†
                current_page_files = [f['name'] for f in filtered_files[start_idx:end_idx] if not f.get('summary') and f.get('doc_ids')]
                if current_page_files:
                    all_selected = all(fname in st.session_state.selected_for_summary for fname in current_page_files)
                    select_all = cols[0].checkbox("å…¨é€‰", value=all_selected, key=f"select_all_page_{st.session_state.file_page}", label_visibility="collapsed")
                    
                    # æ ¹æ®å…¨é€‰æ¡†çŠ¶æ€æ›´æ–°é€‰ä¸­åˆ—è¡¨
                    if select_all:
                        st.session_state.selected_for_summary.update(current_page_files)
                    else:
                        st.session_state.selected_for_summary.difference_update(current_page_files)
                else:
                    cols[0].markdown("**âœ¨**")
                
                cols[1].markdown("**æ–‡ä»¶å**")
                cols[2].markdown("**ç±»å‹**")
                cols[3].markdown("**ç‰‡æ®µ**")
                cols[4].markdown("**å¤§å°**")
                cols[5].markdown("**è´¨é‡**")
                cols[6].markdown("**æ—¶é—´**")
                cols[7].markdown("**æ“ä½œ**")
                st.divider()
                
                # æ¸²æŸ“æ–‡ä»¶åˆ—è¡¨
                for i in range(start_idx, end_idx):
                    f = filtered_files[i]
                    # æ‰¾åˆ°åŸå§‹ç´¢å¼•ç”¨äºåˆ é™¤
                    orig_idx = manifest['files'].index(f)
                    
                    cols = st.columns([0.5, 2.5, 1, 0.8, 1, 0.8, 1.2, 0.8])
                    
                    # æ‘˜è¦å¤é€‰æ¡†ï¼ˆä»…å¯¹æ²¡æœ‰æ‘˜è¦çš„æ–‡ä»¶æ˜¾ç¤ºï¼‰
                    if not f.get('summary') and f.get('doc_ids'):
                        # æ ¹æ® session_state è®¾ç½®å¤é€‰æ¡†çš„å€¼
                        is_checked = f['name'] in st.session_state.selected_for_summary
                        checked = cols[0].checkbox("é€‰æ‹©", value=is_checked, key=f"sum_{f['name']}_{st.session_state.file_page}", label_visibility="collapsed")
                        
                        # æ›´æ–° session_state
                        if checked:
                            st.session_state.selected_for_summary.add(f['name'])
                        else:
                            st.session_state.selected_for_summary.discard(f['name'])
                    else:
                        cols[0].write("")
                    
                    # æ–‡ä»¶åï¼ˆå¸¦å›¾æ ‡ï¼‰
                    cols[1].caption(f'{f["icon"]} {f["name"]}')
                    
                    # ç±»å‹
                    cols[2].caption(f['type'])
                    
                    # ç‰‡æ®µæ•°
                    chunk_count = len(f.get('doc_ids', []))
                    cols[3].caption(str(chunk_count))
                    
                    # å¤§å°
                    cols[4].caption(f['size'])
                    
                    # è´¨é‡æŒ‡ç¤ºå™¨ï¼ˆæ–°å¢ï¼‰
                    if chunk_count == 0:
                        quality_icon = "âŒ"
                    elif chunk_count < 2:
                        quality_icon = "âš ï¸"
                    elif chunk_count < 10:
                        quality_icon = "âœ…"
                    else:
                        quality_icon = "ğŸ‰"
                    cols[5].caption(quality_icon)
                    
                    # æ—¶é—´
                    cols[6].caption(f['added_at'])
                    
                    # åˆ é™¤æŒ‰é’®
                    if cols[7].button("ğŸ—‘ï¸", key=f"del_{orig_idx}_{i}"):
                        with st.status(f"æ­£åœ¨åˆ é™¤ {f['name']}...", expanded=True) as status:
                            try:
                                ctx = StorageContext.from_defaults(persist_dir=db_path)
                                idx = load_index_from_storage(ctx)
                                for did in f.get('doc_ids', []):
                                    idx.delete_ref_doc(did, delete_from_docstore=True)
                                idx.storage_context.persist(persist_dir=db_path)
                                remove_file_from_manifest(db_path, f['name'])
                                status.update(label="âœ… å·²åˆ é™¤", state="complete")
                                st.session_state.chat_engine = None
                                time.sleep(1); st.rerun()
                            except Exception as e: st.error(str(e))
                    
                    # æ–‡ä»¶æ‘˜è¦å±•å¼€
                    if f.get('summary'):
                        with st.expander(f"ğŸ“– {f['summary'][:50]}...", expanded=False):
                            st.markdown(f.get('summary'))
                    
                    # æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
                    with st.expander(f"ğŸ“Š è¯¦æƒ… - {f['name']}", expanded=False):
                        chunk_count = len(f.get('doc_ids', []))
                        
                        # åŸºç¡€ä¿¡æ¯ï¼ˆ4åˆ—ç´§å‡‘æ˜¾ç¤ºï¼‰
                        detail_cols = st.columns(4)
                        detail_cols[0].metric("ğŸ“¦ ç‰‡æ®µ", chunk_count)
                        detail_cols[1].metric("ğŸ’¾ å¤§å°", f['size'])
                        detail_cols[2].metric("ğŸ“… æ—¶é—´", f['added_at'][:10])
                        detail_cols[3].metric("ğŸ·ï¸ ç±»å‹", f['type'])
                        
                        # è´¨é‡è¯„ä¼°ï¼ˆå•è¡Œç´§å‡‘æ˜¾ç¤ºï¼‰
                        if chunk_count == 0:
                            quality_info = "âŒ è§£æå¤±è´¥"
                        elif chunk_count < 2:
                            quality_info = "âš ï¸ ä½è´¨ï¼ˆå†…å®¹è¿‡å°‘ï¼‰"
                        elif chunk_count < 10:
                            quality_info = "âœ… æ­£å¸¸"
                        else:
                            quality_info = "ğŸ‰ ä¼˜ç§€ï¼ˆå†…å®¹ä¸°å¯Œï¼‰"
                        
                        estimated_chars = chunk_count * 500
                        st.caption(f"**è´¨é‡**: {quality_info} Â· **å­—ç¬¦**: ~{estimated_chars:,} Â· **å‘é‡**: {chunk_count}")
                        
                        # å…ƒæ•°æ®ä¿¡æ¯ï¼ˆæ–°å¢ï¼‰
                        if f.get('hit_count', 0) > 0 or f.get('keywords') or f.get('category'):
                            st.divider()
                            meta_cols = st.columns(4)
                            
                            # æ£€ç´¢ç»Ÿè®¡
                            hit_count = f.get('hit_count', 0)
                            avg_score = f.get('avg_score', 0.0)
                            heat = "ğŸ”¥" if hit_count > 10 else "ğŸ“Š" if hit_count > 3 else "ğŸ“¦" if hit_count > 0 else "â„ï¸"
                            
                            meta_cols[0].metric("ğŸ”¥ å‘½ä¸­", f"{hit_count} æ¬¡")
                            meta_cols[1].metric("â­ å¾—åˆ†", f"{avg_score:.2f}")
                            meta_cols[2].metric("ğŸŒ¡ï¸ çƒ­åº¦", heat)
                            
                            # æœ€åè®¿é—®
                            last_accessed = f.get('last_accessed')
                            if last_accessed:
                                meta_cols[3].metric("ğŸ• è®¿é—®", last_accessed[:10])
                            else:
                                meta_cols[3].metric("ğŸ• è®¿é—®", "ä»æœª")
                            
                            # åˆ†ç±»å’Œè¯­è¨€
                            category = f.get('category', 'å…¶ä»–')
                            language = f.get('language', 'unknown')
                            lang_map = {"zh": "ğŸ‡¨ğŸ‡³", "en": "ğŸ‡¬ğŸ‡§", "zh-en": "ğŸŒ", "unknown": "â“"}
                            lang_icon = lang_map.get(language, "â“")
                            
                            st.caption(f"**ğŸ“‚ åˆ†ç±»**: {category} Â· **ğŸŒ è¯­è¨€**: {lang_icon} {language}")
                            
                            # å…³é”®è¯
                            keywords = f.get('keywords', [])
                            if keywords:
                                st.caption(f"**ğŸ·ï¸ å…³é”®è¯**: {' Â· '.join(keywords[:5])}")
                            
                            # æ–‡ä»¶å“ˆå¸Œï¼ˆæŠ˜å ï¼‰
                            file_hash = f.get('file_hash', '')
                            if file_hash:
                                with st.expander("ğŸ” æ–‡ä»¶å“ˆå¸Œ", expanded=False):
                                    st.code(file_hash, language="text")
                        
                        # æ–‡æ¡£IDï¼ˆç´§å‡‘æ˜¾ç¤ºï¼‰
                        if f.get('doc_ids'):
                            if len(f['doc_ids']) <= 3:
                                st.caption(f"**ç‰‡æ®µID**: `{', '.join(f['doc_ids'])}`")
                            else:
                                st.caption(f"**ç‰‡æ®µID**: `{f['doc_ids'][0]}` ... (å…±{len(f['doc_ids'])}ä¸ª)")
                                with st.expander("æŸ¥çœ‹å…¨éƒ¨ID", expanded=False):
                                    st.code('\n'.join(f['doc_ids']), language=None)
                        else:
                            st.warning("âš ï¸ æœªç”Ÿæˆç‰‡æ®µ Â· å¯èƒ½åŸå› ï¼šæ–‡ä»¶ä¸ºç©º/æ ¼å¼ä¸æ”¯æŒ/å·²æŸå/åŠ å¯†")
                        
                        # ç›¸ä¼¼æ–‡ä»¶ï¼ˆç´§å‡‘æ˜¾ç¤ºï¼‰
                        if chunk_count > 0:
                            similar_files = [
                                other for other in manifest['files']
                                if other['name'] != f['name']
                                and other['type'] == f['type']
                                and abs(len(other.get('doc_ids', [])) - chunk_count) < chunk_count * 0.5
                            ][:3]
                            
                            if similar_files:
                                similar_names = [f"{s['icon']} {s['name'][:20]}..." for s in similar_files]
                                st.caption(f"**ç›¸ä¼¼**: {' Â· '.join(similar_names)}")
                        # ç”Ÿæˆæ‘˜è¦æŒ‰é’®ï¼ˆåªå¯¹æœ‰ç‰‡æ®µçš„æ–‡ä»¶æ˜¾ç¤ºï¼‰
                        if not f.get('summary') and f.get('doc_ids'):
                            if st.button("âœ¨ ç”Ÿæˆæ‘˜è¦", key=f"gen_sum_{f['name']}", use_container_width=True):
                                with st.spinner("ç”Ÿæˆä¸­..."):
                                    try:
                                        # ä½¿ç”¨æ£€ç´¢å™¨è·å–æ–‡æ¡£å†…å®¹
                                        from llama_index.core import StorageContext, load_index_from_storage as load_idx
                                        storage_context = StorageContext.from_defaults(persist_dir=db_path)
                                        idx = load_idx(storage_context)
                                        
                                        # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæŸ¥è¯¢ï¼Œæ£€ç´¢ç›¸å…³å†…å®¹
                                        retriever = idx.as_retriever(similarity_top_k=3)
                                        nodes = retriever.retrieve(f['name'])
                                        
                                        doc_text = ""
                                        for node in nodes:
                                            if hasattr(node, 'node') and hasattr(node.node, 'text'):
                                                doc_text += node.node.text + "\n"
                                            elif hasattr(node, 'text'):
                                                doc_text += node.text + "\n"
                                            if len(doc_text) > 2000:
                                                break
                                        
                                        if doc_text.strip():
                                            # ç”Ÿæˆæ‘˜è¦
                                            summary = generate_doc_summary(doc_text, f['name'])
                                            
                                            # æ›´æ–° manifest
                                            manifest = load_manifest(db_path)
                                            for file in manifest['files']:
                                                if file['name'] == f['name']:
                                                    file['summary'] = summary
                                                    break
                                            
                                            # ä¿å­˜ manifest
                                            with open(get_manifest_path(db_path), 'w', encoding='utf-8') as mf:
                                                json.dump(manifest, mf, indent=4, ensure_ascii=False)
                                            
                                            st.success("âœ… æ‘˜è¦å·²ç”Ÿæˆ")
                                        else:
                                            st.error("âŒ æ— æ³•è¯»å–æ–‡æ¡£å†…å®¹")
                                        time.sleep(0.5)
                                    except Exception as e:
                                        st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
                
                # åº•éƒ¨åˆ†é¡µï¼ˆæ–¹ä¾¿ç¿»é¡µï¼‰
                if total_pages > 1:
                    st.divider()
                    col1, col2, col3 = st.columns([1, 2, 1])
                    with col2:
                        page_cols = st.columns([1, 3, 1])
                        if page_cols[0].button("â¬…ï¸", key="prev_bottom", disabled=st.session_state.file_page <= 1):
                            st.session_state.file_page -= 1
                        page_cols[1].markdown(f"<div style='text-align:center'>ç¬¬ {st.session_state.file_page}/{total_pages} é¡µ Â· å…± {total_files} ä¸ªæ–‡ä»¶</div>", unsafe_allow_html=True)
                        if page_cols[2].button("â¡ï¸", key="next_bottom", disabled=st.session_state.file_page >= total_pages):
                            st.session_state.file_page += 1

    st.divider()

elif is_create_mode:
    st.markdown("""
    <div class="welcome-box">
        <h2>ğŸ‘‹ æ¬¢è¿ä½¿ç”¨çŸ¥è¯†åº“</h2>
        <p>è¯·åœ¨å·¦ä¾§ <b>ä¾§è¾¹æ </b> é…ç½®æ•°æ®æº (æ”¯æŒç²˜è´´è·¯å¾„æˆ–æ‹–æ‹½æ–‡ä»¶)ï¼Œç‚¹å‡» <b>ğŸš€ ç«‹å³åˆ›å»º</b> å¼€å§‹ã€‚</p>
    </div>
    """, unsafe_allow_html=True)


# è‡ªåŠ¨æ‘˜è¦ (ä»…åœ¨çŸ¥è¯†åº“é¦–æ¬¡åŠ è½½ä¸”æ— å†å²æ¶ˆæ¯æ—¶è§¦å‘)
if active_kb_name and st.session_state.chat_engine and not st.session_state.messages:
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        summary_placeholder = st.empty()
        with st.status("âœ¨ æ­£åœ¨åˆ†ææ–‡æ¡£ç”Ÿæˆæ‘˜è¦...", expanded=True):
            try:
                # ä½¿ç”¨çŸ¥è¯†åº“çš„æ¨¡å‹ï¼ˆå·²åœ¨æŒ‚è½½æ—¶è®¾ç½®ï¼Œæ— éœ€é‡å¤è®¾ç½®ï¼‰
                current_model = getattr(Settings.embed_model, '_model_name', 'Unknown')
                terminal_logger.info(f"ğŸ’¬ æ‘˜è¦ç”Ÿæˆä½¿ç”¨æ¨¡å‹: {current_model}")
                
                prompt = "è¯·ç”¨ä¸€æ®µè¯ç®€è¦æ€»ç»“æ­¤çŸ¥è¯†åº“çš„æ ¸å¿ƒå†…å®¹ã€‚ç„¶åï¼Œæå‡º3ä¸ªç”¨æˆ·å¯èƒ½æœ€å…³å¿ƒçš„é—®é¢˜ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦åºå·ã€‚"
                full = ""
                resp = st.session_state.chat_engine.stream_chat(prompt)
                
                for t in resp.response_gen:
                    full += t
                    summary_placeholder.markdown(full + "â–Œ")
                summary_placeholder.markdown(full)
                
                summary_lines = full.split('\n')
                summary = summary_lines[0]
                sug = [re.sub(r'^\d+\.\s*', '', q.strip()) for q in summary_lines[1:] if q.strip()][:3]

                st.session_state.messages.append({"role": "assistant", "content": summary, "suggestions": sug})
                save_chat_history(active_kb_name, st.session_state.messages)
                st.rerun()
            except Exception as e:
                error_msg = str(e)
                if "timed out" in error_msg.lower() or "timeout" in error_msg.lower():
                    summary_placeholder.info("â±ï¸ LLM å“åº”è¶…æ—¶ï¼Œå·²è·³è¿‡è‡ªåŠ¨æ‘˜è¦ã€‚æ‚¨å¯ä»¥ç›´æ¥å¼€å§‹æé—®ã€‚")
                    terminal_logger.warning(f"â±ï¸ æ‘˜è¦ç”Ÿæˆè¶…æ—¶: {e}")
                else:
                    summary_placeholder.warning(f"æ‘˜è¦ç”Ÿæˆå—é˜»: {e}")
                    terminal_logger.error(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
                st.session_state.messages.append({"role": "assistant", "content": "ğŸ‘‹ çŸ¥è¯†åº“å·²å°±ç»ªã€‚"})

# æ¸²æŸ“æ¶ˆæ¯
for msg_idx, msg in enumerate(st.session_state.messages):
    role = msg["role"]
    avatar = "ğŸ¤–" if role == "assistant" else "ğŸ§‘â€ğŸ’»"
    with st.chat_message(role, avatar=avatar):
        st.markdown(msg["content"])
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰- ä½¿ç”¨æ–°ç»„ä»¶ (Stage 3.1)
        if "stats" in msg and msg["stats"]:
            render_message_stats(msg["stats"])
        
        # æ¸²æŸ“å¼•ç”¨æº - ä½¿ç”¨æ–°ç»„ä»¶ (Stage 3.1)
        if "sources" in msg:
            render_source_references(msg["sources"], expanded=False)
        
        # å¼•ç”¨æŒ‰é’® (P2 æ¢å¤åŠŸèƒ½)
        if role == "assistant":
            if st.button("ğŸ“Œ å¼•ç”¨æ­¤å›å¤", key=f"quote_{msg_idx}"):
                st.session_state.quote_content = msg["content"]
                st.rerun()

        # æ¸²æŸ“é™æ€å»ºè®® (ä»…ç”¨äºè‡ªåŠ¨æ‘˜è¦)
        is_last_message = msg_idx == len(st.session_state.messages) - 1
        if "suggestions" in msg and msg["suggestions"] and is_last_message and not st.session_state.suggestions_history:
            st.write("")
            for idx, q in enumerate(msg["suggestions"]):
                if st.button(f"ğŸ‘‰ {q}", key=f"sug_{msg_idx}_{idx}", use_container_width=True):
                    click_btn(q)
    
    # åœ¨æœ€åä¸€æ¡ assistant æ¶ˆæ¯ä¹‹åæ˜¾ç¤ºåŠ¨æ€è¿½é—®æ¨èï¼ˆåœ¨ chat_message å®¹å™¨å¤–ï¼‰
    is_last_message = msg_idx == len(st.session_state.messages) - 1
    if is_last_message and msg["role"] == "assistant" and active_kb_name and st.session_state.chat_engine:
        import hashlib
        msg_hash = hashlib.md5(msg['content'][:100].encode()).hexdigest()[:8]
        
        st.divider()
        
        @st.fragment
        def suggestions_fragment():
            if st.session_state.suggestions_history:
                st.markdown("##### ğŸš€ è¿½é—®æ¨è")
                for idx, q in enumerate(st.session_state.suggestions_history):
                    if st.button(f"ğŸ‘‰ {q}", key=f"dyn_sug_{msg_hash}_{idx}", use_container_width=True):
                        click_btn(q)
            
            if st.button("âœ¨ ç»§ç»­æ¨è 3 ä¸ªè¿½é—® (æ— é™è¿½é—®)", key=f"gen_more_{msg_hash}", type="secondary", use_container_width=True):
                with st.spinner("â³ æ­£åœ¨ç”Ÿæˆæ–°é—®é¢˜..."):
                    all_history_questions = [m['content'] for m in st.session_state.messages if m['role'] == 'user']
                    all_history_questions.extend(st.session_state.suggestions_history)
                    
                    new_sugs = generate_follow_up_questions(
                        context_text=msg['content'], 
                        num_questions=3,
                        existing_questions=all_history_questions,
                        query_engine=st.session_state.chat_engine if st.session_state.get('chat_engine') else None
                    )
                    
                    if new_sugs:
                        st.session_state.suggestions_history.extend(new_sugs)
                        st.rerun(scope="fragment")
                    else:
                        st.warning("æœªèƒ½ç”Ÿæˆæ›´å¤šè¿½é—®ï¼Œè¯·å°è¯•è¾“å…¥æ–°é—®é¢˜ã€‚")
            
        suggestions_fragment()

# å¼•ç”¨å†…å®¹é¢„è§ˆåŒº
if st.session_state.get("quote_content"):
    quote_text = st.session_state.quote_content
    display_text = quote_text[:60].replace('\n', ' ') + "..." if len(quote_text) > 60 else quote_text
    
    with st.container():
        st.info(f"ğŸ“Œ **å·²å¼•ç”¨**: {display_text}")
        col1, col2 = st.columns([8, 2])
        col1.caption("åŸºäºæ­¤å†…å®¹æé—®...")
        if col2.button("å–æ¶ˆå¼•ç”¨", key="cancel_quote", use_container_width=True):
            st.session_state.quote_content = None
            st.rerun()

# å¤„ç†è¾“å…¥
user_input = st.chat_input("è¾“å…¥é—®é¢˜...")
final_prompt = st.session_state.prompt_trigger if st.session_state.prompt_trigger else user_input
if st.session_state.prompt_trigger: st.session_state.prompt_trigger = None

# æ˜¾ç¤ºé˜Ÿåˆ—çŠ¶æ€
if st.session_state.get('is_processing'):
    st.info("â³ æ­£åœ¨å¤„ç†ä¸Šä¸€ä¸ªé—®é¢˜ï¼Œæ–°é—®é¢˜å·²æ’é˜Ÿ...")

if final_prompt:
    if not st.session_state.chat_engine:
        st.error("è¯·å…ˆç‚¹å‡»å·¦ä¾§ã€ğŸš€ æ‰§è¡Œå¤„ç†ã€‘å¯åŠ¨ç³»ç»Ÿ")
    else:
        st.session_state.suggestions_history = []
        st.session_state.is_processing = True  # æ ‡è®°æ­£åœ¨å¤„ç†
        
        # å¼ºåˆ¶æ£€æµ‹çŸ¥è¯†åº“ç»´åº¦å¹¶åˆ‡æ¢æ¨¡å‹ï¼ˆé™é»˜å¤„ç†ï¼Œä¸æ˜¾ç¤ºåŠ è½½ï¼‰
        db_path = os.path.join(output_base, active_kb_name)
        kb_dim = get_kb_embedding_dim(db_path)
        
        # ä¸ºå†å²çŸ¥è¯†åº“è‡ªåŠ¨ä¿å­˜ä¿¡æ¯
        auto_save_kb_info(db_path, embed_model)
        
        # ç»´åº¦æ˜ å°„
        model_map = {
            512: "BAAI/bge-small-zh-v1.5",
            768: "BAAI/bge-base-zh-v1.5",
            1024: "BAAI/bge-m3"
        }
        
        # å¦‚æœæ£€æµ‹åˆ°ç»´åº¦ï¼Œå¼ºåˆ¶åˆ‡æ¢
        if kb_dim and kb_dim in model_map:
            required_model = model_map[kb_dim]
            if embed_model != required_model:
                print(f"ğŸ”„ å¼ºåˆ¶åˆ‡æ¢æ¨¡å‹: {embed_model} â†’ {required_model} (ç»´åº¦: {kb_dim}D)")
                embed_model = required_model
                embed = get_embed(embed_provider, embed_model, embed_key, embed_url)
                if embed:
                    Settings.embed_model = embed
                    print(f"âœ… æ¨¡å‹å·²åˆ‡æ¢")
        else:
            # ç»´åº¦æ£€æµ‹å¤±è´¥æ—¶ï¼Œé™çº§åˆ°æœ€å°æ¨¡å‹ï¼ˆ512ç»´ï¼‰
            print(f"âš ï¸ ç»´åº¦æ£€æµ‹å¤±è´¥ï¼Œé™çº§åˆ°æœ€å°æ¨¡å‹")
            fallback_model = "BAAI/bge-small-zh-v1.5"
            if embed_model != fallback_model:
                print(f"ğŸ”„ é™çº§åˆ‡æ¢: {embed_model} â†’ {fallback_model}")
                embed_model = fallback_model
                embed = get_embed(embed_provider, embed_model, embed_key, embed_url)
                if embed:
                    Settings.embed_model = embed
                    print(f"âœ… å·²é™çº§åˆ°æœ€å°æ¨¡å‹")
        
        terminal_logger.separator("çŸ¥è¯†åº“æŸ¥è¯¢")
        terminal_logger.start_operation("æŸ¥è¯¢", f"çŸ¥è¯†åº“: {active_kb_name}")
        
        # æ˜¾ç¤ºç³»ç»Ÿèµ„æºåˆ©ç”¨
        import psutil
        mem_percent = psutil.virtual_memory().percent
        terminal_logger.info(f"ç³»ç»Ÿå†…å­˜ä½¿ç”¨: {mem_percent:.1f}%")
        
        # å¤„ç†å¼•ç”¨å†…å®¹
        if st.session_state.get("quote_content"):
            quoted_text = st.session_state.quote_content
            # é™åˆ¶å¼•ç”¨é•¿åº¦ï¼Œé˜²æ­¢ prompt è¿‡é•¿
            if len(quoted_text) > 2000:
                quoted_text = quoted_text[:2000] + "...(å·²æˆªæ–­)"
            
            # æ„å»ºåŒ…å«å¼•ç”¨çš„ prompt
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿®æ”¹ final_prompt å‘é€ç»™ LLMï¼Œä½†åœ¨ UI ä¸Šç”¨æˆ·åªçœ‹åˆ°è‡ªå·±çš„ç®€çŸ­è¾“å…¥
            # ä¸ºäº†å†å²è®°å½•çš„å®Œæ•´æ€§ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¿å­˜ç»„åˆåçš„ promptï¼Œæˆ–è€…åˆ†å¼€ä¿å­˜
            # è¿™é‡Œé€‰æ‹©ä¿®æ”¹ final_promptï¼Œè¿™æ ·å†å²è®°å½•é‡Œä¹Ÿæ˜¯å®Œæ•´çš„ï¼Œæ–¹ä¾¿åç»­å›é¡¾
            original_prompt = final_prompt
            final_prompt = f"åŸºäºä»¥ä¸‹å¼•ç”¨å†…å®¹ï¼š\n> {quoted_text}\n\næˆ‘çš„é—®é¢˜æ˜¯ï¼š{original_prompt}"
            
            # æ¸…é™¤å¼•ç”¨çŠ¶æ€
            st.session_state.quote_content = None
            terminal_logger.info("ğŸ“Œ å·²åº”ç”¨å¼•ç”¨å†…å®¹")
        
        logger.log_user_question(final_prompt, kb_name=active_kb_name)
        
        st.session_state.messages.append({"role": "user", "content": final_prompt})
        if active_kb_name: save_chat_history(active_kb_name, st.session_state.messages)

        with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"): st.markdown(final_prompt)
        
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            msg_placeholder = st.empty()
            with st.status("â³ æ­£åœ¨æ£€ç´¢å¹¶æ€è€ƒ...", expanded=True):
                try:
                    # å¼€å§‹è®¡æ—¶
                    start_time = time.time()
                    
                    # èµ„æºç›‘æ§
                    cpu_start, mem_start, gpu_start, _ = check_resource_usage(threshold=80.0)
                    terminal_logger.info(f"ğŸ”‹ èµ„æºçŠ¶æ€: CPU {cpu_start:.1f}% | å†…å­˜ {mem_start:.1f}% | GPU {gpu_start:.1f}%")
                    
                    # æ˜¾ç¤ºå¯ç”¨çš„æ£€ç´¢å¢å¼ºåŠŸèƒ½
                    enhancements = []
                    if st.session_state.get('enable_bm25', False):
                        enhancements.append("BM25æ··åˆæ£€ç´¢")
                    if st.session_state.get('enable_rerank', False):
                        enhancements.append("Re-rankingé‡æ’åº")
                    
                    if enhancements:
                        enhancement_str = " + ".join(enhancements)
                        terminal_logger.info(f"ğŸ¯ æ£€ç´¢å¢å¼º: {enhancement_str}")
                        logger.log("æŸ¥è¯¢å¯¹è¯", "æ£€ç´¢å¢å¼º", f"å¯ç”¨åŠŸèƒ½: {enhancement_str}")
                    
                    with terminal_logger.timer("æ£€ç´¢ç›¸å…³æ–‡æ¡£"):
                        logger.log_retrieval_start(kb_name=active_kb_name)
                        
                        # ç¡®ä¿ embedding æ¨¡å‹å·²è®¾ç½®
                        embed = get_embed(embed_provider, embed_model, embed_key, embed_url)
                        if embed:
                            Settings.embed_model = embed
                        
                        # GPUåŠ é€Ÿæ£€ç´¢ - æ‰¹é‡å¤„ç†
                        retrieval_start = time.time()
                        response = st.session_state.chat_engine.stream_chat(final_prompt)
                        retrieval_time = time.time() - retrieval_start
                        
                        terminal_logger.info(f"ğŸ” æ£€ç´¢è€—æ—¶: {retrieval_time:.2f}s (GPUåŠ é€Ÿ)")
                        
                        full_text = ""
                        # æµå¼è¾“å‡º + èµ„æºæ§åˆ¶
                        token_count = 0 # è¿™é‡Œçš„è®¡æ•°ä»…ç”¨äºè¿›åº¦ä¼°ç®—
                        full_text = ""
                        
                        for token in response.response_gen:
                            full_text += token
                            msg_placeholder.markdown(full_text + "â–Œ")
                            
                            # æ¯50ä¸ªtokenæ£€æŸ¥èµ„æº
                            token_count += 1
                            if token_count % 50 == 0:
                                cpu_now, mem_now, gpu_now, should_throttle = check_resource_usage(threshold=80.0)
                                if should_throttle:
                                    terminal_logger.info(f"âš ï¸ èµ„æºé™æµ: CPU {cpu_now:.1f}% | å†…å­˜ {mem_now:.1f}% | GPU {gpu_now:.1f}%")
                                    time.sleep(0.05)  # è½»å¾®å»¶è¿Ÿ
                        
                        msg_placeholder.markdown(full_text)
                    
                    # æå– token ç»Ÿè®¡ (ä¼˜å…ˆä½¿ç”¨çœŸå®æ•°æ®)
                    prompt_tokens = 0
                    completion_tokens = 0
                    
                    if hasattr(response, 'raw') and response.raw:
                        usage = response.raw.get('usage', {})
                        prompt_tokens = usage.get('prompt_tokens', 0)
                        completion_tokens = usage.get('completion_tokens', 0)
                    
                    # å¦‚æœæ²¡æœ‰çœŸå® Usageï¼Œåˆ™è¿›è¡Œä¼°ç®—
                    if completion_tokens == 0:
                        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦çº¦0.6 tokenï¼Œè‹±æ–‡å­—ç¬¦çº¦0.25 token (WordCount)
                        # è¿™é‡Œä½¿ç”¨æ›´é€šç”¨çš„ä¼°ç®—ï¼šä¸­æ–‡ * 1.5, è‹±æ–‡ * 0.5 (token count)
                        # æˆ–è€…ç›´æ¥æ˜¾ç¤ºå­—ç¬¦æ•°æ›´å‡†ç¡®
                        total_chars = len(full_text)
                        chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', full_text))
                        # æ··åˆä¼°ç®—
                        completion_tokens = int((chinese_chars * 1.5) + ((total_chars - chinese_chars) * 0.3))
                        token_count = completion_tokens # æ›´æ–°ä¸ºæ›´å‡†ç¡®çš„ä¼°ç®—å€¼
                    else:
                        token_count = completion_tokens # ä½¿ç”¨çœŸå®å€¼

                    # å¤šæ ¸å¹¶è¡Œå¤„ç†èŠ‚ç‚¹
                    srcs = []
                    if response.source_nodes:
                        logger.log_retrieval_result(len(response.source_nodes), kb_name=active_kb_name)
                        terminal_logger.data_summary("æ£€ç´¢ç»“æœ", {
                            "æŸ¥è¯¢": final_prompt[:50] + "..." if len(final_prompt) > 50 else final_prompt,
                            "ç›¸å…³æ–‡æ¡£": len(response.source_nodes),
                            "çŸ¥è¯†åº“": active_kb_name
                        })
                        
                        # å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†èŠ‚ç‚¹ï¼ˆçœŸæ­£åˆ©ç”¨å¤šæ ¸CPUï¼‰
                        max_workers = max(2, os.cpu_count() - 1)  # ä¿ç•™1æ ¸ç»™ç³»ç»Ÿ
                        
                        # æå–èŠ‚ç‚¹æ•°æ®ï¼ˆåºåˆ—åŒ–å‹å¥½ï¼‰
                        node_data = []
                        for node in response.source_nodes:
                            # å®‰å…¨æå–æ–‡æœ¬
                            text = ''
                            try:
                                if hasattr(node, 'get_text'):
                                    text = node.get_text()
                                elif hasattr(node, 'text'):
                                    text = node.text
                                elif hasattr(node, 'node') and hasattr(node.node, 'text'):
                                    text = node.node.text
                                else:
                                    text = str(node)[:150]
                            except:
                                text = str(node)[:150]
                            
                            node_data.append({
                                'metadata': getattr(node, 'metadata', {}),
                                'score': getattr(node, 'score', 0.0),
                                'text': text
                            })
                        
                        # æ™ºèƒ½å¤šè¿›ç¨‹å¤„ç† (ä¼˜åŒ–ç‰ˆ - ä¸“å®¶å»ºè®® P2)
                        if len(node_data) > 10:
                            max_workers = max(2, min(os.cpu_count() - 1, len(node_data) // 2))
                            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                                srcs = [s for s in executor.map(_process_node_worker, 
                                       [(d, active_kb_name) for d in node_data]) if s]
                            terminal_logger.info(f"âš¡ å¤šè¿›ç¨‹å¤„ç†: {len(srcs)} ä¸ªèŠ‚ç‚¹ | ä½¿ç”¨ {max_workers} è¿›ç¨‹")
                        else:
                            # å°‘é‡èŠ‚ç‚¹ç›´æ¥ä¸²è¡Œå¤„ç†ï¼Œé¿å…è¿›ç¨‹å¼€é”€
                            srcs = [_process_node_worker((d, active_kb_name)) for d in node_data]
                            srcs = [s for s in srcs if s]
                            terminal_logger.info(f"âš¡ ä¸²è¡Œå¤„ç†: {len(srcs)} ä¸ªèŠ‚ç‚¹ (å°‘é‡æ•°æ®)")
                    
                    logger.log_answer_complete(
                        kb_name=active_kb_name, 
                        model=llm_model, 
                        tokens=token_count,
                        prompt_tokens=prompt_tokens,
                        completion_tokens=completion_tokens
                    )
                    
                    # è®¡ç®—æ€»è€—æ—¶
                    total_time = time.time() - start_time
                    
                    # èµ„æºç›‘æ§ç»“æŸ
                    cpu_end, mem_end, gpu_end, _ = check_resource_usage(threshold=80.0)
                    terminal_logger.info(f"âœ… èµ„æºå³°å€¼: CPU {max(cpu_start, cpu_end):.1f}% | å†…å­˜ {max(mem_start, mem_end):.1f}% | GPU {max(gpu_start, gpu_end):.1f}%")
                    terminal_logger.complete_operation(f"æŸ¥è¯¢å®Œæˆ (è€—æ—¶ {total_time:.2f}s)")
                    
                    # å‡†å¤‡ç»Ÿè®¡ä¿¡æ¯
                    tokens_per_sec = token_count / total_time if total_time > 0 else 0
                    stats = {
                        "time": total_time,
                        "tokens": token_count,
                        "tokens_per_sec": tokens_per_sec,
                        "prompt_tokens": prompt_tokens,
                        "completion_tokens": completion_tokens,
                        "cpu": max(cpu_start, cpu_end),
                        "mem": max(mem_start, mem_end),
                        "gpu": max(gpu_start, gpu_end)
                    }
                    
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": full_text, 
                        "sources": srcs,
                        "stats": stats
                    })
                    # å†å²è®°å½•ä¿å­˜å·²ç§»åŠ¨åˆ°æµç¨‹æœ«å°¾
                    
                    # åœ¨å‰ç«¯æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ (ä¼˜åŒ–ç‰ˆ - ä¸“å®¶å»ºè®® P2)
                    # 1. ç®€å•æ¦‚è§ˆ
                    stats_simple = f"â±ï¸ {total_time:.1f}ç§’ | ğŸ“ çº¦ {token_count} å­—ç¬¦"
                    st.caption(stats_simple)
                    
                    # 2. è¯¦ç»†ä¿¡æ¯ (æŠ˜å )
                    with st.expander("ğŸ“Š è¯¦ç»†ç»Ÿè®¡", expanded=False):
                        st.caption(f"ğŸš€ é€Ÿåº¦: {tokens_per_sec:.1f} tokens/s")
                        if prompt_tokens:
                            st.caption(f"ğŸ“¥ è¾“å…¥: {prompt_tokens} | ğŸ“¤ è¾“å‡º: {completion_tokens}")
                        st.caption(f"ğŸ’» èµ„æº: CPU {max(cpu_start, cpu_end):.1f}% | å†…å­˜ {max(mem_start, mem_end):.1f}% | GPU {max(gpu_start, gpu_end):.1f}%")
                    
                    # é—®ç­”ç»“æŸåï¼Œè‡ªåŠ¨ç”Ÿæˆåˆå§‹è¿½é—®ï¼Œå¹¶æ·»åŠ åˆ° suggestions_history
                    # ä½¿ç”¨ container æ¥æ˜¾ç¤ºåŠ è½½çŠ¶æ€ï¼Œé¿å…ç•Œé¢è·³åŠ¨
                    st.divider()
                    sug_container = st.empty()
                    sug_container.caption("âœ¨ æ­£åœ¨ç”Ÿæˆæ¨èé—®é¢˜...")
                    initial_sugs = generate_follow_up_questions(
                        full_text, 
                        num_questions=3,
                        query_engine=st.session_state.chat_engine if st.session_state.get('chat_engine') else None
                    )
                    sug_container.empty()
                    
                    if initial_sugs:
                        st.session_state.suggestions_history.extend(initial_sugs)
                        terminal_logger.info(f"âœ¨ ç”Ÿæˆ {len(initial_sugs)} ä¸ªæ¨èé—®é¢˜")
                        
                        # ç«‹å³æ˜¾ç¤ºç”Ÿæˆçš„æ¨èé—®é¢˜ (æ— éœ€ç­‰å¾…é‡ç»˜)
                        st.markdown("##### ğŸš€ è¿½é—®æ¨è")
                        for idx, q in enumerate(initial_sugs):
                            if st.button(f"ğŸ‘‰ {q}", key=f"temp_sug_{int(time.time())}_{idx}", use_container_width=True):
                                click_btn(q)
                    else:
                        terminal_logger.info("âš ï¸ æ¨èé—®é¢˜ç”Ÿæˆå¤±è´¥")
                    
                    # å»¶è¿Ÿä¿å­˜ï¼šç¡®è®¤æ‰€æœ‰æ­¥éª¤ï¼ˆåŒ…æ‹¬æ¨èé—®é¢˜ï¼‰éƒ½æˆåŠŸåå†ä¿å­˜
                    if active_kb_name: save_chat_history(active_kb_name, st.session_state.messages)
                    
                    # é‡Šæ”¾å†…å­˜
                    cleanup_memory()
                    terminal_logger.info("ğŸ§¹ å¯¹è¯å®Œæˆï¼Œå†…å­˜å·²æ¸…ç†")
                    
                    st.session_state.is_processing = False  # å¤„ç†å®Œæˆ
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°é—®é¢˜æ’é˜Ÿ
                    if st.session_state.prompt_trigger:
                        st.rerun()  # åªåœ¨æœ‰æ–°é—®é¢˜æ—¶æ‰é‡æ–°è¿è¡Œ
                except Exception as e: 
                    print(f"âŒ æŸ¥è¯¢å‡ºé”™: {e}\n")
                    st.error(f"å‡ºé”™: {e}")
                    
                    # å‘ç”Ÿé”™è¯¯ï¼Œå›æ»šæœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆå¦‚æœæ˜¯ assistant ç”Ÿæˆçš„ï¼‰
                    # é¿å…ä¿å­˜ä¸å®Œæ•´çš„å›ç­”
                    if st.session_state.messages and st.session_state.messages[-1]['role'] == 'assistant':
                        st.session_state.messages.pop()
                    
                    # é‡Šæ”¾å†…å­˜
                    cleanup_memory()
                    terminal_logger.info("ğŸ§¹ é”™è¯¯å¤„ç†å®Œæˆï¼Œå†…å­˜å·²æ¸…ç†")
                    st.session_state.is_processing = False