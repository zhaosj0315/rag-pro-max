"""
æŸ¥è¯¢å¤„ç†å™¨æ¨¡å—
è´Ÿè´£å¤„ç†ç”¨æˆ·æŸ¥è¯¢ã€æµå¼å“åº”å’Œç»“æžœå¤„ç†
"""

import os
import re
import time
import streamlit as st
from llama_index.core import Settings

from src.app_logging import LogManager
from src.chat import HistoryManager
from src.chat_utils_improved import generate_follow_up_questions_safe as generate_follow_up_questions
from src.utils.memory import cleanup_memory
from src.utils.model_manager import load_embedding_model
from src.utils.enhanced_cache import smart_cache_manager

logger = LogManager()


def process_node_worker(args):
    """å¤„ç†å•ä¸ªèŠ‚ç‚¹çš„å·¥ä½œå‡½æ•°"""
    node_data, active_kb_name = args
    
    try:
        metadata = node_data.get('metadata', {})
        score = node_data.get('score', 0.0)
        text = node_data.get('text', '')
        
        # æå–æ–‡ä»¶å
        file_name = metadata.get('file_name', 'Unknown')
        if not file_name or file_name == 'Unknown':
            file_name = metadata.get('filename', 'Unknown')
        
        # æ¸…ç†æ–‡æœ¬
        clean_text = text.replace('\n', ' ').strip()
        if len(clean_text) > 150:
            clean_text = clean_text[:150] + "..."
        
        return {
            'file_name': file_name,
            'score': score,
            'text': clean_text,
            'metadata': metadata
        }
    except Exception as e:
        logger.warning(f"å¤„ç†èŠ‚ç‚¹å¤±è´¥: {e}")
        return None


class QueryProcessor:
    """æŸ¥è¯¢å¤„ç†å™¨"""
    
    def __init__(self):
        self.executor = ParallelExecutor()
    
    @smart_cache_manager.cached_query
    def process_query(self, query, chat_engine, active_kb_name, embed_provider, embed_model, embed_key, embed_url):
        """å¤„ç†æŸ¥è¯¢å¹¶è¿”å›žç»“æžœ"""
        try:
            logger.separator("çŸ¥è¯†åº“æŸ¥è¯¢")
            logger.start_operation("æŸ¥è¯¢", f"çŸ¥è¯†åº“: {active_kb_name}")
            logger.log("INFO", f"ç”¨æˆ·æé—®: {query}", stage="æŸ¥è¯¢å¯¹è¯", details={"kb_name": active_kb_name})
            
            # å¼€å§‹è®¡æ—¶
            start_time = time.time()
            
            # æ˜¾ç¤ºå¯ç”¨çš„æ£€ç´¢å¢žå¼ºåŠŸèƒ½
            enhancements = []
            if st.session_state.get('enable_bm25', False):
                enhancements.append("BM25æ··åˆæ£€ç´¢")
            if st.session_state.get('enable_rerank', False):
                enhancements.append("Re-rankingé‡æŽ’åº")
            
            if enhancements:
                enhancement_str = " + ".join(enhancements)
                logger.info(f"ðŸŽ¯ æ£€ç´¢å¢žå¼º: {enhancement_str}")
            
            with logger.timer("æ£€ç´¢ç›¸å…³æ–‡æ¡£"):
                logger.log("INFO", "å¼€å§‹æ£€ç´¢ç›¸å…³æ–‡æ¡£", stage="æŸ¥è¯¢å¯¹è¯", details={"kb_name": active_kb_name})
                
                # ç¡®ä¿ embedding æ¨¡åž‹å·²è®¾ç½®
                embed = load_embedding_model(embed_provider, embed_model, embed_key, embed_url)
                if embed:
                    Settings.embed_model = embed
                
                # GPUåŠ é€Ÿæ£€ç´¢ - æ·»åŠ è¶…æ—¶æŽ§åˆ¶
                retrieval_start = time.time()
                
                try:
                    # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
                    import signal
                    
                    def timeout_handler(signum, frame):
                        raise TimeoutError("æ£€ç´¢è¶…æ—¶")
                    
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(30)  # 30ç§’è¶…æ—¶
                    
                    response = chat_engine.stream_chat(query)
                    signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                    
                except TimeoutError:
                    logger.error("â° æ£€ç´¢è¶…æ—¶ (30s)ï¼Œè¯·å°è¯•ç®€åŒ–æŸ¥è¯¢")
                    yield {"type": "error", "content": "æ£€ç´¢è¶…æ—¶ï¼Œè¯·å°è¯•ç®€åŒ–æŸ¥è¯¢æˆ–ç¨åŽé‡è¯•"}
                    return
                except Exception as e:
                    signal.alarm(0)  # ç¡®ä¿å–æ¶ˆè¶…æ—¶
                    raise e
                retrieval_time = time.time() - retrieval_start
                
                logger.info(f"ðŸ” æ£€ç´¢è€—æ—¶: {retrieval_time:.2f}s (GPUåŠ é€Ÿ)")
                
                # æµå¼è¾“å‡ºå¤„ç†
                full_text = ""
                for token in response.response_gen:
                    full_text += token
                    yield {"type": "token", "content": token, "full_text": full_text}
                
                # å¤„ç†å®Œæˆ
                yield {"type": "complete", "content": full_text, "response": response, "start_time": start_time}
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            yield {"type": "error", "content": str(e)}
    
    def process_response_complete(self, full_text, response, start_time, active_kb_name, llm_model):
        """å¤„ç†å“åº”å®ŒæˆåŽçš„é€»è¾‘"""
        try:
            # æå– token ç»Ÿè®¡
            prompt_tokens = 0
            completion_tokens = 0
            
            if hasattr(response, 'raw') and response.raw:
                usage = response.raw.get('usage', {})
                prompt_tokens = usage.get('prompt_tokens', 0)
                completion_tokens = usage.get('completion_tokens', 0)
            
            # å¦‚æžœæ²¡æœ‰çœŸå®ž Usageï¼Œåˆ™è¿›è¡Œä¼°ç®—
            if completion_tokens == 0:
                total_chars = len(full_text)
                chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', full_text))
                completion_tokens = int((chinese_chars * 1.5) + ((total_chars - chinese_chars) * 0.3))
            
            # å¤„ç†æºèŠ‚ç‚¹
            srcs = []
            if response.source_nodes:
                logger.log("INFO", f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(response.source_nodes)} ä¸ªç›¸å…³æ–‡æ¡£", 
                          stage="æŸ¥è¯¢å¯¹è¯", details={"kb_name": active_kb_name})
                
                # æå–èŠ‚ç‚¹æ•°æ®
                node_data = []
                for node in response.source_nodes:
                    text = self._extract_node_text(node)
                    node_data.append({
                        'metadata': getattr(node, 'metadata', {}),
                        'score': getattr(node, 'score', 0.0),
                        'text': text
                    })
                
                # å¹¶è¡Œå¤„ç†èŠ‚ç‚¹ï¼ˆä¼˜åŒ–é˜ˆå€¼ï¼‰
                tasks = [(d, active_kb_name) for d in node_data]
                # é™ä½Žå¹¶è¡Œé˜ˆå€¼ï¼š2ä¸ªèŠ‚ç‚¹å°±å¹¶è¡Œï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸
                parallel_threshold = 2
                srcs = [s for s in self.executor.execute(process_node_worker, tasks, threshold=parallel_threshold) if s]
                
                if len(node_data) >= parallel_threshold:
                    logger.info(f"âš¡ å¹¶è¡Œå¤„ç†: {len(srcs)} ä¸ªèŠ‚ç‚¹ (é˜ˆå€¼: {parallel_threshold})")
                else:
                    logger.info(f"âš¡ å•èŠ‚ç‚¹å¤„ç†: {len(srcs)} ä¸ªèŠ‚ç‚¹")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_time = time.time() - start_time
            tokens_per_sec = completion_tokens / total_time if total_time > 0 else 0
            
            stats = {
                "time": total_time,
                "tokens": completion_tokens,
                "tokens_per_sec": tokens_per_sec,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens
            }
            
            logger.log("SUCCESS", "å›žç­”ç”Ÿæˆå®Œæˆ", stage="æŸ¥è¯¢å¯¹è¯", 
                      details={"kb_name": active_kb_name, "model": llm_model, 
                              "tokens": completion_tokens, "prompt_tokens": prompt_tokens, 
                              "completion_tokens": completion_tokens})
            logger.complete_operation(f"æŸ¥è¯¢å®Œæˆ (è€—æ—¶ {total_time:.2f}s)")
            
            return {
                "sources": srcs,
                "stats": stats,
                "full_text": full_text
            }
            
        except Exception as e:
            logger.error(f"å“åº”å¤„ç†å¤±è´¥: {e}")
            return {"sources": [], "stats": {}, "full_text": full_text}
    
    def generate_suggestions(self, full_text, existing_questions, chat_engine):
        """ç”Ÿæˆè¿½é—®å»ºè®®"""
        try:
            # å°è¯•ä»Žchat_engineèŽ·å–LLM
            llm_model = None
            if chat_engine and hasattr(chat_engine, '_llm'):
                llm_model = chat_engine._llm
            elif chat_engine and hasattr(chat_engine, 'llm'):
                llm_model = chat_engine.llm
            
            initial_sugs = generate_follow_up_questions(
                full_text,
                num_questions=3,
                existing_questions=existing_questions,
                query_engine=chat_engine if chat_engine else None,
                llm_model=llm_model
            )
            
            if initial_sugs:
                logger.info(f"âœ¨ ç”Ÿæˆ {len(initial_sugs)} ä¸ªæ–°æŽ¨èé—®é¢˜")
                # è¯¦ç»†è®°å½•æ¯ä¸ªæŽ¨èé—®é¢˜
                for i, q in enumerate(initial_sugs[:3], 1):
                    logger.info(f"   {i}. {q}")
                return initial_sugs[:3]
            else:
                logger.info("âš ï¸ æŽ¨èé—®é¢˜ç”Ÿæˆå¤±è´¥")
                return []
                
        except Exception as e:
            logger.error(f"æŽ¨èé—®é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def save_message_and_cleanup(self, active_kb_name, messages):
        """ä¿å­˜æ¶ˆæ¯å¹¶æ¸…ç†å†…å­˜"""
        try:
            if active_kb_name:
                HistoryManager.save(active_kb_name, messages)
            cleanup_memory()
            logger.info("ðŸ§¹ å¯¹è¯å®Œæˆï¼Œå†…å­˜å·²æ¸…ç†")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¶ˆæ¯å¤±è´¥: {e}")
    
    def _extract_node_text(self, node):
        """æå–èŠ‚ç‚¹æ–‡æœ¬"""
        try:
            if hasattr(node, 'get_text'):
                return node.get_text()
            elif hasattr(node, 'text'):
                return node.text
            elif hasattr(node, 'node') and hasattr(node.node, 'text'):
                return node.node.text
            else:
                return str(node)[:150]
        except:
            return str(node)[:150]
    
    def check_duplicate_query(self, query, messages):
        """æ£€æŸ¥é‡å¤æŸ¥è¯¢ - ä½¿ç”¨æ™ºèƒ½ç›¸ä¼¼åº¦æ£€æµ‹"""
        from src.chat_utils_improved import _is_similar_question
        
        # èŽ·å–æœ€è¿‘çš„ç”¨æˆ·é—®é¢˜
        recent_queries = [m['content'] for m in messages[-6:] if m['role'] == 'user']
        
        # ä½¿ç”¨æ™ºèƒ½ç›¸ä¼¼åº¦æ£€æµ‹ï¼Œé™ä½Žé˜ˆå€¼ä»¥æ•èŽ·æ›´å¤šç›¸ä¼¼é—®é¢˜
        for recent_query in recent_queries:
            if _is_similar_question(query, recent_query, threshold=0.6):  # é™ä½Žé˜ˆå€¼
                return True
        
        return False
    
    def prepare_quoted_query(self, query, quote_content):
        """å‡†å¤‡åŒ…å«å¼•ç”¨çš„æŸ¥è¯¢"""
        if quote_content:
            # é™åˆ¶å¼•ç”¨é•¿åº¦
            if len(quote_content) > 2000:
                quote_content = quote_content[:2000] + "...(å·²æˆªæ–­)"
            
            return f"åŸºäºŽä»¥ä¸‹å¼•ç”¨å†…å®¹ï¼š\n> {quote_content}\n\næˆ‘çš„é—®é¢˜æ˜¯ï¼š{query}"
        return query
