"""
çŸ¥è¯†åº“åŠ è½½å™¨æ¨¡å—
è´Ÿè´£çŸ¥è¯†åº“çš„æŒ‚è½½ã€åˆå§‹åŒ–å’Œé…ç½®
"""

import os
import time
import json
import glob
import threading
import streamlit as st
from llama_index.core import StorageContext, load_index_from_storage
from llama_index.core.memory import ChatMemoryBuffer

from src.logging import LogManager
from src.config import ManifestManager
from src.utils.model_manager import load_embedding_model

logger = LogManager()


class KnowledgeBaseLoader:
    """çŸ¥è¯†åº“åŠ è½½å™¨"""
    
    def __init__(self, output_base):
        self.output_base = output_base
    
    def get_kb_embedding_dim(self, db_path):
        """æ£€æµ‹çŸ¥è¯†åº“çš„å‘é‡ç»´åº¦"""
        try:
            # å°è¯•è¯»å– .kb_info.json
            kb_info_file = os.path.join(db_path, ".kb_info.json")
            if os.path.exists(kb_info_file):
                with open(kb_info_file, 'r') as f:
                    kb_info = json.load(f)
                    model = kb_info.get('embedding_model', '')
                    # æ ¹æ®æ¨¡å‹åæ¨æ–­ç»´åº¦
                    if 'small' in model:
                        return 512
                    elif 'base' in model:
                        return 768
                    elif 'm3' in model:
                        return 1024
            
            # å°è¯•ä»å‘é‡æ–‡ä»¶æ¨æ–­
            vector_files = glob.glob(os.path.join(db_path, "**/*.json"), recursive=True)
            if vector_files:
                # ç®€å•å¯å‘å¼ï¼šæ ¹æ®æ–‡ä»¶å¤§å°æ¨æ–­
                total_size = sum(os.path.getsize(f) for f in vector_files) / (1024 * 1024)
                if total_size < 50:
                    return 512  # å°æ¨¡å‹
                elif total_size < 200:
                    return 768  # ä¸­æ¨¡å‹
                else:
                    return 1024  # å¤§æ¨¡å‹
        except:
            pass
        return None
    
    def load_knowledge_base(self, kb_name, embed_provider, embed_model, embed_key, embed_url):
        """åŠ è½½çŸ¥è¯†åº“"""
        db_path = os.path.join(self.output_base, kb_name)
        
        if not os.path.exists(db_path):
            return None, "çŸ¥è¯†åº“ä¸å­˜åœ¨"
        
        try:
            logger.log("INFO", f"å¼€å§‹åŠ è½½çŸ¥è¯†åº“: {kb_name}", stage="çŸ¥è¯†åº“åŠ è½½")
            
            # æ£€æµ‹çŸ¥è¯†åº“çš„å‘é‡ç»´åº¦
            kb_dim = self.get_kb_embedding_dim(db_path)
            if kb_dim:
                model_map = {
                    512: "BAAI/bge-small-zh-v1.5",
                    768: "BAAI/bge-base-zh-v1.5", 
                    1024: "BAAI/bge-m3"
                }
                
                if kb_dim in model_map:
                    required_model = model_map[kb_dim]
                    if embed_model != required_model:
                        logger.warning(f"âš ï¸ çŸ¥è¯†åº“ç»´åº¦: {kb_dim}Dï¼Œè‡ªåŠ¨åˆ‡æ¢æ¨¡å‹: {required_model}")
                        embed_model = required_model
                        embed = load_embedding_model(embed_provider, embed_model, embed_key, embed_url)
                        if embed:
                            from llama_index.core import Settings
                            Settings.embed_model = embed
            
            # æ£€æŸ¥çŸ¥è¯†åº“å¤§å°
            vector_files = glob.glob(os.path.join(db_path, "**/*.json"), recursive=True)
            total_size = sum(os.path.getsize(f) for f in vector_files) / (1024 * 1024)
            is_large_kb = len(vector_files) > 100 or total_size > 100
            
            if is_large_kb:
                return self._load_large_kb(db_path, kb_name, vector_files, total_size)
            else:
                return self._load_small_kb(db_path, kb_name, embed_provider, embed_model, embed_key, embed_url)
                
        except Exception as e:
            logger.log("ERROR", f"çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {kb_name} - {str(e)}", stage="çŸ¥è¯†åº“åŠ è½½")
            return None, f"çŸ¥è¯†åº“æŒ‚è½½å¤±è´¥ï¼š{e}"
    
    def _load_large_kb(self, db_path, kb_name, vector_files, total_size):
        """åŠ è½½å¤§å‹çŸ¥è¯†åº“"""
        load_start = time.time()
        logger.info(f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡: {len(vector_files)} ä¸ªæ–‡ä»¶, {total_size:.1f}MB")
        
        progress_placeholder = st.empty()
        progress_bar = progress_placeholder.progress(0, text="â³ å‡†å¤‡åŠ è½½çŸ¥è¯†åº“... 0%")
        
        with st.status(f"ğŸ“š æ­£åœ¨æŒ‚è½½å¤§å‹çŸ¥è¯†åº“: {kb_name}ï¼ˆ{len(vector_files)} ä¸ªæ–‡ä»¶, {total_size:.1f}MBï¼‰", expanded=True) as status:
            # é˜¶æ®µ1: åŠ è½½å‘é‡æ•°æ®
            status.write("â³ [1/3] æ­£åœ¨åŠ è½½å‘é‡æ•°æ®...")
            logger.processing("[1/3] å¼€å§‹åŠ è½½å‘é‡æ•°æ®...")
            
            stage1_start = time.time()
            storage_context = self._load_with_progress(
                lambda: StorageContext.from_defaults(persist_dir=db_path),
                progress_bar, 5, 39, "[1/3] åŠ è½½å‘é‡æ•°æ®"
            )
            stage1_time = time.time() - stage1_start
            
            progress_bar.progress(40, text=f"âœ… [1/3] å‘é‡æ•°æ®åŠ è½½å®Œæˆ ({stage1_time:.1f}s) - 40%")
            status.write(f"âœ… [1/3] å‘é‡æ•°æ®åŠ è½½å®Œæˆ (è€—æ—¶ {stage1_time:.1f}s)")
            
            # é˜¶æ®µ2: æ„å»ºç´¢å¼•
            status.write("â³ [2/3] æ­£åœ¨æ„å»ºç´¢å¼•...")
            logger.processing("[2/3] å¼€å§‹æ„å»ºç´¢å¼•...")
            
            stage2_start = time.time()
            index = self._load_with_progress(
                lambda: load_index_from_storage(storage_context),
                progress_bar, 45, 79, "[2/3] æ„å»ºç´¢å¼•"
            )
            stage2_time = time.time() - stage2_start
            
            progress_bar.progress(80, text=f"âœ… [2/3] ç´¢å¼•æ„å»ºå®Œæˆ ({stage2_time:.1f}s) - 80%")
            status.write(f"âœ… [2/3] ç´¢å¼•æ„å»ºå®Œæˆ (è€—æ—¶ {stage2_time:.1f}s)")
            
            # é˜¶æ®µ3: åˆå§‹åŒ–é—®ç­”å¼•æ“
            status.write("â³ [3/3] æ­£åœ¨åˆå§‹åŒ–é—®ç­”å¼•æ“...")
            logger.processing("[3/3] åˆå§‹åŒ–é—®ç­”å¼•æ“...")
            
            stage3_start = time.time()
            chat_engine = self._create_chat_engine(index, db_path, status)
            stage3_time = time.time() - stage3_start
            load_time = time.time() - load_start
            
            progress_bar.progress(100, text=f"âœ… å…¨éƒ¨å®Œæˆï¼æ€»è€—æ—¶: {load_time:.1f}s - 100%")
            status.write(f"âœ… [3/3] é—®ç­”å¼•æ“åˆå§‹åŒ–å®Œæˆ (è€—æ—¶ {stage3_time:.1f}s)")
            status.update(label=f"âœ… çŸ¥è¯†åº“ '{kb_name}' æŒ‚è½½æˆåŠŸï¼æ€»è€—æ—¶: {load_time:.1f}s", state="complete")
            
            # æ¸…ç†è¿›åº¦æ¡
            time.sleep(1.5)
            progress_placeholder.empty()
            
            return chat_engine, None
    
    def _load_small_kb(self, db_path, kb_name, embed_provider, embed_model, embed_key, embed_url):
        """åŠ è½½å°å‹çŸ¥è¯†åº“"""
        with st.spinner(f"ğŸ“š æ­£åœ¨æŒ‚è½½çŸ¥è¯†åº“: {kb_name}..."):
            try:
                # è¯»å–çŸ¥è¯†åº“ä¿¡æ¯
                kb_info_file = os.path.join(db_path, ".kb_info.json")
                if os.path.exists(kb_info_file):
                    with open(kb_info_file, 'r') as f:
                        kb_info = json.load(f)
                        kb_embed_model = kb_info.get('embedding_model', 'BAAI/bge-large-zh-v1.5')
                else:
                    kb_manifest = ManifestManager.load(db_path)
                    kb_embed_model = kb_manifest.get('embed_model', 'BAAI/bge-large-zh-v1.5')
                
                # ä½¿ç”¨çŸ¥è¯†åº“çš„æ¨¡å‹åŠ è½½
                embed = load_embedding_model(embed_provider, kb_embed_model, embed_key, embed_url)
                if embed:
                    from llama_index.core import Settings
                    Settings.embed_model = embed
                else:
                    raise ValueError(f"æ— æ³•åŠ è½½åµŒå…¥æ¨¡å‹: {kb_embed_model}")
                
                storage_context = StorageContext.from_defaults(persist_dir=db_path)
                index = load_index_from_storage(storage_context)
                
                chat_engine = index.as_chat_engine(
                    chat_mode="context",
                    memory=ChatMemoryBuffer.from_defaults(token_limit=2000),
                    similarity_top_k=3,
                    streaming=True,
                    timeout=25.0,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œè¯·åŠ¡å¿…ä»…åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚å›ç­”åº”æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šã€‚"
                )
                
                return chat_engine, None
                
            except Exception as e:
                if "shapes" in str(e) and "not aligned" in str(e):
                    return None, self._handle_dimension_mismatch(embed_model, str(e))
                else:
                    raise
    
    def _load_with_progress(self, load_func, progress_bar, start_progress, end_progress, stage_name):
        """å¸¦è¿›åº¦æ˜¾ç¤ºçš„åŠ è½½"""
        result = [None]
        
        def load_task():
            result[0] = load_func()
        
        thread = threading.Thread(target=load_task)
        thread.start()
        
        progress = start_progress
        stage_start = time.time()
        while thread.is_alive():
            progress = min(progress + 1, end_progress)
            elapsed = time.time() - stage_start
            progress_bar.progress(progress, text=f"â³ {stage_name}... {progress}% (å·²ç”¨æ—¶ {elapsed:.0f}s)")
            time.sleep(0.5)
        
        thread.join()
        return result[0]
    
    def _create_chat_engine(self, index, db_path, status):
        """åˆ›å»ºèŠå¤©å¼•æ“"""
        node_postprocessors = []
        similarity_top_k = 5
        retriever = None
        
        # BM25 æ··åˆæ£€ç´¢é…ç½®
        if st.session_state.get('enable_bm25', False):
            try:
                from llama_index.retrievers.bm25 import BM25Retriever
                from llama_index.core.retrievers import QueryFusionRetriever
                
                status.write("   ğŸ” æ„å»º BM25 æ··åˆæ£€ç´¢...")
                nodes = index.docstore.docs.values()
                
                bm25_retriever = BM25Retriever.from_defaults(
                    nodes=list(nodes),
                    similarity_top_k=5
                )
                
                vector_retriever = index.as_retriever(similarity_top_k=5)
                
                retriever = QueryFusionRetriever(
                    retrievers=[vector_retriever, bm25_retriever],
                    similarity_top_k=5,
                    num_queries=1,
                    mode="reciprocal_rerank",
                    use_async=False,
                )
                
                status.write("   âœ… BM25 æ··åˆæ£€ç´¢æ„å»ºæˆåŠŸ")
            except Exception as e:
                status.write(f"   âš ï¸ BM25 æ„å»ºå¤±è´¥: {e}")
        
        # Re-ranking é…ç½®
        if st.session_state.get('enable_rerank', False):
            try:
                from llama_index.core.postprocessor import SentenceTransformerRerank
                
                rerank_model = st.session_state.get('rerank_model', 'BAAI/bge-reranker-base')
                status.write(f"   ğŸ¯ åŠ è½½ Re-ranking æ¨¡å‹: {rerank_model}...")
                
                reranker = SentenceTransformerRerank(
                    top_n=3,
                    model=rerank_model,
                    keep_retrieval_score=True,
                )
                node_postprocessors.append(reranker)
                similarity_top_k = 10
                
                status.write("   âœ… Re-ranking æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                status.write(f"   âš ï¸ Re-ranking åŠ è½½å¤±è´¥: {e}")
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        if retriever:
            return index.as_chat_engine(
                chat_mode="context",
                retriever=retriever,
                memory=ChatMemoryBuffer.from_defaults(token_limit=2000),
                similarity_top_k=3,
                streaming=True,
                timeout=25.0,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œè¯·åŠ¡å¿…ä»…åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚å›ç­”åº”æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šã€‚",
                node_postprocessors=node_postprocessors if node_postprocessors else None
            )
        else:
            return index.as_chat_engine(
                chat_mode="context",
                memory=ChatMemoryBuffer.from_defaults(token_limit=2000),
                similarity_top_k=3,
                streaming=True,
                timeout=25.0,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œè¯·åŠ¡å¿…ä»…åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚å›ç­”åº”æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šã€‚",
                node_postprocessors=node_postprocessors if node_postprocessors else None
            )
    
    def _handle_dimension_mismatch(self, embed_model, error_msg):
        """å¤„ç†ç»´åº¦ä¸åŒ¹é…é”™è¯¯"""
        logger.warning("âš ï¸ å‘é‡ç»´åº¦ä¸åŒ¹é…")
        logger.info(f"å½“å‰æ¨¡å‹: {embed_model}")
        logger.info(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
        
        st.error("âŒ å‘é‡ç»´åº¦ä¸åŒ¹é…")
        st.warning(f"""
**å½“å‰æ¨¡å‹:** {embed_model}

**åŸå› :** çŸ¥è¯†åº“æ˜¯ç”¨å…¶ä»–ç»´åº¦çš„æ¨¡å‹åˆ›å»ºçš„ï¼Œæ— æ³•ç›´æ¥æŸ¥è¯¢ã€‚

**è§£å†³æ–¹æ¡ˆ:**
1. **ä¿ç•™æ—§æ•°æ®** - åˆ‡æ¢å›åŸæ¨¡å‹ï¼ˆbge-small-zh-v1.5ï¼‰
2. **é‡å»ºç´¢å¼•** - ç”¨æ–°æ¨¡å‹é‡æ–°åµŒå…¥æ‰€æœ‰æ–‡æ¡£ï¼ˆè€—æ—¶è¾ƒé•¿ï¼‰
""")
        
        return "ç»´åº¦ä¸åŒ¹é…é”™è¯¯"
