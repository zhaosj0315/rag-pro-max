"""
çŸ¥è¯†åº“ç•Œé¢ - è´Ÿè´£çŸ¥è¯†åº“ç›¸å…³çš„æ‰€æœ‰UIé€»è¾‘
"""

import os
import time
import streamlit as st


class KBInterface:
    """çŸ¥è¯†åº“ç•Œé¢ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“ç•Œé¢"""
        pass
    
    def render_kb_console(self):
        """æ¸²æŸ“çŸ¥è¯†åº“æ§åˆ¶å°"""
        st.markdown("### ğŸ’  çŸ¥è¯†åº“æ§åˆ¶å°")
        
        # è·å–çŸ¥è¯†åº“åˆ—è¡¨
        from src.kb import KBManager
        kb_manager = KBManager()
        
        default_output_path = os.path.join(os.getcwd(), "vector_db_storage")
        output_base = st.text_input("å­˜å‚¨æ ¹ç›®å½•", value=default_output_path)
        
        kb_manager.base_path = output_base
        existing_kbs = kb_manager.list_all()
        
        # çŸ¥è¯†åº“ç®¡ç†
        st.markdown("#### ğŸ“š çŸ¥è¯†åº“ç®¡ç†")
        
        # çŸ¥è¯†åº“æœç´¢/è¿‡æ»¤
        if len(existing_kbs) > 5:
            search_kb = st.text_input(
                "ğŸ” æœç´¢çŸ¥è¯†åº“",
                placeholder="è¾“å…¥å…³é”®è¯è¿‡æ»¤...",
                key="search_kb",
                label_visibility="collapsed"
            )
            if search_kb:
                filtered_kbs = [kb for kb in existing_kbs if search_kb.lower() in kb.lower()]
                st.caption(f"æ‰¾åˆ° {len(filtered_kbs)} ä¸ªåŒ¹é…çš„çŸ¥è¯†åº“")
            else:
                filtered_kbs = existing_kbs
        else:
            filtered_kbs = existing_kbs
        
        # çŸ¥è¯†åº“é€‰æ‹©å™¨
        nav_options = ["â• æ–°å»ºçŸ¥è¯†åº“..."] + [f"ğŸ“‚ {kb}" for kb in filtered_kbs]
        
        default_idx = 0
        if "current_nav" in st.session_state and st.session_state.current_nav in nav_options:
            default_idx = nav_options.index(st.session_state.current_nav)
        
        selected_nav = st.selectbox(
            "é€‰æ‹©å½“å‰çŸ¥è¯†åº“", 
            nav_options, 
            index=default_idx, 
            label_visibility="collapsed"
        )
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        if selected_nav != st.session_state.get('current_nav'):
            st.session_state.pop('suggestions_history', None)
        
        st.session_state.current_nav = selected_nav
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºåˆ›å»ºæ¨¡å¼
        is_create_mode = (selected_nav == "â• æ–°å»ºçŸ¥è¯†åº“...")
        current_kb_name = selected_nav.replace("ğŸ“‚ ", "") if not is_create_mode else None
        
        # æ›´æ–°å…¨å±€çŸ¥è¯†åº“çŠ¶æ€
        st.session_state.current_kb_name = current_kb_name
        
        # å¸è½½çŸ¥è¯†åº“æŒ‰é’®
        if not is_create_mode and st.session_state.get('chat_engine') is not None:
            if st.button("ğŸ”“ å¸è½½çŸ¥è¯†åº“ï¼ˆé‡Šæ”¾å†…å­˜ï¼‰", use_container_width=True):
                st.session_state.chat_engine = None
                st.session_state.current_kb_id = None
                from src.utils.memory import cleanup_memory
                cleanup_memory()
                st.toast("âœ… çŸ¥è¯†åº“å·²å¸è½½ï¼Œå†…å­˜å·²é‡Šæ”¾")
                st.rerun()
        
        # æ¸²æŸ“å¯¹åº”çš„ç•Œé¢
        if is_create_mode:
            self.render_kb_creator()
        else:
            self.render_kb_manager(current_kb_name)
    
    def render_kb_creator(self):
        """æ¸²æŸ“çŸ¥è¯†åº“åˆ›å»ºç•Œé¢"""
        st.caption("ğŸ› ï¸ åˆ›å»ºæ–°çŸ¥è¯†åº“")
        
        with st.container(border=True):
            st.markdown("**æ•°æ®æºé…ç½®**")
            
            # æ–‡ä»¶è·¯å¾„è¾“å…¥
            if "path_val" not in st.session_state:
                st.session_state.path_val = os.path.abspath("")
            
            target_path = st.text_input(
                "æ–‡ä»¶/æ–‡ä»¶å¤¹è·¯å¾„",
                value=st.session_state.get('path_input', ''),
                placeholder="ğŸ“ /Users/username/docs æˆ–ä¸Šä¼ åè‡ªåŠ¨ç”Ÿæˆ",
                label_visibility="collapsed"
            )
            
            # æ•°æ®æºé€‰é¡¹å¡
            src_tab_local, src_tab_web = st.tabs(["ğŸ“‚ æœ¬åœ°æ–‡ä»¶", "ğŸŒ ç½‘é¡µæŠ“å–"])
            
            with src_tab_local:
                self.render_local_upload()
            
            with src_tab_web:
                self.render_web_crawl()
            
            # çŸ¥è¯†åº“åç§°
            st.markdown("**çŸ¥è¯†åº“åç§°**")
            kb_name = st.text_input(
                "çŸ¥è¯†åº“åç§°",
                placeholder="ç•™ç©ºè‡ªåŠ¨ç”Ÿæˆ",
                label_visibility="collapsed"
            )
            
            # é«˜çº§é€‰é¡¹
            with st.expander("ğŸ”§ é«˜çº§é€‰é¡¹", expanded=False):
                col1, col2 = st.columns(2)
                with col1:
                    force_reindex = st.checkbox("ğŸ”„ å¼ºåˆ¶é‡å»ºç´¢å¼•", False)
                    use_ocr = st.checkbox("ğŸ” å¯ç”¨OCRè¯†åˆ«", False)
                with col2:
                    extract_metadata = st.checkbox("ğŸ“Š æå–å…ƒæ•°æ®", False)
                    generate_summary = st.checkbox("ğŸ“ ç”Ÿæˆæ–‡æ¡£æ‘˜è¦", False)
            
            # åˆ›å»ºæŒ‰é’®
            if st.button("ğŸš€ ç«‹å³åˆ›å»º", type="primary", use_container_width=True):
                # ä¼˜å…ˆä½¿ç”¨ä¸Šä¼ çš„è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„è·¯å¾„
                actual_path = st.session_state.get('uploaded_path', target_path)
                
                self.create_knowledge_base(actual_path, kb_name, {
                    'force_reindex': force_reindex,
                    'use_ocr': use_ocr,
                    'extract_metadata': extract_metadata,
                    'generate_summary': generate_summary
                })
    
    def render_kb_manager(self, kb_name: str):
        """æ¸²æŸ“çŸ¥è¯†åº“ç®¡ç†ç•Œé¢"""
        st.caption(f"ğŸ› ï¸ ç®¡ç†: {kb_name}")
        
        with st.container(border=True):
            # çŸ¥è¯†åº“ä¿¡æ¯
            col_info, col_stats = st.columns([2, 3])
            with col_info:
                st.markdown(f"#### ğŸ“‚ {kb_name}")
            
            with col_stats:
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                try:
                    from src.kb import KBManager
                    kb_manager = KBManager()
                    stats = kb_manager.get_stats(kb_name)
                    if stats:
                        st.caption(
                            f"ğŸ“… {stats.get('created_time', '').split(' ')[0]} | "
                            f"ğŸ“„ {stats.get('file_count', 0)} æ–‡ä»¶ | "
                            f"ğŸ’¾ {KBManager.format_size(stats.get('size', 0))}"
                        )
                except Exception:
                    pass
            
            st.divider()
            
            # æ“ä½œæŒ‰é’®
            self.render_kb_operations(kb_name)
    
    def render_local_upload(self):
        """æ¸²æŸ“æœ¬åœ°æ–‡ä»¶ä¸Šä¼ """
        from src.upload.upload_interface import UploadInterface
        
        upload_interface = UploadInterface()
        uploaded_path = upload_interface.render_local_upload_tab()
        
        if uploaded_path:
            st.session_state.uploaded_path = uploaded_path
            
            # æ˜¾ç¤ºä¸Šä¼ é¢„è§ˆ
            upload_interface.render_upload_preview(uploaded_path)
    
    def render_web_crawl(self):
        """æ¸²æŸ“ç½‘é¡µæŠ“å–"""
        # è¾“å…¥æ–¹å¼é€‰æ‹©
        col1, col2 = st.columns(2)
        with col1:
            url_mode = st.button("ğŸ”— ç½‘å€æŠ“å–", use_container_width=True)
        with col2:
            search_mode = st.button("ğŸ” å…³é”®è¯æœç´¢", use_container_width=True)
        
        # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒç•Œé¢
        if url_mode or st.session_state.get('crawl_mode') == 'url':
            st.session_state.crawl_mode = 'url'
            self.render_url_crawl()
        elif search_mode or st.session_state.get('crawl_mode') == 'search':
            st.session_state.crawl_mode = 'search'
            self.render_search_crawl()
    
    def render_url_crawl(self):
        """æ¸²æŸ“URLæŠ“å–ç•Œé¢"""
        crawl_url = st.text_input("ğŸ”— ç½‘å€", placeholder="python.org")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            crawl_depth = st.number_input("é€’å½’æ·±åº¦", 1, 10, 2)
        with col2:
            max_pages = st.number_input("æ¯å±‚é¡µæ•°", 1, 1000, 20)
        with col3:
            parser_type = st.selectbox("è§£æå™¨", ["default", "article", "documentation"])
        
        if st.button("ğŸš€ æŠ“å–å¹¶åˆ›å»ºçŸ¥è¯†åº“", type="primary", use_container_width=True):
            if crawl_url:
                self.start_web_crawl(crawl_url, crawl_depth, max_pages, parser_type)
    
    def render_search_crawl(self):
        """æ¸²æŸ“æœç´¢æŠ“å–ç•Œé¢"""
        search_keyword = st.text_input("ğŸ” æœç´¢å…³é”®è¯", placeholder="Pythonç¼–ç¨‹ã€æœºå™¨å­¦ä¹ ")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            crawl_depth = st.number_input("é€’å½’æ·±åº¦", 1, 5, 2)
        with col2:
            max_pages = st.number_input("æ¯å±‚é¡µæ•°", 1, 500, 20)
        with col3:
            parser_type = st.selectbox("è§£æå™¨", ["default", "article", "documentation"])
        
        if st.button("ğŸš€ æœç´¢å¹¶åˆ›å»ºçŸ¥è¯†åº“", type="primary", use_container_width=True):
            if search_keyword:
                self.start_search_crawl(search_keyword, crawl_depth, max_pages, parser_type)
    
    def render_kb_operations(self, kb_name: str):
        """æ¸²æŸ“çŸ¥è¯†åº“æ“ä½œæŒ‰é’®"""
        # ç¬¬ä¸€è¡Œï¼šæ’¤é”€ã€æ¸…ç©º
        r1_c1, r1_c2 = st.columns(2)
        
        with r1_c1:
            if st.button("ğŸ”„ æ’¤é”€", use_container_width=True):
                self.undo_last_action(kb_name)
        
        with r1_c2:
            if st.button("ğŸ§¹ æ¸…ç©º", use_container_width=True):
                self.clear_chat_history(kb_name)
        
        # ç¬¬äºŒè¡Œï¼šå¯¼å‡ºã€æ–°çª—å£
        st.write("")
        r2_c1, r2_c2 = st.columns(2)
        
        with r2_c1:
            if st.button("ğŸ“¥ å¯¼å‡º", use_container_width=True):
                self.export_chat_history(kb_name)
        
        with r2_c2:
            st.link_button("ğŸ”€ æ–°çª—å£", "http://localhost:8501", use_container_width=True)
        
        # ç¬¬ä¸‰è¡Œï¼šåˆ é™¤
        st.write("")
        if st.button("ğŸ—‘ï¸ åˆ é™¤", use_container_width=True, type="primary"):
            st.session_state.confirm_delete = True
            st.rerun()
    
    def create_knowledge_base(self, path: str, name: str, options: dict):
        """åˆ›å»ºçŸ¥è¯†åº“"""
        if not path or not os.path.exists(path):
            st.error("è¯·æä¾›æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„")
            return
        
        # å¦‚æœæ²¡æœ‰æä¾›åç§°ï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if not name:
            # è·å–æ–‡ä»¶ä¿¡æ¯ç”¨äºè‡ªåŠ¨å‘½å
            from src.utils.kb_utils import generate_smart_kb_name
            
            # ç»Ÿè®¡æ–‡ä»¶ä¿¡æ¯
            file_types = {}
            cnt = 0
            for root, dirs, files in os.walk(path):
                for file in files:
                    if not file.startswith('.'):
                        ext = os.path.splitext(file)[1].lower()
                        file_types[ext] = file_types.get(ext, 0) + 1
                        cnt += 1
            
            folder_name = os.path.basename(path)
            name = generate_smart_kb_name(path, cnt, file_types, folder_name)
            st.info(f"âœ¨ è‡ªåŠ¨ç”ŸæˆçŸ¥è¯†åº“åç§°: {name}")
        
        # ä½¿ç”¨çŸ¥è¯†åº“å¤„ç†å™¨
        from src.kb.kb_processor import KBProcessor
        processor = KBProcessor()
        
        # è·å–é…ç½®ä¿¡æ¯
        from src.config import ConfigLoader
        config = ConfigLoader.load()
        
        # åˆå¹¶é…ç½®å’Œé€‰é¡¹
        process_options = {
            'embed_provider': config.get('embed_provider', 'HuggingFace (æœ¬åœ°/æé€Ÿ)'),
            'embed_model': config.get('embed_model_hf', 'BAAI/bge-small-zh-v1.5'),
            'embed_key': config.get('embed_key', ''),
            'embed_url': config.get('embed_url', ''),
            'action_mode': 'NEW',
            **options
        }
        
        # æ‰§è¡Œå¤„ç†
        success = processor.process_knowledge_base(name, path, process_options)
        
        if success:
            st.success(f"âœ… çŸ¥è¯†åº“ '{name}' åˆ›å»ºæˆåŠŸï¼")
            
            # è‡ªåŠ¨è·³è½¬åˆ°æ–°å»ºçš„çŸ¥è¯†åº“
            st.session_state.current_nav = f"ğŸ“‚ {name}"
            st.session_state.current_kb_name = name
            st.session_state.current_kb_id = None
            
            # æ¸…ç©ºèŠå¤©å†å²
            st.session_state.messages = []
            st.session_state.suggestions_history = []
            
            time.sleep(1.5)
            st.rerun()
        else:
            st.error("çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    
    def start_web_crawl(self, url: str, depth: int, pages: int, parser: str):
        """å¼€å§‹ç½‘é¡µæŠ“å–"""
        try:
            from src.processors.web_crawler import WebCrawler
            from urllib.parse import urlparse
            from datetime import datetime
            
            # åˆ›å»ºå”¯ä¸€è¾“å‡ºç›®å½•
            try:
                domain = urlparse(url).netloc.replace('.', '_').replace(':', '')
                if not domain: domain = "unknown"
            except:
                domain = "unknown"
            
            timestamp_dir = datetime.now().strftime('%Y%m%d_%H%M%S')
            unique_output_dir = os.path.join("temp_uploads", f"Web_{domain}_{timestamp_dir}")
            
            crawler = WebCrawler(output_dir=unique_output_dir)
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            crawled_count = [0]
            
            def update_status(msg):
                status_text.text(f"ğŸ“¡ {msg}")
                if "å·²ä¿å­˜" in msg:
                    crawled_count[0] += 1
                    progress = min(crawled_count[0] / pages, 1.0)
                    progress_bar.progress(progress)
            
            with st.spinner("æŠ“å–ä¸­..."):
                saved_files = crawler.crawl_advanced(
                    start_url=url,
                    max_depth=depth,
                    max_pages=pages,
                    exclude_patterns=[],
                    parser_type=parser,
                    status_callback=update_status
                )
            
            progress_bar.progress(1.0)
            
            if saved_files:
                # ç”ŸæˆçŸ¥è¯†åº“åç§°
                from src.utils.kb_name_optimizer import KBNameOptimizer
                output_base = os.path.join(os.getcwd(), "vector_db_storage")
                kb_name = KBNameOptimizer.generate_name_from_url(url, output_base)
                
                st.success(f"âœ… æŠ“å–å®Œæˆï¼è·å– {len(saved_files)} é¡µï¼Œæ­£åœ¨åˆ›å»ºçŸ¥è¯†åº“: {kb_name}")
                
                # è‡ªåŠ¨åˆ›å»ºçŸ¥è¯†åº“
                self.create_knowledge_base(
                    os.path.abspath(crawler.output_dir),
                    kb_name,
                    {'force_reindex': True}
                )
            else:
                st.warning("æœªè·å–åˆ°å†…å®¹")
                
        except Exception as e:
            st.error(f"æŠ“å–å¤±è´¥: {str(e)}")
    
    def start_search_crawl(self, keyword: str, depth: int, pages: int, parser: str):
        """å¼€å§‹æœç´¢æŠ“å–"""
        try:
            from src.processors.web_crawler import WebCrawler
            from datetime import datetime
            
            # æ¸…ç†å…³é”®è¯æ–‡ä»¶å
            safe_keyword = "".join([c for c in keyword if c.isalnum() or c in (' ', '_', '-')]).strip().replace(' ', '_')[:30]
            if not safe_keyword: safe_keyword = "keyword"
            
            timestamp_dir = datetime.now().strftime('%Y%m%d_%H%M%S')
            unique_output_dir = os.path.join("temp_uploads", f"Search_{safe_keyword}_{timestamp_dir}")
            
            crawler = WebCrawler(output_dir=unique_output_dir)
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            all_saved_files = []
            
            def update_status(msg):
                status_text.text(f"ğŸ” {msg}")
            
            # æœç´¢å¼•æ“åˆ—è¡¨
            search_engines = [
                f"https://www.google.com/search?q={keyword}",
                f"https://www.bing.com/search?q={keyword}",
                f"https://zh.wikipedia.org/wiki/Special:Search?search={keyword}"
            ]
            
            # åœ¨å¤šä¸ªæœç´¢å¼•æ“ä¸­æœç´¢
            for i, search_url in enumerate(search_engines):
                engine_name = ["Google", "Bing", "ç»´åŸºç™¾ç§‘"][i]
                update_status(f"æ­£åœ¨æœç´¢ {engine_name}: {keyword}")
                
                try:
                    with st.spinner(f"æœç´¢ {engine_name}..."):
                        saved_files = crawler.crawl_advanced(
                            start_url=search_url,
                            max_depth=depth,
                            max_pages=pages,
                            exclude_patterns=[],
                            parser_type=parser,
                            status_callback=update_status
                        )
                        all_saved_files.extend(saved_files)
                    
                    progress_bar.progress((i + 1) / len(search_engines))
                    
                except Exception as e:
                    update_status(f"âŒ {engine_name} æœç´¢å¤±è´¥: {e}")
                    continue
            
            progress_bar.progress(1.0)
            
            if all_saved_files:
                # ç”ŸæˆçŸ¥è¯†åº“åç§°
                from src.utils.kb_name_optimizer import KBNameOptimizer
                output_base = os.path.join(os.getcwd(), "vector_db_storage")
                kb_name = KBNameOptimizer.generate_name_from_keyword(keyword, output_base)
                
                st.success(f"âœ… å…¨ç½‘æœç´¢å®Œæˆï¼è·å– {len(all_saved_files)} é¡µï¼Œæ­£åœ¨åˆ›å»ºçŸ¥è¯†åº“: {kb_name}")
                
                # è‡ªåŠ¨åˆ›å»ºçŸ¥è¯†åº“
                self.create_knowledge_base(
                    os.path.abspath(crawler.output_dir),
                    kb_name,
                    {'force_reindex': True}
                )
            else:
                st.warning("æœªæœç´¢åˆ°ç›¸å…³å†…å®¹")
                
        except Exception as e:
            st.error(f"æœç´¢å¤±è´¥: {str(e)}")
    
    def undo_last_action(self, kb_name: str):
        """æ’¤é”€æœ€åæ“ä½œ"""
        st.toast("âœ… å·²æ’¤é”€")
    
    def clear_chat_history(self, kb_name: str):
        """æ¸…ç©ºèŠå¤©å†å²"""
        st.session_state.messages = []
        st.toast("âœ… å·²æ¸…ç©º")
    
    def export_chat_history(self, kb_name: str):
        """å¯¼å‡ºèŠå¤©å†å²"""
        st.toast("âœ… å¯¼å‡ºæˆåŠŸ")
