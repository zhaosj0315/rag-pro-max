"""
RAG Pro Max - RAG æ ¸å¿ƒå¼•æ“
æå–è‡ª apppro.pyï¼Œè´Ÿè´£çŸ¥è¯†åº“çš„åˆ›å»ºã€åŠ è½½å’ŒæŸ¥è¯¢
"""

import os
import time
import shutil
from typing import List, Dict, Optional, Tuple
from llama_index.core import (
    VectorStoreIndex, 
    StorageContext, 
    load_index_from_storage,
    Settings,
    PromptTemplate
)
from llama_index.core.schema import Document


class RAGEngine:
    """RAG æ ¸å¿ƒå¼•æ“"""
    
    def __init__(
        self, 
        kb_name: str,
        persist_dir: str,
        embed_model,
        llm_model,
        logger=None
    ):
        """
        åˆå§‹åŒ– RAG å¼•æ“
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            persist_dir: æŒä¹…åŒ–ç›®å½•
            embed_model: åµŒå…¥æ¨¡å‹
            llm_model: LLM æ¨¡å‹
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.kb_name = kb_name
        self.persist_dir = persist_dir
        self.embed_model = embed_model
        self.llm_model = llm_model
        self.logger = logger
        self.index = None
        
        # è®¾ç½®å…¨å±€æ¨¡å‹
        if embed_model:
            Settings.embed_model = embed_model
        if llm_model:
            Settings.llm = llm_model
    
    def load_existing_index(self) -> bool:
        """
        åŠ è½½å·²æœ‰ç´¢å¼•
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if not os.path.exists(self.persist_dir):
            return False
        
        try:
            if self.logger:
                self.logger.info(f"ğŸ“‚ åŠ è½½ç°æœ‰ç´¢å¼•: {self.kb_name}")
            
            storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
            self.index = load_index_from_storage(storage_context)
            
            if self.logger:
                self.logger.success("âœ… ç´¢å¼•åŠ è½½æˆåŠŸ")
            
            return True
            
        except Exception as e:
            error_msg = str(e)
            if self.logger:
                if "shapes" in error_msg and "not aligned" in error_msg:
                    self.logger.warning("âš ï¸  å‘é‡ç»´åº¦ä¸åŒ¹é…ï¼Œéœ€è¦é‡å»ºç´¢å¼•")
                else:
                    self.logger.warning(f"âš ï¸  ç´¢å¼•åŠ è½½å¤±è´¥: {error_msg}")
            
            # æ¸…ç†æŸåçš„ç´¢å¼•
            shutil.rmtree(self.persist_dir, ignore_errors=True)
            self.index = None
            return False
    
    def create_index(
        self, 
        documents: List[Document],
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        show_progress: bool = True
    ) -> VectorStoreIndex:
        """
        åˆ›å»ºå‘é‡ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å 
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            VectorStoreIndex: åˆ›å»ºçš„ç´¢å¼•
        """
        if self.logger:
            self.logger.info(f"ğŸ”¨ åˆ›å»ºå‘é‡ç´¢å¼•: {len(documents)} ä¸ªæ–‡æ¡£")
        
        start_time = time.time()
        
        # åˆ›å»ºç´¢å¼•
        if self.index:
            # è¿½åŠ åˆ°ç°æœ‰ç´¢å¼•
            if self.logger:
                self.logger.info("ğŸ“ è¿½åŠ æ–‡æ¡£åˆ°ç°æœ‰ç´¢å¼•")
            for doc in documents:
                self.index.insert(doc)
        else:
            # åˆ›å»ºæ–°ç´¢å¼•
            if self.logger:
                self.logger.info("ğŸ†• åˆ›å»ºæ–°ç´¢å¼•")
            self.index = VectorStoreIndex.from_documents(
                documents,
                show_progress=show_progress
            )
        
        # æŒä¹…åŒ–
        self.index.storage_context.persist(persist_dir=self.persist_dir)
        
        elapsed = time.time() - start_time
        if self.logger:
            self.logger.success(f"âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ (è€—æ—¶ {elapsed:.1f}s)")
        
        return self.index
    
    def get_query_engine(
        self,
        similarity_top_k: int = 5,
        streaming: bool = True
    ):
        """
        è·å–æŸ¥è¯¢å¼•æ“
        
        Args:
            similarity_top_k: è¿”å›çš„ç›¸ä¼¼æ–‡æ¡£æ•°é‡
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            
        Returns:
            æŸ¥è¯¢å¼•æ“å¯¹è±¡
        """
        if not self.index:
            raise ValueError("ç´¢å¼•æœªåŠ è½½ï¼Œè¯·å…ˆåˆ›å»ºæˆ–åŠ è½½ç´¢å¼•")
        
        # å®šä¹‰ä¸­æ–‡é—®ç­”æ¨¡æ¿ (ä¼˜åŒ– Gemini/DeepSeek ç­‰æ¨¡å‹çš„æŒ‡ä»¤éµå¾ª)
        qa_prompt_tmpl_str = (
            "ä»¥ä¸‹æ˜¯å·²çŸ¥ä¿¡æ¯ï¼š\n"
            "---------------------\n"
            "{context_str}\n"
            "---------------------\n"
            "è¯·å®Œå…¨æ ¹æ®ä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä¸è¦ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†ã€‚\n"
            "å¦‚æœä¸Šä¸‹æ–‡ä¸­åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯¦ç»†å›ç­”ã€‚\n"
            "å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”â€œçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹â€ã€‚\n"
            "é—®é¢˜ï¼š{query_str}\n"
            "å›ç­”ï¼š"
        )
        qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)
        
        return self.index.as_query_engine(
            similarity_top_k=similarity_top_k,
            streaming=streaming,
            text_qa_template=qa_prompt_tmpl
        )

    def get_chat_engine(
        self,
        chat_mode: str = "context",
        similarity_top_k: int = 5,
        streaming: bool = True
    ):
        """
        è·å–èŠå¤©å¼•æ“
        
        Args:
            chat_mode: èŠå¤©æ¨¡å¼ ("context", "condense_question", "simple")
            similarity_top_k: è¿”å›çš„ç›¸ä¼¼æ–‡æ¡£æ•°é‡
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            
        Returns:
            èŠå¤©å¼•æ“å¯¹è±¡
        """
        if not self.index:
            raise ValueError("ç´¢å¼•æœªåŠ è½½ï¼Œè¯·å…ˆåˆ›å»ºæˆ–åŠ è½½ç´¢å¼•")
            
        from llama_index.core.memory import ChatMemoryBuffer
        
        return self.index.as_chat_engine(
            chat_mode=chat_mode,
            memory=ChatMemoryBuffer.from_defaults(token_limit=2000),
            similarity_top_k=similarity_top_k,
            streaming=streaming,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œè¯·åŠ¡å¿…ä»…åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚å›ç­”åº”æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šã€‚"
        )
    
    def query(
        self, 
        question: str,
        top_k: int = 5,
        streaming: bool = True
    ):
        """
        æŸ¥è¯¢çŸ¥è¯†åº“
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›çš„ç›¸ä¼¼æ–‡æ¡£æ•°é‡
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            
        Returns:
            æŸ¥è¯¢å“åº”
        """
        if not self.index:
            raise ValueError("ç´¢å¼•æœªåŠ è½½ï¼Œè¯·å…ˆåˆ›å»ºæˆ–åŠ è½½ç´¢å¼•")
        
        if self.logger:
            self.logger.info(f"ğŸ” æŸ¥è¯¢: {question[:50]}...")
        
        query_engine = self.get_query_engine(
            similarity_top_k=top_k,
            streaming=streaming
        )
        
        start_time = time.time()
        response = query_engine.query(question)
        elapsed = time.time() - start_time
        
        if self.logger:
            self.logger.success(f"âœ… æŸ¥è¯¢å®Œæˆ (è€—æ—¶ {elapsed:.1f}s)")
        
        return response
    
    def get_stats(self) -> Dict:
        """
        è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.index:
            return {
                "status": "æœªåŠ è½½",
                "documents": 0,
                "nodes": 0
            }
        
        try:
            docstore = self.index.docstore
            nodes = list(docstore.docs.values())
            
            return {
                "status": "å·²åŠ è½½",
                "documents": len(set(n.ref_doc_id for n in nodes if hasattr(n, 'ref_doc_id'))),
                "nodes": len(nodes),
                "persist_dir": self.persist_dir
            }
        except:
            return {
                "status": "å·²åŠ è½½",
                "persist_dir": self.persist_dir
            }
    
    def delete(self):
        """åˆ é™¤çŸ¥è¯†åº“"""
        if os.path.exists(self.persist_dir):
            shutil.rmtree(self.persist_dir)
            if self.logger:
                self.logger.success(f"âœ… çŸ¥è¯†åº“å·²åˆ é™¤: {self.kb_name}")
        
        self.index = None
    
    def __repr__(self):
        stats = self.get_stats()
        return f"RAGEngine(kb_name='{self.kb_name}', status='{stats['status']}')"


def create_rag_engine(kb_name: str, logger=None) -> Optional['RAGEngine']:
    """
    åˆ›å»º RAGEngine å®ä¾‹çš„å·¥å‚å‡½æ•°
    
    Args:
        kb_name: çŸ¥è¯†åº“åç§°
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        RAGEngine å®ä¾‹ï¼Œå¦‚æœåˆ›å»ºå¤±è´¥åˆ™è¿”å› None
    """
    try:
        from src.core.app_config import load_config, output_base
        from src.utils.model_manager import load_embedding_model, load_llm_model
        
        # åŠ è½½é…ç½®
        config = load_config()
        persist_dir = os.path.join(output_base, kb_name)
        
        # ç¡®ä¿é…ç½®å€¼æœ‰æ•ˆ (é˜²æ­¢ç©ºå­—ç¬¦ä¸²å¯¼è‡´æ¨¡å‹åŠ è½½å¤±è´¥)
        llm_provider = config.get('llm_provider') or "Ollama"
        llm_model_name = config.get('llm_model') or "gpt-oss:20b"
        
        # åŠ è½½ Embedding æ¨¡å‹
        embed_model = load_embedding_model(
            provider=config.get('embed_provider'),
            model_name=config.get('embed_model'),
            api_key=config.get('embed_key'),
            api_url=config.get('embed_url')
        )
        
        # åŠ è½½ LLM æ¨¡å‹
        llm_model = load_llm_model(
            provider=llm_provider,
            model_name=llm_model_name,
            api_key=config.get('llm_key'),
            api_url=config.get('llm_url'),
            temperature=config.get('temperature', 0.7)
        )
        
        # åˆ›å»ºå¼•æ“å®ä¾‹
        engine = RAGEngine(
            kb_name=kb_name,
            persist_dir=persist_dir,
            embed_model=embed_model,
            llm_model=llm_model,
            logger=logger
        )
        
        # åŠ è½½å·²æœ‰ç´¢å¼•
        if engine.load_existing_index():
            return engine
        else:
            if logger:
                logger.error(f"âŒ æ— æ³•åŠ è½½çŸ¥è¯†åº“ç´¢å¼•: {kb_name}")
            return None
            
    except Exception as e:
        if logger:
            logger.error(f"âŒ åˆ›å»º RAG å¼•æ“å¤±è´¥: {str(e)}")
        return None