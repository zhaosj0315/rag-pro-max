"""
è‡ªå®šä¹‰åµŒå…¥æ¨¡åž‹ - æ”¯æŒå¤§ batch_sizeï¼Œç»•è¿‡ LlamaIndex é™åˆ¶
"""
from typing import List
import torch
from transformers import AutoTokenizer, AutoModel
from llama_index.core.embeddings import BaseEmbedding


class CustomHuggingFaceEmbedding(BaseEmbedding):
    """è‡ªå®šä¹‰ HuggingFace åµŒå…¥ï¼Œæ”¯æŒå¤§ batch_size"""
    
    def __init__(
        self,
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        cache_folder: str = "./hf_cache",
        batch_size: int = 2048,
        device: str = "mps"
    ):
        super().__init__()
        self._model_name = model_name
        self._batch_size = batch_size
        self._device = device
        
        print(f"ðŸ”„ åŠ è½½æ¨¡åž‹: {model_name}")
        print(f"ðŸ“¦ Batch Size: {batch_size}")
        print(f"ðŸŽ® è®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡åž‹å’Œåˆ†è¯å™¨
        self._tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_folder
        )
        self._model = AutoModel.from_pretrained(
            model_name,
            cache_dir=cache_folder
        ).to(device)
        self._model.eval()
        
        # ä¼˜åŒ–GPUåˆ©ç”¨çŽ‡
        if device in ["mps", "cuda"]:
            try:
                # PyTorch 2.0+ ç¼–è¯‘ä¼˜åŒ–
                if hasattr(torch, 'compile'):
                    self._model = torch.compile(self._model, mode="max-autotune")
                    print(f"ðŸš€ å·²å¯ç”¨ torch.compile åŠ é€Ÿ")
            except:
                pass
        
        print(f"âœ… æ¨¡åž‹åŠ è½½å®Œæˆ")
    
    def _mean_pooling(self, model_output, attention_mask):
        """å¹³å‡æ± åŒ–"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def _get_query_embedding(self, query: str) -> List[float]:
        """èŽ·å–æŸ¥è¯¢åµŒå…¥"""
        return self._get_text_embedding(query)
    
    def _get_text_embedding(self, text: str) -> List[float]:
        """èŽ·å–æ–‡æœ¬åµŒå…¥"""
        return self._get_text_embeddings([text])[0]
    
    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡èŽ·å–æ–‡æœ¬åµŒå…¥ - æ”¯æŒå¤§ batch_sizeï¼Œä¼˜åŒ–GPUåˆ©ç”¨çŽ‡"""
        all_embeddings = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), self._batch_size):
            batch_texts = texts[i:i + self._batch_size]
            
            # ç¼–ç ï¼ˆä½¿ç”¨ pin_memory åŠ é€Ÿä¼ è¾“ï¼‰
            encoded_input = self._tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            )
            
            # ä¼˜åŒ–æ•°æ®ä¼ è¾“
            if self._device == "cuda":
                encoded_input = {k: v.pin_memory().to(self._device, non_blocking=True) 
                               for k, v in encoded_input.items()}
            else:
                encoded_input = {k: v.to(self._device) for k, v in encoded_input.items()}
            
            # æŽ¨ç†
            with torch.no_grad():
                model_output = self._model(**encoded_input)
            
            # æ± åŒ–
            embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])
            
            # å½’ä¸€åŒ–
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨
            all_embeddings.extend(embeddings.cpu().tolist())
        
        return all_embeddings
    
    async def _aget_query_embedding(self, query: str) -> List[float]:
        """å¼‚æ­¥èŽ·å–æŸ¥è¯¢åµŒå…¥"""
        return self._get_query_embedding(query)
    
    async def _aget_text_embedding(self, text: str) -> List[float]:
        """å¼‚æ­¥èŽ·å–æ–‡æœ¬åµŒå…¥"""
        return self._get_text_embedding(text)


def create_custom_embedding(
    model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    cache_folder: str = "./hf_cache",
    batch_size: int = 2048,
    device: str = "mps"
) -> CustomHuggingFaceEmbedding:
    """åˆ›å»ºè‡ªå®šä¹‰åµŒå…¥æ¨¡åž‹"""
    return CustomHuggingFaceEmbedding(
        model_name=model_name,
        cache_folder=cache_folder,
        batch_size=batch_size,
        device=device
    )
