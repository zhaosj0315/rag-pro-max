"""
é…ç½®è¡¨å•ç»„ä»¶
Stage 3.2.2 - ä¸­é£é™©é‡æ„
æå–è‡ª apppro.py
ä½¿ç”¨ç»Ÿä¸€é…ç½®ç»„ä»¶
"""

import os
import streamlit as st
from typing import Tuple, Dict
from .model_selectors import (
    render_ollama_model_selector,
    render_openai_model_selector,
    render_hf_embedding_selector
)
from .unified_config_components import render_basic_config, render_embedding_config
from src.utils.model_utils import fetch_remote_models
from src.services.unified_config_service import save_config, load_config
from src.utils.model_manager import set_global_llm_model


def render_llm_config(defaults: dict) -> Tuple[str, str, str, str, dict]:
    """
    æ¸²æŸ“ LLM é…ç½®è¡¨å• (å¢å¼ºç‰ˆ)
    Returns: (provider, url, model, key, extra_params)
    """
    with st.container(border=True):
        st.markdown("#### ğŸ¤– LLM å¯¹è¯æ¨¡å‹")
        
        # 1. ä¾›åº”å•†é€‰æ‹©
        provider_options = [
            "Ollama (æœ¬åœ°)", 
            "OpenAI (äº‘ç«¯)", 
            "Azure OpenAI", 
            "Anthropic (Claude)", 
            "Moonshot (Kimi)", 
            "Gemini (Google)", 
            "Groq (æé€Ÿ)",
            "Other (è‡ªå®šä¹‰)"
        ]
        
        # å°è¯•æ¢å¤ä¸Šæ¬¡çš„é€‰æ‹©
        saved_provider = defaults.get("llm_provider_label", "Ollama (æœ¬åœ°)")
        if saved_provider not in provider_options:
            saved_provider = "Ollama (æœ¬åœ°)"
            
        llm_provider_choice = st.selectbox(
            "ä¾›åº”å•†",
            provider_options,
            index=provider_options.index(saved_provider),
            key="config_llm_provider_select"
        )
        
        # ä¿å­˜æ˜¾ç¤ºæ ‡ç­¾ä»¥ä¾¿ä¸‹æ¬¡æ¢å¤
        st.session_state.llm_provider_label = llm_provider_choice
        
        llm_provider = ""
        llm_url = ""
        llm_model = ""
        llm_key = ""
        extra_params = {}

        # 2. åŠ¨æ€é…ç½®è¡¨å•
        if llm_provider_choice.startswith("Ollama"):
            llm_provider = "Ollama"
            col_url, col_status = st.columns([3, 1])
            with col_url:
                llm_url = st.text_input("Ollama URL", defaults.get("llm_url_ollama") or "http://localhost:11434", key="config_ollama_url")
            
            from src.utils.model_utils import check_ollama_status
            ollama_ok = check_ollama_status(llm_url)
            
            with col_status:
                st.write("")
                if ollama_ok:
                    st.caption("âœ… å·²è¿æ¥")
                else:
                    st.caption("âš ï¸ æœªè¿è¡Œ")
            
            saved_model = defaults.get("llm_model_ollama", "gpt-oss:20b")
            llm_model, _ = render_ollama_model_selector(llm_url, saved_model, ollama_ok)
            
            # æŒä¹…åŒ–ä¿å­˜æŒ‰é’® (æ–°å¢)
            if st.button("ğŸ’¾ ä¿å­˜ Ollama é…ç½®", key="save_ollama_config"):
                config_data = {
                    "llm_provider": "Ollama",
                    "llm_url_ollama": llm_url,
                    "llm_model_ollama": llm_model,
                    "llm_provider_label": "Ollama (æœ¬åœ°)"
                }
                
                # åŠ è½½ç°æœ‰é…ç½®å¹¶åˆå¹¶
                existing_config = load_config("rag_config")
                existing_config.update(config_data)
                
                # ä¿å­˜åˆ° rag_config.json
                if save_config(existing_config, "rag_config"):
                    # ç«‹å³ç”Ÿæ•ˆï¼šæ›´æ–°å…¨å±€ LLM
                    if set_global_llm_model(llm_provider, llm_model, "", llm_url):
                        st.success("âœ… Ollama é…ç½®å·²ä¿å­˜å¹¶ç”Ÿæ•ˆ (Hot Reload)")
                        st.session_state.selected_model = llm_model
                    else:
                        st.warning("âš ï¸ é…ç½®å·²ä¿å­˜ï¼Œä½†çƒ­æ›´æ–°å¤±è´¥")
                        
                    defaults.update(config_data)
                else:
                    st.error("âŒ ä¿å­˜å¤±è´¥")
            
        elif llm_provider_choice.startswith("OpenAI"):
            llm_provider = "OpenAI"
            
            col1, col2 = st.columns([2, 1])
            with col1:
                llm_url = st.text_input("Base URL", defaults.get("llm_url_openai", "https://api.openai.com/v1"), key="config_openai_url")
            with col2:
                llm_key = st.text_input("API Key", defaults.get("llm_key", ""), type="password", key="config_openai_key")
            
            # è‡ªåŠ¨è·å–æ¨¡å‹é€»è¾‘
            # ä½¿ç”¨ URL + Key ä½œä¸ºç¼“å­˜é”®
            cache_key = f"models_openai_{llm_url}_{llm_key}"
            available_models = st.session_state.get(cache_key, [])
            
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ä¸”æœ‰è¶³å¤Ÿçš„å‡­è¯ï¼Œå°è¯•è·å–
            if not available_models and llm_url and llm_key:
                with st.spinner("ğŸ”„ æ­£åœ¨è‡ªåŠ¨åŠ è½½æ¨¡å‹åˆ—è¡¨..."):
                    models, err = fetch_remote_models(llm_url, llm_key)
                    if models:
                        available_models = models
                        st.session_state[cache_key] = models
                        st.toast(f"âœ… å·²åŠ è½½ {len(models)} ä¸ªæ¨¡å‹")
                    elif err:
                        st.caption(f"âš ï¸ æ— æ³•åŠ è½½æ¨¡å‹: {err}")
            
            # æ¨¡å‹é€‰æ‹©å™¨
            saved_model = defaults.get("llm_model_openai", "gpt-3.5-turbo")
            
            if available_models:
                # ç¡®ä¿ä¿å­˜çš„æ¨¡å‹åœ¨åˆ—è¡¨ä¸­
                if saved_model not in available_models:
                    available_models.insert(0, saved_model)
                
                llm_model = st.selectbox(
                    "é€‰æ‹©æ¨¡å‹", 
                    available_models, 
                    index=available_models.index(saved_model) if saved_model in available_models else 0,
                    key="config_openai_model_select"
                )
            else:
                llm_model = st.text_input("æ¨¡å‹åç§°", saved_model, help="æ— æ³•è‡ªåŠ¨åŠ è½½æ—¶è¯·æ‰‹åŠ¨è¾“å…¥", key="config_openai_model_input")
            
            # æŒä¹…åŒ–ä¿å­˜æŒ‰é’®
            if st.button("ğŸ’¾ ä¿å­˜ OpenAI é…ç½®", key="save_openai_config"):
                config_data = {
                    "llm_provider": "OpenAI",
                    "llm_url_openai": llm_url,
                    "llm_key": llm_key,
                    "llm_model_openai": llm_model,
                    "llm_provider_label": "OpenAI (äº‘ç«¯)"
                }
                
                # åŠ è½½ç°æœ‰é…ç½®å¹¶åˆå¹¶
                existing_config = load_config("rag_config")
                existing_config.update(config_data)
                
                # ä¿å­˜åˆ° rag_config.json
                if save_config(existing_config, "rag_config"):
                    # ç«‹å³ç”Ÿæ•ˆï¼šæ›´æ–°å…¨å±€ LLM
                    if set_global_llm_model(llm_provider, llm_model, llm_key, llm_url):
                        st.success("âœ… é…ç½®å·²ä¿å­˜å¹¶ç”Ÿæ•ˆ (Hot Reload)")
                        st.session_state.selected_model = llm_model  # <--- å…³é”®ä¿®å¤ï¼šç«‹å³æ›´æ–°å‰ç«¯çŠ¶æ€
                    else:
                        st.warning("âš ï¸ é…ç½®å·²ä¿å­˜ï¼Œä½†çƒ­æ›´æ–°å¤±è´¥")
                        
                    # åŒæ—¶ä¹Ÿæ›´æ–°å½“å‰çš„ defaults ä»¥ä¾¿å³æ—¶ç”Ÿæ•ˆ (å¯é€‰)
                    defaults.update(config_data)
                else:
                    st.error("âŒ ä¿å­˜å¤±è´¥")

        elif llm_provider_choice.startswith("Other"):
            llm_provider = "OpenAI-Compatible"
            st.caption("ğŸ’¡ é€‚ç”¨äº DeepSeek, Yi, ChatGLM, vLLM ç­‰å…¼å®¹ OpenAI åè®®çš„æœåŠ¡")
            
            col1, col2 = st.columns([2, 1])
            with col1:
                # æ™ºèƒ½å›é€€ï¼šå¦‚æœ specific key ä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨ generic key
                def_url = defaults.get("llm_url_other") or defaults.get("llm_url") or "https://api.deepseek.com/v1"
                llm_url = st.text_input("Base URL", def_url, key="config_other_url")
            with col2:
                # æ™ºèƒ½å›é€€ï¼šå¦‚æœ specific key ä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨ generic key
                def_key = defaults.get("llm_key_other") or defaults.get("llm_key", "")
                llm_key = st.text_input("API Key", def_key, type="password", key="config_other_key")
            
            # è‡ªåŠ¨è·å–æ¨¡å‹é€»è¾‘ (å¤ç”¨ OpenAI é€»è¾‘)
            cache_key = f"models_other_{llm_url}_{llm_key}"
            available_models = st.session_state.get(cache_key, [])
            
            if not available_models and llm_url:
                with st.spinner("ğŸ”„ æ­£åœ¨æ¢æµ‹æ¨¡å‹åˆ—è¡¨..."):
                    models, err = fetch_remote_models(llm_url, llm_key)
                    if models:
                        available_models = models
                        st.session_state[cache_key] = models
                        st.toast(f"âœ… å·²åŠ è½½ {len(models)} ä¸ªæ¨¡å‹")
            
            saved_model = defaults.get("llm_model_other", "")
            
            if available_models:
                if saved_model and saved_model not in available_models:
                    available_models.insert(0, saved_model)
                
                llm_model = st.selectbox(
                    "é€‰æ‹©æ¨¡å‹", 
                    available_models, 
                    index=available_models.index(saved_model) if saved_model in available_models else 0,
                    key="config_other_model_select"
                )
            else:
                llm_model = st.text_input("æ¨¡å‹åç§°", saved_model, placeholder="ä¾‹å¦‚: deepseek-chat", key="config_other_model_input")
            
            if st.button("ğŸ’¾ ä¿å­˜è‡ªå®šä¹‰é…ç½®", key="save_other_config"):
                config_data = {
                    "llm_provider": "OpenAI-Compatible", # å†…éƒ¨æ ‡è¯†ä¸ºå…¼å®¹æ¨¡å¼
                    "llm_url_other": llm_url,
                    "llm_key_other": llm_key,
                    "llm_model_other": llm_model,
                    "llm_provider_label": "Other (è‡ªå®šä¹‰)"
                }
                
                # ä¸ºäº†å…¼å®¹ç»Ÿä¸€è¯»å–é€»è¾‘ï¼Œæˆ‘ä»¬åŒæ—¶ä¹Ÿå†™å…¥æ ‡å‡†å­—æ®µ
                config_data["llm_url"] = llm_url
                config_data["llm_key"] = llm_key
                config_data["llm_model"] = llm_model
                
                # åŠ è½½ç°æœ‰é…ç½®å¹¶åˆå¹¶
                existing_config = load_config("rag_config")
                existing_config.update(config_data)
                
                if save_config(existing_config, "rag_config"):
                    if set_global_llm_model("OpenAI-Compatible", llm_model, llm_key, llm_url):
                        st.success("âœ… è‡ªå®šä¹‰é…ç½®å·²ä¿å­˜å¹¶ç”Ÿæ•ˆ")
                        st.session_state.selected_model = llm_model  # <--- å…³é”®ä¿®å¤ï¼šç«‹å³æ›´æ–°å‰ç«¯çŠ¶æ€
                    defaults.update(config_data)
                else:
                    st.error("âŒ ä¿å­˜å¤±è´¥")
            
        elif llm_provider_choice.startswith("Azure"):
            llm_provider = "Azure OpenAI"
            llm_url = st.text_input("Azure Endpoint", defaults.get("azure_endpoint", ""), placeholder="https://{resource}.openai.azure.com/", key="config_azure_endpoint")
            llm_key = st.text_input("API Key", defaults.get("azure_key", ""), type="password", key="config_azure_key")
            llm_model = st.text_input("Deployment Name", defaults.get("azure_deployment", ""), help="åœ¨Azureæ§åˆ¶å°ä¸­éƒ¨ç½²çš„æ¨¡å‹åç§°", key="config_azure_deployment")
            
            api_version = st.text_input("API Version", defaults.get("azure_api_version", "2023-05-15"), help="ä¾‹å¦‚: 2023-05-15, 2024-02-15-preview", key="config_azure_api_version")
            extra_params = {"api_version": api_version}
            
            if st.button("ğŸ’¾ ä¿å­˜ Azure é…ç½®", key="save_azure_config"):
                config_data = {
                    "llm_provider": "Azure OpenAI",
                    "azure_endpoint": llm_url,
                    "azure_key": llm_key,
                    "azure_deployment": llm_model,
                    "azure_api_version": api_version,
                    "llm_provider_label": "Azure OpenAI",
                    # å…¼å®¹æ€§å­—æ®µ
                    "llm_url": llm_url,
                    "llm_key": llm_key,
                    "llm_model": llm_model
                }
                
                existing_config = load_config("rag_config")
                existing_config.update(config_data)
                
                if save_config(existing_config, "rag_config"):
                    if set_global_llm_model("Azure OpenAI", llm_model, llm_key, llm_url, api_version=api_version):
                        st.success("âœ… Azure é…ç½®å·²ä¿å­˜å¹¶ç”Ÿæ•ˆ")
                        st.session_state.selected_model = llm_model
                    else:
                        st.warning("âš ï¸ ä¿å­˜æˆåŠŸä½†çƒ­æ›´æ–°å¤±è´¥")
                    defaults.update(config_data)
                else:
                    st.error("âŒ ä¿å­˜å¤±è´¥")
            
        elif llm_provider_choice.startswith("Anthropic"):
            llm_provider = "Anthropic"
            llm_key = st.text_input("API Key", defaults.get("anthropic_key", ""), type="password", key="config_anthropic_key")
            llm_model = st.selectbox("æ¨¡å‹", ["claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"], index=0, key="config_anthropic_model")
            
            if st.button("ğŸ’¾ ä¿å­˜ Anthropic é…ç½®", key="save_anthropic_config"):
                config_data = {
                    "llm_provider": "Anthropic",
                    "anthropic_key": llm_key,
                    "config_anthropic_model": llm_model,
                    "llm_provider_label": "Anthropic (Claude)",
                    "llm_key": llm_key,
                    "llm_model": llm_model
                }
                
                existing_config = load_config("rag_config")
                existing_config.update(config_data)
                
                if save_config(existing_config, "rag_config"):
                    if set_global_llm_model("Anthropic", llm_model, llm_key):
                        st.success("âœ… Anthropic é…ç½®å·²ä¿å­˜å¹¶ç”Ÿæ•ˆ")
                        st.session_state.selected_model = llm_model
                    defaults.update(config_data)
                else:
                    st.error("âŒ ä¿å­˜å¤±è´¥")
            
        elif llm_provider_choice.startswith("Moonshot"):
            llm_provider = "Moonshot"
            llm_url = st.text_input("Base URL", "https://api.moonshot.cn/v1", disabled=True, key="config_moonshot_url")
            llm_key = st.text_input("API Key", defaults.get("moonshot_key", ""), type="password", key="config_moonshot_key")
            llm_model = st.selectbox("æ¨¡å‹", ["moonshot-v1-8k", "moonshot-v1-32k", "moonshot-v1-128k"], index=0, key="config_moonshot_model")
            
            if st.button("ğŸ’¾ ä¿å­˜ Moonshot é…ç½®", key="save_moonshot_config"):
                config_data = {
                    "llm_provider": "Moonshot",
                    "moonshot_key": llm_key,
                    "config_moonshot_model": llm_model,
                    "llm_provider_label": "Moonshot (Kimi)",
                    "llm_key": llm_key,
                    "llm_model": llm_model,
                    "llm_url": llm_url
                }
                
                existing_config = load_config("rag_config")
                existing_config.update(config_data)
                
                if save_config(existing_config, "rag_config"):
                    if set_global_llm_model("Moonshot", llm_model, llm_key, llm_url):
                        st.success("âœ… Moonshot é…ç½®å·²ä¿å­˜å¹¶ç”Ÿæ•ˆ")
                        st.session_state.selected_model = llm_model
                    defaults.update(config_data)
                else:
                    st.error("âŒ ä¿å­˜å¤±è´¥")
            
        elif llm_provider_choice.startswith("Gemini"):
            llm_provider = "Gemini"
            llm_key = st.text_input("API Key", defaults.get("gemini_key", ""), type="password", key="config_gemini_key")
            llm_model = st.selectbox("æ¨¡å‹", ["gemini-pro", "gemini-pro-vision"], index=0, key="config_gemini_model")
            
            if st.button("ğŸ’¾ ä¿å­˜ Gemini é…ç½®", key="save_gemini_config"):
                config_data = {
                    "llm_provider": "Gemini",
                    "gemini_key": llm_key,
                    "config_gemini_model": llm_model,
                    "llm_provider_label": "Gemini (Google)",
                    "llm_key": llm_key,
                    "llm_model": llm_model
                }
                
                existing_config = load_config("rag_config")
                existing_config.update(config_data)
                
                if save_config(existing_config, "rag_config"):
                    if set_global_llm_model("Gemini", llm_model, llm_key):
                        st.success("âœ… Gemini é…ç½®å·²ä¿å­˜å¹¶ç”Ÿæ•ˆ")
                        st.session_state.selected_model = llm_model
                    defaults.update(config_data)
                else:
                    st.error("âŒ ä¿å­˜å¤±è´¥")
            
        elif llm_provider_choice.startswith("Groq"):
            llm_provider = "Groq"
            llm_url = st.text_input("Base URL", "https://api.groq.com/openai/v1", disabled=True, key="config_groq_url")
            llm_key = st.text_input("API Key", defaults.get("groq_key", ""), type="password", key="config_groq_key")
            llm_model = st.selectbox("æ¨¡å‹", ["llama3-8b-8192", "llama3-70b-8192", "mixtral-8x7b-32768"], index=0, key="config_groq_model")

            if st.button("ğŸ’¾ ä¿å­˜ Groq é…ç½®", key="save_groq_config"):
                config_data = {
                    "llm_provider": "Groq",
                    "groq_key": llm_key,
                    "config_groq_model": llm_model,
                    "llm_provider_label": "Groq (æé€Ÿ)",
                    "llm_key": llm_key,
                    "llm_model": llm_model,
                    "llm_url": llm_url
                }
                
                existing_config = load_config("rag_config")
                existing_config.update(config_data)
                
                if save_config(existing_config, "rag_config"):
                    if set_global_llm_model("Groq", llm_model, llm_key, llm_url):
                        st.success("âœ… Groq é…ç½®å·²ä¿å­˜å¹¶ç”Ÿæ•ˆ")
                        st.session_state.selected_model = llm_model
                    defaults.update(config_data)
                else:
                    st.error("âŒ ä¿å­˜å¤±è´¥")

    return llm_provider, llm_url, llm_model, llm_key, extra_params

    return llm_provider, llm_url, llm_model, llm_key, extra_params


def render_embedding_config(defaults: dict) -> Tuple[str, str, str, str]:
    """
    æ¸²æŸ“ Embedding é…ç½®è¡¨å• (ä¼˜åŒ–ç‰ˆ)
    """
    with st.container(border=True):
        st.markdown("#### ğŸ§¬ å‘é‡æ¨¡å‹ (Embedding)")
        
        embed_idx = defaults.get("embed_provider_idx", 0)
        if embed_idx > 2: embed_idx = 0
        
        col1, col2 = st.columns([1, 1.5])
        
        with col1:
            embed_provider = st.selectbox(
                "ä¾›åº”å•†",
                ["HuggingFace (æœ¬åœ°/æé€Ÿ)", "OpenAI-Compatible", "Ollama"],
                index=embed_idx,
                key="config_embed_provider",
                label_visibility="collapsed"
            )
        
        with col2:
            if embed_provider.startswith("HuggingFace"):
                saved_model = defaults.get("embed_model_hf", "sentence-transformers/all-MiniLM-L6-v2")
                embed_model = render_hf_embedding_selector(saved_model)
                embed_url = ""
                embed_key = ""
                
                # å¤„ç†é»˜è®¤ä¿å­˜
                if st.session_state.get('save_embed_model'):
                    from src.config import ConfigLoader
                    import time
                    config = ConfigLoader.load()
                    config["embed_model_hf"] = st.session_state.save_embed_model
                    ConfigLoader.save(config)
                    st.toast(f"âœ… é»˜è®¤åµŒå…¥æ¨¡å‹å·²æ›´æ–°")
                    del st.session_state.save_embed_model
                    time.sleep(1)
                    st.rerun()
            elif embed_provider.startswith("OpenAI"):
                embed_model = st.text_input("æ¨¡å‹å", defaults.get("embed_model_openai", "text-embedding-3-small"))
                embed_url = st.text_input("Base URL", defaults.get("embed_url_openai", "https://api.openai.com/v1"))
                embed_key = st.text_input("API Key", defaults.get("embed_key", ""), type="password")
            else:  # Ollama
                embed_model = st.text_input("æ¨¡å‹å", defaults.get("embed_model_ollama", "nomic-embed-text"))
                embed_url = st.text_input("URL", defaults.get("embed_url_ollama", "http://localhost:11434"))
                embed_key = ""
                
    return embed_provider, embed_model, embed_url, embed_key


def render_basic_config(defaults: dict) -> dict:
    """
    æ¸²æŸ“å®Œæ•´çš„åŸºç¡€é…ç½®åŒºåŸŸ
    
    Args:
        defaults: é»˜è®¤é…ç½®å­—å…¸
        
    Returns:
        dict: é…ç½®å­—å…¸
    """
    # LLM é…ç½® (æ¥æ”¶ 5 ä¸ªè¿”å›å€¼)
    llm_provider, llm_url, llm_model, llm_key, extra_params = render_llm_config(defaults)
    
    # Embedding é…ç½®
    embed_provider, embed_model, embed_url, embed_key = render_embedding_config(defaults)
    
    # åˆå¹¶ extra_params åˆ°è¿”å›ç»“æœ
    result = {
        'llm_provider': llm_provider,
        'llm_url': llm_url,
        'llm_model': llm_model,
        'llm_key': llm_key,
        'embed_provider': embed_provider,
        'embed_model': embed_model,
        'embed_url': embed_url,
        'embed_key': embed_key
    }
    result.update(extra_params)
    return result
