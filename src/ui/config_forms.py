"""
é…ç½®è¡¨å•ç»„ä»¶
Stage 3.2.2 - ä¸­é£é™©é‡æ„
æå–è‡ª apppro.py
ä½¿ç”¨ç»Ÿä¸€é…ç½®ç»„ä»¶
"""

import os
import streamlit as st
from typing import Tuple, Dict
from .model_selectors import (
    render_ollama_model_selector,
    render_openai_model_selector,
    render_hf_embedding_selector
)
from src.utils.model_utils import fetch_remote_models
from src.services.unified_config_service import save_config, load_config
from src.utils.model_manager import set_global_llm_model


def render_llm_config(defaults: dict) -> Tuple[str, str, str, str, dict]:
    """
    æ¸²æŸ“ LLM é…ç½®è¡¨å• (v3.2 é¡¶éƒ¨å¯¼èˆª + ä¿®å¤æ•°æ®è¦†ç›– Bug)
    """
    st.markdown("#### ğŸ§  æ¨¡å‹æœåŠ¡ä¸­å¿ƒ")
    
    # 1. å‡†å¤‡ä¾›åº”å•†æ•°æ®
    BASE_PROVIDERS = {
        "Ollama": "ğŸ¦™ Ollama",
        "OpenAI": "â˜ï¸ OpenAI",
        "OpenAI-Compatible": "ğŸ”Œ OpenAI-Other",
        "Azure OpenAI": "ğŸŸ¦ Azure",
        "Anthropic": "ğŸ§  Anthropic",
        "Moonshot": "ğŸŒ™ Moonshot",
        "Gemini": "ğŸ’ Gemini",
        "Groq": "âš¡ Groq"
    }
    
    custom_providers = defaults.get("custom_llm_providers", {})
    PROVIDERS = BASE_PROVIDERS.copy()
    for cp_id, cp_info in custom_providers.items():
        PROVIDERS[cp_id] = f"ğŸ¨ {cp_info.get('name', cp_id)}"
    
    nav_keys = list(PROVIDERS.keys()) + ["ADD_CUSTOM"]
    
    # --- æ ¸å¿ƒä¿®å¤ 1: ä½¿ç”¨é¡¶éƒ¨å•é€‰æ¡†æ¨¡æ‹Ÿæ ‡ç­¾é¡µï¼Œç¡®ä¿èƒ½è·å–é€‰ä¸­çš„ Key ---
    saved_provider = defaults.get("llm_provider", "Ollama")
    if saved_provider not in nav_keys: saved_provider = "Ollama"
    
    # åœ¨é¡¶éƒ¨æ˜¾ç¤ºæ°´å¹³é€‰æ‹©å™¨
    selected_key = st.radio(
        "å‚å•†åˆ‡æ¢",
        options=nav_keys,
        format_func=lambda x: PROVIDERS.get(x, "â• æ–°å¢è‡ªå®šä¹‰"),
        index=nav_keys.index(saved_provider),
        horizontal=True,
        key="top_provider_selector"
    )
    
    st.divider()

    # åˆå§‹è¿”å›å˜é‡ (æ ¸å¿ƒä¿®å¤ 2: ç¡®ä¿åªä»é€‰ä¸­çš„å‚å•†æå–æ•°æ®)
    llm_provider = selected_key
    llm_url = ""
    llm_model = ""
    llm_key = ""
    extra_params = {}

    # --- 2. æ ¹æ®é€‰ä¸­çš„ Key æ¸²æŸ“å¯¹åº”çš„é…ç½®å¡ç‰‡ ---
    with st.container(border=True):
        if selected_key == "ADD_CUSTOM":
            st.markdown("##### â• æ–°å¢è‡ªå®šä¹‰æœåŠ¡å•†")
            c1, c2 = st.columns(2)
            with c1:
                custom_name = st.text_input("å‚å•†åç§°", placeholder="MyAI", key="new_custom_name")
                custom_url = st.text_input("Base URL", placeholder="https://api.domain.com/v1", key="new_custom_url")
            with c2:
                custom_key = st.text_input("API Key", type="password", key="new_custom_key")
                custom_model = _render_remote_model_selector(custom_url, custom_key, "", "custom_new")
            
            if st.button("âœ¨ ç«‹å³åˆ›å»ºå¹¶ä¿å­˜", type="primary", use_container_width=True):
                if custom_name and custom_url:
                    cp_id = f"custom_{hash(custom_name + custom_url) % 10000}"
                    new_cp_info = {"name": custom_name, "url": custom_url, "key": custom_key, "model": custom_model}
                    existing_custom = defaults.get("custom_llm_providers", {})
                    existing_custom[cp_id] = new_cp_info
                    config_data = {
                        "custom_llm_providers": existing_custom,
                        "llm_provider": cp_id,
                        "llm_provider_label": f"ğŸ¨ {custom_name}",
                        f"llm_url_{cp_id}": custom_url,
                        f"llm_key_{cp_id}": custom_key,
                        f"llm_model_{cp_id}": custom_model
                    }
                    _save_and_apply_config(config_data, cp_id, custom_model, custom_key, custom_url, defaults)
                    st.rerun()
                else:
                    st.error("è¯·å¡«å†™å®Œæ•´ä¿¡æ¯")

        elif selected_key in custom_providers:
            # æ¸²æŸ“å·²æœ‰çš„è‡ªå®šä¹‰æœåŠ¡å•† (ä¸¥æ ¼é”šå®šæ•°æ®)
            cp = custom_providers[selected_key]
            st.markdown(f"##### {PROVIDERS[selected_key]} é…ç½®")
            col1, col2 = st.columns([2, 1])
            with col1:
                llm_url = st.text_input("Base URL", defaults.get(f"llm_url_{selected_key}") or cp.get('url', ""), key=f"config_{selected_key}_url")
            with col2:
                llm_key = st.text_input("API Key", defaults.get(f"llm_key_{selected_key}") or cp.get('key', ""), type="password", key=f"config_{selected_key}_key")
            
            saved_model = defaults.get(f"llm_model_{selected_key}") or cp.get('model', "")
            llm_model = _render_remote_model_selector(llm_url, llm_key, saved_model, selected_key)
            
            b1, b2 = st.columns([4, 1])
            with b1:
                if st.button(f"ğŸ’¾ ä¿å­˜ {cp['name']} ä¿®æ”¹", type="primary", use_container_width=True, key=f"save_{selected_key}"):
                    cp.update({"url": llm_url, "key": llm_key, "model": llm_model})
                    custom_providers[selected_key] = cp
                    _save_and_apply_config({"custom_llm_providers": custom_providers, "llm_provider": selected_key, f"llm_url_{selected_key}": llm_url, f"llm_key_{selected_key}": llm_key, f"llm_model_{selected_key}": llm_model}, selected_key, llm_model, llm_key, llm_url, defaults)
            with b2:
                if st.button("ğŸ—‘ï¸ åˆ é™¤", key=f"del_{selected_key}", use_container_width=True):
                    del custom_providers[selected_key]
                    _save_and_apply_config({"custom_llm_providers": custom_providers}, "Ollama", "gpt-oss:20b", "", "http://localhost:11434", defaults)
                    st.rerun()

        else:
            # å†…ç½®æœåŠ¡å•†é€»è¾‘ (ä¸¥æ ¼è¯»å– defaults)
            st.markdown(f"##### {PROVIDERS[selected_key]} è®¾ç½®")
            
            if selected_key == "Ollama":
                # URL, åˆ·æ–°, çŠ¶æ€ ä¸€è¡ŒåŒ–
                c1, c2, c3 = st.columns([3, 0.8, 1.2])
                with c1:
                    cur_ollama_url = st.text_input("Ollama URL", defaults.get("llm_url_ollama") or "http://localhost:11434", key="config_ollama_url", label_visibility="collapsed")
                
                from src.utils.model_utils import check_ollama_status, fetch_remote_models
                ollama_ok = check_ollama_status(cur_ollama_url)
                
                with c2:
                    if st.button("ğŸ”„", key="refresh_ollama_btn", help="åˆ·æ–° Ollama æ¨¡å‹åˆ—è¡¨"):
                        if ollama_ok:
                            from src.ui.model_selectors import _fetch_ollama_models
                            models = _fetch_ollama_models(cur_ollama_url)
                            if models:
                                st.session_state.ollama_models = models
                                st.toast(f"âœ… å·²åŠ è½½ {len(models)} ä¸ªæ¨¡å‹")
                                st.rerun()
                        else: st.warning("æœªè¿è¡Œ")
                
                with c3:
                    st.caption("âœ… å·²è¿æ¥" if ollama_ok else "âš ï¸ æœªè¿è¡Œ")
                
                saved_ollama_model = defaults.get("llm_model_ollama", "gpt-oss:20b")
                sel_ollama_model, _ = render_ollama_model_selector(cur_ollama_url, saved_ollama_model, ollama_ok)
                if st.button("ğŸ’¾ ä¿å­˜ Ollama é…ç½®", type="primary", use_container_width=True, key="save_ollama"):
                    _save_and_apply_config({"llm_provider": "Ollama", "llm_url_ollama": cur_ollama_url, "llm_model_ollama": sel_ollama_model, "llm_provider_label": PROVIDERS["Ollama"]}, "Ollama", sel_ollama_model, "", cur_ollama_url, defaults)
                
                # èµ‹å€¼ç»™è¿”å›å˜é‡
                llm_url, llm_model, llm_key = cur_ollama_url, sel_ollama_model, ""

            elif selected_key == "OpenAI":
                # URL ä¸ åˆ·æ–° ä¸€è¡ŒåŒ–
                c1, c2 = st.columns([4, 1])
                with c1: cur_openai_url = st.text_input("Base URL", defaults.get("llm_url_openai") or "https://api.openai.com/v1", key="config_openai_url", help="API åŸºç¡€åœ°å€")
                cur_openai_key = st.text_input("API Key", defaults.get("llm_key") or "", type="password", key="config_openai_key")
                
                with c2:
                    st.write("") # é—´è·å¯¹é½
                    if st.button("ğŸ”„", key="refresh_openai_btn", help="åˆ·æ–° OpenAI æ¨¡å‹åˆ—è¡¨"):
                        from src.utils.model_utils import fetch_remote_models
                        models, err = fetch_remote_models(cur_openai_url, cur_openai_key)
                        if models:
                            cache_key = f"models_openai_{cur_openai_url}_{cur_openai_key}"
                            st.session_state[cache_key] = models
                            st.toast(f"âœ… å·²åŠ è½½ {len(models)} ä¸ªæ¨¡å‹")
                            st.rerun()
                        else: st.error(f"å¤±è´¥: {err}")

                saved_openai_model = defaults.get("llm_model_openai", "gpt-3.5-turbo")
                sel_openai_model = _render_remote_model_selector(cur_openai_url, cur_openai_key, saved_openai_model, "openai")
                if st.button("ğŸ’¾ ä¿å­˜ OpenAI é…ç½®", type="primary", use_container_width=True, key="save_openai"):
                    _save_and_apply_config({"llm_provider": "OpenAI", "llm_url_openai": cur_openai_url, "llm_key": cur_openai_key, "llm_model_openai": sel_openai_model, "llm_provider_label": PROVIDERS["OpenAI"]}, "OpenAI", sel_openai_model, cur_openai_key, cur_openai_url, defaults)
                
                # èµ‹å€¼ç»™è¿”å›å˜é‡
                llm_url, llm_model, llm_key = cur_openai_url, sel_openai_model, cur_openai_key

            elif selected_key == "OpenAI-Compatible":
                # URL ä¸ åˆ·æ–° ä¸€è¡ŒåŒ–
                c1, c2 = st.columns([4, 1])
                with c1: cur_other_url = st.text_input("Base URL", defaults.get("llm_url_other") or "https://api.deepseek.com/v1", key="config_other_url")
                cur_other_key = st.text_input("API Key", defaults.get("llm_key_other") or "", type="password", key="config_other_key")
                
                with c2:
                    st.write("") # é—´è·å¯¹é½
                    if st.button("ğŸ”„", key="refresh_other_btn", help="åˆ·æ–°æ¨¡å‹åˆ—è¡¨"):
                        from src.utils.model_utils import fetch_remote_models
                        models, err = fetch_remote_models(cur_other_url, cur_other_key)
                        if models:
                            cache_key = f"models_other_{cur_other_url}_{cur_other_key}"
                            st.session_state[cache_key] = models
                            st.toast(f"âœ… å·²åŠ è½½ {len(models)} ä¸ªæ¨¡å‹")
                            st.rerun()
                        else: st.error(f"å¤±è´¥: {err}")

                saved_other_model = defaults.get("llm_model_other", "")
                sel_other_model = _render_remote_model_selector(cur_other_url, cur_other_key, saved_other_model, "other")
                if st.button("ğŸ’¾ ä¿å­˜è‡ªå®šä¹‰é…ç½®", type="primary", use_container_width=True, key="save_other"):
                    _save_and_apply_config({"llm_provider": "OpenAI-Compatible", "llm_url_other": cur_other_url, "llm_key_other": cur_other_key, "llm_model_other": sel_other_model, "llm_provider_label": PROVIDERS["OpenAI-Compatible"]}, "OpenAI-Compatible", sel_other_model, cur_other_key, cur_other_url, defaults)
                
                llm_url, llm_model, llm_key = cur_other_url, sel_other_model, cur_other_key

            elif selected_key == "Azure OpenAI":
                c1, c2 = st.columns(2)
                with c1:
                    cur_az_url = st.text_input("Azure Endpoint", defaults.get("azure_endpoint", ""), key="config_azure_endpoint")
                    cur_az_model = st.text_input("Deployment Name", defaults.get("azure_deployment", ""), key="config_azure_deployment")
                with c2:
                    cur_az_key = st.text_input("API Key", defaults.get("azure_key", ""), type="password", key="config_azure_key")
                    cur_az_ver = st.text_input("API Version", defaults.get("azure_api_version", "2023-05-15"), key="config_azure_api_version")
                if st.button("ğŸ’¾ ä¿å­˜ Azure é…ç½®", type="primary", use_container_width=True, key="save_azure"):
                    _save_and_apply_config({"llm_provider": "Azure OpenAI", "azure_endpoint": cur_az_url, "azure_key": cur_az_key, "azure_deployment": cur_az_model, "azure_api_version": cur_az_ver}, "Azure OpenAI", cur_az_model, cur_az_key, cur_az_url, defaults, api_version=cur_az_ver)
                
                llm_url, llm_model, llm_key = cur_az_url, cur_az_model, cur_az_key

            elif selected_key == "Anthropic":
                cur_ant_key = st.text_input("API Key", defaults.get("anthropic_key", ""), type="password", key="config_anthropic_key")
                cur_ant_model = st.selectbox("æ¨¡å‹", ["claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"], key="config_anthropic_model_sel")
                if st.button("ğŸ’¾ ä¿å­˜ Anthropic é…ç½®", type="primary", use_container_width=True, key="save_anthropic"):
                    _save_and_apply_config({"anthropic_key": cur_ant_key, "config_anthropic_model": cur_ant_model}, "Anthropic", cur_ant_model, cur_ant_key, "", defaults)
                
                llm_url, llm_model, llm_key = "", cur_ant_model, cur_ant_key

            elif selected_key == "Moonshot":
                ms_url = "https://api.moonshot.cn/v1"
                st.text_input("Base URL", ms_url, disabled=True, key="config_moonshot_url")
                cur_ms_key = st.text_input("API Key", defaults.get("moonshot_key", ""), type="password", key="config_moonshot_key")
                cur_ms_model = st.selectbox("æ¨¡å‹", ["moonshot-v1-8k", "moonshot-v1-32k", "moonshot-v1-128k"], key="config_moonshot_model_sel")
                if st.button("ğŸ’¾ ä¿å­˜ Moonshot é…ç½®", type="primary", use_container_width=True, key="save_moonshot"):
                    _save_and_apply_config({"moonshot_key": cur_ms_key, "config_moonshot_model": cur_ms_model, "llm_url": ms_url}, "Moonshot", cur_ms_model, cur_ms_key, ms_url, defaults)
                
                llm_url, llm_model, llm_key = ms_url, cur_ms_model, cur_ms_key
            
            elif selected_key == "Gemini":
                cur_gem_key = st.text_input("API Key", defaults.get("gemini_key", ""), type="password", key="config_gemini_key")
                cur_gem_model = st.selectbox("æ¨¡å‹", ["gemini-pro", "gemini-pro-vision"], key="config_gemini_model_sel")
                if st.button("ğŸ’¾ ä¿å­˜ Gemini é…ç½®", type="primary", use_container_width=True, key="save_gemini"):
                    _save_and_apply_config({"gemini_key": cur_gem_key, "config_gemini_model": cur_gem_model}, "Gemini", cur_gem_model, cur_gem_key, "", defaults)
                
                llm_url, llm_model, llm_key = "", cur_gem_model, cur_gem_key
            
            elif selected_key == "Groq":
                groq_url = "https://api.groq.com/openai/v1"
                st.text_input("Base URL", groq_url, disabled=True, key="config_groq_url")
                cur_groq_key = st.text_input("API Key", defaults.get("groq_key", ""), type="password", key="config_groq_key")
                cur_groq_model = st.selectbox("æ¨¡å‹", ["llama3-8b-8192", "llama3-70b-8192", "mixtral-8x7b-32768"], key="config_groq_model_sel")
                if st.button("ğŸ’¾ ä¿å­˜ Groq é…ç½®", type="primary", use_container_width=True, key="save_groq"):
                    _save_and_apply_config({"groq_key": cur_groq_key, "config_groq_model": cur_groq_model, "llm_url": groq_url}, "Groq", cur_groq_model, cur_groq_key, groq_url, defaults)
                
                llm_url, llm_model, llm_key = groq_url, cur_groq_model, cur_groq_key

    # 3. åº•éƒ¨é€šç”¨è®¾ç½® (å·²ç§»è‡³å¯¹è¯ç•Œé¢ï¼Œæ­¤å¤„ä»…ä¿ç•™å‚æ•°å ä½)
    extra_params['chat_history_limit'] = defaults.get("chat_history_limit", 10)
    extra_params['system_prompt'] = defaults.get("system_prompt", "")

    return llm_provider, llm_url, llm_model, llm_key, extra_params


def _render_remote_model_selector(url: str, key: str, saved_model: str, prefix: str) -> str:
    """è¾…åŠ©å‡½æ•°ï¼šæ¸²æŸ“è¿œç¨‹æ¨¡å‹é€‰æ‹©å™¨ (v2.9.5 è‡ªåŠ¨åŠ è½½ä¼˜åŒ–)"""
    from src.utils.model_utils import fetch_remote_models
    
    cache_key = f"models_{prefix}_{url}_{key}"
    available_models = st.session_state.get(cache_key, [])
    
    # --- æ ¸å¿ƒæ”¹è¿›ï¼šè‡ªåŠ¨åŠ è½½é€»è¾‘ (v2.9.5) ---
    auto_load_flag = f"auto_load_{prefix}_{hash(url + key)}"
    
    if url and not available_models and auto_load_flag not in st.session_state:
        can_try = True
        if prefix in ["openai", "other"] and not key:
            can_try = False
            
        if can_try:
            with st.spinner("ğŸ”„"):
                models, err = fetch_remote_models(url, key)
                if models:
                    available_models = models
                    st.session_state[cache_key] = models
                    st.session_state[auto_load_flag] = True
                else:
                    st.session_state[auto_load_flag] = False

    # æ¨¡å‹é€‰æ‹©
    if available_models:
        if saved_model and saved_model not in available_models:
            available_models.insert(0, saved_model)
        
        return st.selectbox(
            "é€‰æ‹©æ¨¡å‹", 
            available_models, 
            index=available_models.index(saved_model) if saved_model in available_models else 0,
            key=f"config_{prefix}_model_select",
            label_visibility="collapsed"
        )
    else:
        return st.text_input("æ¨¡å‹åç§°", saved_model, placeholder="ä¾‹å¦‚: gpt-3.5-turbo", key=f"config_{prefix}_model_input", label_visibility="collapsed")


def _save_and_apply_config(config_data: dict, provider: str, model: str, key: str, url: str, defaults: dict, only_chat_settings: bool = False, **kwargs):
    """è¾…åŠ©å‡½æ•°ï¼šä¿å­˜å¹¶åº”ç”¨é…ç½®"""
    existing_config = load_config("rag_config")
    existing_config.update(config_data)
    
    # ç¡®ä¿ system_prompt è¢«åŒ…å«
    if "system_prompt" not in config_data:
        # å¦‚æœå½“å‰ä¿å­˜çš„ä¸æ˜¯èŠå¤©è®¾ç½®ï¼Œæˆ‘ä»¬éœ€è¦ä»ç°æœ‰é…ç½®æˆ– defaults ä¸­è·å– system_promptï¼Œä»¥é˜²é‡ç½®ä¸ºç©º
        system_prompt = existing_config.get("system_prompt") or defaults.get("system_prompt", "")
    else:
        system_prompt = config_data["system_prompt"]

    # åŒæ ·å¤„ç† chat_history_limit (è™½ç„¶å®ƒä¸ç›´æ¥å½±å“ set_global_llm_modelï¼Œä½†ä¸ºäº†å®Œæ•´æ€§)
    
    if save_config(existing_config, "rag_config"):
        # å¦‚æœåªæ˜¯ä¿å­˜å¯¹è¯è®¾ç½®ï¼Œæˆ‘ä»¬å¯èƒ½ä¸éœ€è¦é‡æ–°åˆå§‹åŒ–æ•´ä¸ª LLMï¼Œ
        # ä½†ä¸ºäº†è®© System Prompt ç”Ÿæ•ˆï¼Œé€šå¸¸éœ€è¦é‡æ–°åˆå§‹åŒ– LLM (LlamaIndex çš„ LLM å¯¹è±¡é€šå¸¸æ˜¯ä¸å¯å˜çš„é…ç½®)
        
        # å¦‚æœæ˜¯ä» provider æŒ‰é’®è°ƒç”¨çš„ï¼Œå‚æ•°é½å…¨ã€‚
        # å¦‚æœæ˜¯ä» chat settings è°ƒç”¨çš„ï¼Œæˆ‘ä»¬éœ€è¦ä» defaults è¡¥å…¨å‚æ•°
        if only_chat_settings:
            # å°è¯•ä» defaults è·å–å½“å‰æ´»åŠ¨çš„ LLM é…ç½®
            # æ³¨æ„ï¼šè¿™é‡Œçš„ provider å‚æ•°å¯èƒ½åªæ˜¯ selected_keyï¼Œä¸ä¸€å®šæ˜¯å½“å‰å…¨å±€ç”Ÿæ•ˆçš„ provider
            # è¿™æ˜¯ä¸€ä¸ªæ½œåœ¨é—®é¢˜ï¼šç”¨æˆ·åœ¨å·¦ä¾§é€‰äº† Ollamaï¼Œæ”¹äº† System Promptï¼Œç‚¹å‡»ä¿å­˜ï¼Œä½†å½“å‰å…¨å±€ç”Ÿæ•ˆçš„å¯èƒ½æ˜¯ OpenAIã€‚
            # ä½†é€šå¸¸ç”¨æˆ·æ”¹é…ç½®æ—¶ï¼Œæ„å›¾æ˜¯è®©å½“å‰è§†å›¾çš„é…ç½®ç”Ÿæ•ˆã€‚
            # å®é™…ä¸Šï¼Œ`set_global_llm_model` ä¼šåˆ‡æ¢å…¨å±€ LLMã€‚
            # æ‰€ä»¥ï¼Œä¿å­˜ Chat Settings åŒæ—¶ä¹Ÿæ„å‘³ç€åº”ç”¨äº†å½“å‰å·¦ä¾§é¢æ¿é€‰ä¸­çš„ Provider é…ç½®ã€‚
            # ä¸ºäº†é¿å…æ··æ·†ï¼Œæˆ‘ä»¬å¯ä»¥åªæ›´æ–° System Prompt è€Œä¸åˆ‡æ¢ Providerï¼Ÿ
            # ä¸ï¼ŒSystem Prompt æ˜¯ LLM çš„å±æ€§ã€‚
            # ç®€å•ç­–ç•¥ï¼šåº”ç”¨å½“å‰é¢æ¿çš„æ‰€æœ‰é…ç½®ã€‚
            
            # è¡¥å…¨ç¼ºå¤±å‚æ•° (model, key, url)
            # æ³¨æ„ï¼šconfig_data é‡Œåªæœ‰ chat settingsï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä» defaults è·å– provider settings
            # ä½†æ˜¯ defaults æ˜¯æ—§çš„ã€‚æˆ‘ä»¬éœ€è¦ä» UI æ§ä»¶è·å–ï¼Ÿ
            # render_llm_config é‡Œçš„ llm_model ç­‰å˜é‡æ˜¯å½“å‰æ¸²æŸ“çš„å€¼ã€‚
            # æˆ‘ä»¬åœ¨è°ƒç”¨ _save_and_apply_config æ—¶å·²ç»ä¼ å…¥äº†è¿™äº›å€¼ (model, key, url)ã€‚
            pass
            
        if set_global_llm_model(provider, model, key, url, system_prompt=system_prompt, **kwargs):
            st.success(f"âœ… é…ç½®å·²æ›´æ–°å¹¶ç”Ÿæ•ˆ (System Prompt: {'å·²è®¾ç½®' if system_prompt else 'æœªè®¾ç½®'})")
            if not only_chat_settings:
                st.session_state.selected_model = model
        else:
            st.warning("âš ï¸ é…ç½®å·²ä¿å­˜ï¼Œä½†çƒ­æ›´æ–°å¤±è´¥")
            
        defaults.update(config_data)
        if only_chat_settings:
             st.rerun()
    else:
        st.error("âŒ ä¿å­˜å¤±è´¥")


def render_embedding_config(defaults: dict) -> Tuple[str, str, str, str]:
    """
    æ¸²æŸ“ Embedding é…ç½®è¡¨å• (ä¼˜åŒ–ç‰ˆ)
    """
    with st.container(border=True):
        st.markdown("##### ğŸ§¬ å‘é‡æ¨¡å‹ (Embedding)")
        
        embed_idx = defaults.get("embed_provider_idx", 0)
        if embed_idx > 2: embed_idx = 0
        
        col1, col2 = st.columns([1, 1.5])
        
        with col1:
            embed_provider = st.selectbox(
                "ä¾›åº”å•†",
                ["HuggingFace (æœ¬åœ°/æé€Ÿ)", "OpenAI-Compatible", "Ollama"],
                index=embed_idx,
                key="config_embed_provider",
                label_visibility="collapsed"
            )
        
        with col2:
            if embed_provider.startswith("HuggingFace"):
                saved_model = defaults.get("embed_model_hf", "sentence-transformers/all-MiniLM-L6-v2")
                embed_model = render_hf_embedding_selector(saved_model)
                embed_url = ""
                embed_key = ""
                
                # å¤„ç†é»˜è®¤ä¿å­˜
                if st.session_state.get('save_embed_model'):
                    from src.config import ConfigLoader
                    import time
                    config = ConfigLoader.load()
                    config["embed_model_hf"] = st.session_state.save_embed_model
                    ConfigLoader.save(config)
                    st.toast(f"âœ… é»˜è®¤åµŒå…¥æ¨¡å‹å·²æ›´æ–°")
                    del st.session_state.save_embed_model
                    time.sleep(1)
                    st.rerun()
            elif embed_provider.startswith("OpenAI"):
                c1, c2 = st.columns(2)
                with c1: embed_model = st.text_input("æ¨¡å‹å", defaults.get("embed_model_openai", "text-embedding-3-small"), key="embed_openai_model")
                with c2: embed_url = st.text_input("Base URL", defaults.get("embed_url_openai", "https://api.openai.com/v1"), key="embed_openai_url")
                embed_key = st.text_input("API Key", defaults.get("embed_key", ""), type="password", key="embed_openai_key")
            else:  # Ollama
                c1, c2 = st.columns(2)
                with c1: embed_model = st.text_input("æ¨¡å‹å", defaults.get("embed_model_ollama", "nomic-embed-text"), key="embed_ollama_model")
                with c2: embed_url = st.text_input("URL", defaults.get("embed_url_ollama", "http://localhost:11434"), key="embed_ollama_url")
                embed_key = ""
                
    return embed_provider, embed_model, embed_url, embed_key


def render_basic_config(defaults: dict) -> dict:
    """
    æ¸²æŸ“å®Œæ•´çš„åŸºç¡€é…ç½®åŒºåŸŸ
    
    Args:
        defaults: é»˜è®¤é…ç½®å­—å…¸
        
    Returns:
        dict: é…ç½®å­—å…¸
    """
    # LLM é…ç½® (æ¥æ”¶ 5 ä¸ªè¿”å›å€¼)
    llm_provider, llm_url, llm_model, llm_key, extra_params = render_llm_config(defaults)
    
    # Embedding é…ç½®
    embed_provider, embed_model, embed_url, embed_key = render_embedding_config(defaults)
    
    # åˆå¹¶ extra_params åˆ°è¿”å›ç»“æœ
    result = {
        'llm_provider': llm_provider,
        'llm_url': llm_url,
        'llm_model': llm_model,
        'llm_key': llm_key,
        'embed_provider': embed_provider,
        'embed_model': embed_model,
        'embed_url': embed_url,
        'embed_key': embed_key
    }
    result.update(extra_params)
    return result
