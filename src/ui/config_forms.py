"""
é…ç½®è¡¨å•ç»„ä»¶
Stage 3.2.2 - ä¸­é£é™©é‡æ„
æå–è‡ª apppro.py
ä½¿ç”¨ç»Ÿä¸€é…ç½®ç»„ä»¶
"""

import os
import streamlit as st
from typing import Tuple, Dict
from .model_selectors import (
    render_ollama_model_selector,
    render_openai_model_selector,
    render_hf_embedding_selector
)
from src.utils.model_utils import fetch_remote_models
from src.services.unified_config_service import save_config, load_config
from src.utils.model_manager import set_global_llm_model


def render_llm_config(defaults: dict) -> Tuple[str, str, str, str, dict]:
    """
    æ¸²æŸ“ LLM é…ç½®è¡¨å• (ä¼˜åŒ–ç‰ˆ - ä»¿ ChatOllama å¸ƒå±€)
    """
    st.markdown("#### ğŸ§  æ¨¡å‹æœåŠ¡é…ç½®")
    
    # å®šä¹‰ä¾›åº”å•†åˆ—è¡¨
    PROVIDERS = {
        "Ollama": "ğŸ¦™ Ollama (æœ¬åœ°)",
        "OpenAI": "â˜ï¸ OpenAI (äº‘ç«¯)",
        "OpenAI-Compatible": "ğŸ”Œ Other (å…¼å®¹åè®®)",
        "Azure OpenAI": "ğŸŸ¦ Azure OpenAI",
        "Anthropic": "ğŸ§  Anthropic (Claude)",
        "Moonshot": "ğŸŒ™ Moonshot (Kimi)",
        "Gemini": "ğŸ’ Gemini (Google)",
        "Groq": "âš¡ Groq (æé€Ÿ)"
    }
    
    # å¸ƒå±€: å·¦ä¾§å¯¼èˆªï¼Œå³ä¾§è¯¦æƒ…
    col_nav, col_form = st.columns([1, 3])
    
    # --- å·¦ä¾§å¯¼èˆªæ  ---
    with col_nav:
        st.markdown("##### æœåŠ¡å•†")
        
        # å°è¯•æ¢å¤ä¸Šæ¬¡çš„é€‰æ‹© (å°† label è½¬æ¢ä¸º key)
        saved_label = defaults.get("llm_provider_label", "Ollama (æœ¬åœ°)")
        default_key = "Ollama"
        for k, v in PROVIDERS.items():
            if v == saved_label:
                default_key = k
                break
        
        # èƒ½å¤Ÿä¿æŒçŠ¶æ€çš„é€‰æ‹©å™¨
        selected_key = st.radio(
            "é€‰æ‹©æœåŠ¡å•†",
            options=list(PROVIDERS.keys()),
            format_func=lambda x: PROVIDERS[x],
            index=list(PROVIDERS.keys()).index(default_key) if default_key in PROVIDERS else 0,
            key="llm_provider_nav",
            label_visibility="collapsed"
        )
        st.caption("é€‰æ‹© AI æœåŠ¡æä¾›å•†é…ç½®è¿æ¥ä¸æ¨¡å‹")

    # --- å³ä¾§é…ç½®è¡¨å• ---
    llm_provider = selected_key
    llm_url = ""
    llm_model = ""
    llm_key = ""
    extra_params = {}
    
    with col_form:
        st.markdown(f"#### {PROVIDERS[selected_key]} è®¾ç½®")
        
        # 1. Ollama
        if selected_key == "Ollama":
            col_url, col_status = st.columns([3, 1])
            with col_url:
                llm_url = st.text_input("Ollama URL", defaults.get("llm_url_ollama") or "http://localhost:11434", key="config_ollama_url")
            
            from src.utils.model_utils import check_ollama_status
            ollama_ok = check_ollama_status(llm_url)
            
            with col_status:
                st.write("")
                if ollama_ok:
                    st.caption("âœ… å·²è¿æ¥")
                else:
                    st.caption("âš ï¸ æœªè¿è¡Œ")
            
            saved_model = defaults.get("llm_model_ollama", "gpt-oss:20b")
            llm_model, _ = render_ollama_model_selector(llm_url, saved_model, ollama_ok)
            
            # æŒ‰é’®åŒºåŸŸ
            if st.button("ğŸ’¾ ä¿å­˜ Ollama é…ç½®", key="save_ollama_config", type="primary"):
                config_data = {
                    "llm_provider": "Ollama",
                    "llm_url_ollama": llm_url,
                    "llm_model_ollama": llm_model,
                    "llm_provider_label": PROVIDERS["Ollama"]
                }
                _save_and_apply_config(config_data, "Ollama", llm_model, "", llm_url, defaults)

        # 2. OpenAI
        elif selected_key == "OpenAI":
            col1, col2 = st.columns([2, 1])
            with col1:
                llm_url = st.text_input("Base URL", defaults.get("llm_url_openai", "https://api.openai.com/v1"), key="config_openai_url")
            with col2:
                llm_key = st.text_input("API Key", defaults.get("llm_key", ""), type="password", key="config_openai_key")
            
            # æ¨¡å‹é€‰æ‹©é€»è¾‘
            saved_model = defaults.get("llm_model_openai", "gpt-3.5-turbo")
            llm_model = _render_remote_model_selector(llm_url, llm_key, saved_model, "openai")
            
            if st.button("ğŸ’¾ ä¿å­˜ OpenAI é…ç½®", key="save_openai_config", type="primary"):
                config_data = {
                    "llm_provider": "OpenAI",
                    "llm_url_openai": llm_url,
                    "llm_key": llm_key,
                    "llm_model_openai": llm_model,
                    "llm_provider_label": PROVIDERS["OpenAI"]
                }
                _save_and_apply_config(config_data, "OpenAI", llm_model, llm_key, llm_url, defaults)

        # 3. OpenAI-Compatible (Other)
        elif selected_key == "OpenAI-Compatible":
            st.caption("ğŸ’¡ é€‚ç”¨äº DeepSeek, Yi, ChatGLM, vLLM ç­‰å…¼å®¹ OpenAI åè®®çš„æœåŠ¡")
            col1, col2 = st.columns([2, 1])
            with col1:
                def_url = defaults.get("llm_url_other") or defaults.get("llm_url") or "https://api.deepseek.com/v1"
                llm_url = st.text_input("Base URL", def_url, key="config_other_url")
            with col2:
                def_key = defaults.get("llm_key_other") or defaults.get("llm_key", "")
                llm_key = st.text_input("API Key", def_key, type="password", key="config_other_key")
            
            saved_model = defaults.get("llm_model_other", "")
            llm_model = _render_remote_model_selector(llm_url, llm_key, saved_model, "other")
            
            if st.button("ğŸ’¾ ä¿å­˜è‡ªå®šä¹‰é…ç½®", key="save_other_config", type="primary"):
                config_data = {
                    "llm_provider": "OpenAI-Compatible",
                    "llm_url_other": llm_url,
                    "llm_key_other": llm_key,
                    "llm_model_other": llm_model,
                    "llm_provider_label": PROVIDERS["OpenAI-Compatible"],
                    # å…¼å®¹å­—æ®µ
                    "llm_url": llm_url,
                    "llm_key": llm_key,
                    "llm_model": llm_model
                }
                _save_and_apply_config(config_data, "OpenAI-Compatible", llm_model, llm_key, llm_url, defaults)

        # 4. Azure OpenAI
        elif selected_key == "Azure OpenAI":
            llm_url = st.text_input("Azure Endpoint", defaults.get("azure_endpoint", ""), placeholder="https://{resource}.openai.azure.com/", key="config_azure_endpoint")
            llm_key = st.text_input("API Key", defaults.get("azure_key", ""), type="password", key="config_azure_key")
            llm_model = st.text_input("Deployment Name", defaults.get("azure_deployment", ""), help="åœ¨Azureæ§åˆ¶å°ä¸­éƒ¨ç½²çš„æ¨¡å‹åç§°", key="config_azure_deployment")
            api_version = st.text_input("API Version", defaults.get("azure_api_version", "2023-05-15"), help="ä¾‹å¦‚: 2023-05-15", key="config_azure_api_version")
            extra_params = {"api_version": api_version}
            
            if st.button("ğŸ’¾ ä¿å­˜ Azure é…ç½®", key="save_azure_config", type="primary"):
                config_data = {
                    "llm_provider": "Azure OpenAI",
                    "azure_endpoint": llm_url,
                    "azure_key": llm_key,
                    "azure_deployment": llm_model,
                    "azure_api_version": api_version,
                    "llm_provider_label": PROVIDERS["Azure OpenAI"],
                    "llm_url": llm_url,
                    "llm_key": llm_key,
                    "llm_model": llm_model
                }
                _save_and_apply_config(config_data, "Azure OpenAI", llm_model, llm_key, llm_url, defaults, api_version=api_version)

        # 5. Anthropic
        elif selected_key == "Anthropic":
            llm_key = st.text_input("API Key", defaults.get("anthropic_key", ""), type="password", key="config_anthropic_key")
            llm_model = st.selectbox("æ¨¡å‹", ["claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"], index=0, key="config_anthropic_model")
            
            if st.button("ğŸ’¾ ä¿å­˜ Anthropic é…ç½®", key="save_anthropic_config", type="primary"):
                config_data = {
                    "llm_provider": "Anthropic",
                    "anthropic_key": llm_key,
                    "config_anthropic_model": llm_model,
                    "llm_provider_label": PROVIDERS["Anthropic"],
                    "llm_key": llm_key,
                    "llm_model": llm_model
                }
                _save_and_apply_config(config_data, "Anthropic", llm_model, llm_key, "", defaults)

        # 6. Moonshot
        elif selected_key == "Moonshot":
            llm_url = "https://api.moonshot.cn/v1"
            st.text_input("Base URL", llm_url, disabled=True, key="config_moonshot_url")
            llm_key = st.text_input("API Key", defaults.get("moonshot_key", ""), type="password", key="config_moonshot_key")
            llm_model = st.selectbox("æ¨¡å‹", ["moonshot-v1-8k", "moonshot-v1-32k", "moonshot-v1-128k"], index=0, key="config_moonshot_model")
            
            if st.button("ğŸ’¾ ä¿å­˜ Moonshot é…ç½®", key="save_moonshot_config", type="primary"):
                config_data = {
                    "llm_provider": "Moonshot",
                    "moonshot_key": llm_key,
                    "config_moonshot_model": llm_model,
                    "llm_provider_label": PROVIDERS["Moonshot"],
                    "llm_key": llm_key,
                    "llm_model": llm_model,
                    "llm_url": llm_url
                }
                _save_and_apply_config(config_data, "Moonshot", llm_model, llm_key, llm_url, defaults)
        
        # 7. Gemini
        elif selected_key == "Gemini":
            llm_key = st.text_input("API Key", defaults.get("gemini_key", ""), type="password", key="config_gemini_key")
            llm_model = st.selectbox("æ¨¡å‹", ["gemini-pro", "gemini-pro-vision"], index=0, key="config_gemini_model")
            
            if st.button("ğŸ’¾ ä¿å­˜ Gemini é…ç½®", key="save_gemini_config", type="primary"):
                config_data = {
                    "llm_provider": "Gemini",
                    "gemini_key": llm_key,
                    "config_gemini_model": llm_model,
                    "llm_provider_label": PROVIDERS["Gemini"],
                    "llm_key": llm_key,
                    "llm_model": llm_model
                }
                _save_and_apply_config(config_data, "Gemini", llm_model, llm_key, "", defaults)
        
        # 8. Groq
        elif selected_key == "Groq":
            llm_url = "https://api.groq.com/openai/v1"
            st.text_input("Base URL", llm_url, disabled=True, key="config_groq_url")
            llm_key = st.text_input("API Key", defaults.get("groq_key", ""), type="password", key="config_groq_key")
            llm_model = st.selectbox("æ¨¡å‹", ["llama3-8b-8192", "llama3-70b-8192", "mixtral-8x7b-32768"], index=0, key="config_groq_model")

            if st.button("ğŸ’¾ ä¿å­˜ Groq é…ç½®", key="save_groq_config", type="primary"):
                config_data = {
                    "llm_provider": "Groq",
                    "groq_key": llm_key,
                    "config_groq_model": llm_model,
                    "llm_provider_label": PROVIDERS["Groq"],
                    "llm_key": llm_key,
                    "llm_model": llm_model,
                    "llm_url": llm_url
                }
                _save_and_apply_config(config_data, "Groq", llm_model, llm_key, llm_url, defaults)

        # --- é€šç”¨å¯¹è¯è®¾ç½® (ä»¿ Screenshot) ---
        st.divider()
        st.markdown("##### ğŸ’¬ å¯¹è¯è®¾ç½®")
        
        # 1. é™„å¸¦æ¶ˆæ¯æ¡æ•° (Context Window)
        current_limit = defaults.get("chat_history_limit", 10)
        history_limit = st.slider(
            "é™„å¸¦å†å²æ¶ˆæ¯æ•° (Context Window)", 
            min_value=1, 
            max_value=50, 
            value=current_limit,
            help="æ¯æ¬¡å¯¹è¯å‘é€ç»™æ¨¡å‹çš„å†å²æ¶ˆæ¯æ•°é‡ (+1 è¡¨ç¤ºåŠ ä¸Šå½“å‰é—®é¢˜)"
        )

        # ä¿å­˜é€»è¾‘ (ä»…é’ˆå¯¹ Context Window)
        has_changes = (history_limit != current_limit)
        
        if has_changes:
            if st.button("ğŸ’¾ ä¿å­˜å¯¹è¯è®¾ç½®", key="save_chat_settings", type="primary"):
                config_data = {
                    "chat_history_limit": history_limit
                }
                _save_and_apply_config(config_data, selected_key, llm_model, llm_key, llm_url, defaults, only_chat_settings=True)

        extra_params['chat_history_limit'] = history_limit
        # å…¼å®¹æ€§ä¿ç•™
        extra_params['system_prompt'] = defaults.get("system_prompt", "")

    return llm_provider, llm_url, llm_model, llm_key, extra_params


def _render_remote_model_selector(url: str, key: str, saved_model: str, prefix: str) -> str:
    """è¾…åŠ©å‡½æ•°ï¼šæ¸²æŸ“è¿œç¨‹æ¨¡å‹é€‰æ‹©å™¨ (v2.9.5 è‡ªåŠ¨åŠ è½½ä¼˜åŒ–)"""
    from src.utils.model_utils import fetch_remote_models
    
    cache_key = f"models_{prefix}_{url}_{key}"
    available_models = st.session_state.get(cache_key, [])
    
    # --- æ ¸å¿ƒæ”¹è¿›ï¼šè‡ªåŠ¨åŠ è½½é€»è¾‘ (v2.9.5) ---
    # å¦‚æœæœ‰ URL (ä¸”éæœ¬åœ° prefix æ—¶æœ‰ Key)ï¼Œä¸”ç¼“å­˜ä¸ºç©ºï¼Œåˆ™å°è¯•è‡ªåŠ¨åŠ è½½ä¸€æ¬¡
    # ä¸ºäº†é¿å…æ— é™é‡è¯•ï¼Œæˆ‘ä»¬è®°å½•ä¸€ä¸ªè‡ªåŠ¨åŠ è½½å°è¯•æ ‡è®°
    auto_load_flag = f"auto_load_{prefix}_{hash(url + key)}"
    
    if url and not available_models and auto_load_flag not in st.session_state:
        # åªæœ‰ OpenAI ç±»çš„éœ€è¦ Keyï¼Œå…¶å®ƒçš„ï¼ˆå¦‚ Ollama åœ¨å…¶å®ƒåœ°æ–¹å¤„ç†ï¼‰è§†æƒ…å†µè€Œå®š
        # è¿™é‡Œç»Ÿä¸€é€»è¾‘ï¼šæœ‰ URL ä¸”ç¼“å­˜ç©ºï¼Œå°è¯•æ‹‰å–
        can_try = True
        if prefix in ["openai", "other"] and not key:
            can_try = False
            
        if can_try:
            with st.spinner("ğŸ”„ è‡ªåŠ¨åŒæ­¥æ¨¡å‹åˆ—è¡¨..."):
                models, err = fetch_remote_models(url, key)
                if models:
                    available_models = models
                    st.session_state[cache_key] = models
                    # æ ‡è®°å·²å°è¯•è¿‡ï¼Œé¿å…å¤±è´¥æ—¶åå¤è§¦å‘
                    st.session_state[auto_load_flag] = True
                else:
                    # å³ä½¿å¤±è´¥ä¹Ÿæ ‡è®°ï¼Œé˜²æ­¢é˜»å¡ UI
                    st.session_state[auto_load_flag] = False

    # åˆ·æ–°æŒ‰é’® (ä¿ç•™æ‰‹åŠ¨åˆ·æ–°)
    col_select, col_refresh = st.columns([4, 1])
    
    with col_refresh:
        if st.button("ğŸ”„", key=f"refresh_{prefix}", help="åˆ·æ–°æ¨¡å‹åˆ—è¡¨"):
            with st.spinner("ğŸ”„"):
                models, err = fetch_remote_models(url, key)
                if models:
                    available_models = models
                    st.session_state[cache_key] = models
                    st.toast(f"âœ… å·²åŠ è½½ {len(models)} ä¸ªæ¨¡å‹")
                    st.rerun()
                else:
                    st.warning(f"åŠ è½½å¤±è´¥: {err}")

    with col_select:
        if available_models:
            if saved_model and saved_model not in available_models:
                available_models.insert(0, saved_model)
            
            return st.selectbox(
                "é€‰æ‹©æ¨¡å‹", 
                available_models, 
                index=available_models.index(saved_model) if saved_model in available_models else 0,
                key=f"config_{prefix}_model_select",
                label_visibility="collapsed"
            )
        else:
            return st.text_input("æ¨¡å‹åç§°", saved_model, placeholder="ä¾‹å¦‚: gpt-3.5-turbo", key=f"config_{prefix}_model_input", label_visibility="collapsed")


def _save_and_apply_config(config_data: dict, provider: str, model: str, key: str, url: str, defaults: dict, only_chat_settings: bool = False, **kwargs):
    """è¾…åŠ©å‡½æ•°ï¼šä¿å­˜å¹¶åº”ç”¨é…ç½®"""
    existing_config = load_config("rag_config")
    existing_config.update(config_data)
    
    # ç¡®ä¿ system_prompt è¢«åŒ…å«
    if "system_prompt" not in config_data:
        # å¦‚æœå½“å‰ä¿å­˜çš„ä¸æ˜¯èŠå¤©è®¾ç½®ï¼Œæˆ‘ä»¬éœ€è¦ä»ç°æœ‰é…ç½®æˆ– defaults ä¸­è·å– system_promptï¼Œä»¥é˜²é‡ç½®ä¸ºç©º
        system_prompt = existing_config.get("system_prompt") or defaults.get("system_prompt", "")
    else:
        system_prompt = config_data["system_prompt"]

    # åŒæ ·å¤„ç† chat_history_limit (è™½ç„¶å®ƒä¸ç›´æ¥å½±å“ set_global_llm_modelï¼Œä½†ä¸ºäº†å®Œæ•´æ€§)
    
    if save_config(existing_config, "rag_config"):
        # å¦‚æœåªæ˜¯ä¿å­˜å¯¹è¯è®¾ç½®ï¼Œæˆ‘ä»¬å¯èƒ½ä¸éœ€è¦é‡æ–°åˆå§‹åŒ–æ•´ä¸ª LLMï¼Œ
        # ä½†ä¸ºäº†è®© System Prompt ç”Ÿæ•ˆï¼Œé€šå¸¸éœ€è¦é‡æ–°åˆå§‹åŒ– LLM (LlamaIndex çš„ LLM å¯¹è±¡é€šå¸¸æ˜¯ä¸å¯å˜çš„é…ç½®)
        
        # å¦‚æœæ˜¯ä» provider æŒ‰é’®è°ƒç”¨çš„ï¼Œå‚æ•°é½å…¨ã€‚
        # å¦‚æœæ˜¯ä» chat settings è°ƒç”¨çš„ï¼Œæˆ‘ä»¬éœ€è¦ä» defaults è¡¥å…¨å‚æ•°
        if only_chat_settings:
            # å°è¯•ä» defaults è·å–å½“å‰æ´»åŠ¨çš„ LLM é…ç½®
            # æ³¨æ„ï¼šè¿™é‡Œçš„ provider å‚æ•°å¯èƒ½åªæ˜¯ selected_keyï¼Œä¸ä¸€å®šæ˜¯å½“å‰å…¨å±€ç”Ÿæ•ˆçš„ provider
            # è¿™æ˜¯ä¸€ä¸ªæ½œåœ¨é—®é¢˜ï¼šç”¨æˆ·åœ¨å·¦ä¾§é€‰äº† Ollamaï¼Œæ”¹äº† System Promptï¼Œç‚¹å‡»ä¿å­˜ï¼Œä½†å½“å‰å…¨å±€ç”Ÿæ•ˆçš„å¯èƒ½æ˜¯ OpenAIã€‚
            # ä½†é€šå¸¸ç”¨æˆ·æ”¹é…ç½®æ—¶ï¼Œæ„å›¾æ˜¯è®©å½“å‰è§†å›¾çš„é…ç½®ç”Ÿæ•ˆã€‚
            # å®é™…ä¸Šï¼Œ`set_global_llm_model` ä¼šåˆ‡æ¢å…¨å±€ LLMã€‚
            # æ‰€ä»¥ï¼Œä¿å­˜ Chat Settings åŒæ—¶ä¹Ÿæ„å‘³ç€åº”ç”¨äº†å½“å‰å·¦ä¾§é¢æ¿é€‰ä¸­çš„ Provider é…ç½®ã€‚
            # ä¸ºäº†é¿å…æ··æ·†ï¼Œæˆ‘ä»¬å¯ä»¥åªæ›´æ–° System Prompt è€Œä¸åˆ‡æ¢ Providerï¼Ÿ
            # ä¸ï¼ŒSystem Prompt æ˜¯ LLM çš„å±æ€§ã€‚
            # ç®€å•ç­–ç•¥ï¼šåº”ç”¨å½“å‰é¢æ¿çš„æ‰€æœ‰é…ç½®ã€‚
            
            # è¡¥å…¨ç¼ºå¤±å‚æ•° (model, key, url)
            # æ³¨æ„ï¼šconfig_data é‡Œåªæœ‰ chat settingsï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä» defaults è·å– provider settings
            # ä½†æ˜¯ defaults æ˜¯æ—§çš„ã€‚æˆ‘ä»¬éœ€è¦ä» UI æ§ä»¶è·å–ï¼Ÿ
            # render_llm_config é‡Œçš„ llm_model ç­‰å˜é‡æ˜¯å½“å‰æ¸²æŸ“çš„å€¼ã€‚
            # æˆ‘ä»¬åœ¨è°ƒç”¨ _save_and_apply_config æ—¶å·²ç»ä¼ å…¥äº†è¿™äº›å€¼ (model, key, url)ã€‚
            pass
            
        if set_global_llm_model(provider, model, key, url, system_prompt=system_prompt, **kwargs):
            st.success(f"âœ… é…ç½®å·²æ›´æ–°å¹¶ç”Ÿæ•ˆ (System Prompt: {'å·²è®¾ç½®' if system_prompt else 'æœªè®¾ç½®'})")
            if not only_chat_settings:
                st.session_state.selected_model = model
        else:
            st.warning("âš ï¸ é…ç½®å·²ä¿å­˜ï¼Œä½†çƒ­æ›´æ–°å¤±è´¥")
            
        defaults.update(config_data)
        if only_chat_settings:
             st.rerun()
    else:
        st.error("âŒ ä¿å­˜å¤±è´¥")


def render_embedding_config(defaults: dict) -> Tuple[str, str, str, str]:
    """
    æ¸²æŸ“ Embedding é…ç½®è¡¨å• (ä¼˜åŒ–ç‰ˆ)
    """
    with st.container(border=True):
        st.markdown("##### ğŸ§¬ å‘é‡æ¨¡å‹ (Embedding)")
        
        embed_idx = defaults.get("embed_provider_idx", 0)
        if embed_idx > 2: embed_idx = 0
        
        col1, col2 = st.columns([1, 1.5])
        
        with col1:
            embed_provider = st.selectbox(
                "ä¾›åº”å•†",
                ["HuggingFace (æœ¬åœ°/æé€Ÿ)", "OpenAI-Compatible", "Ollama"],
                index=embed_idx,
                key="config_embed_provider",
                label_visibility="collapsed"
            )
        
        with col2:
            if embed_provider.startswith("HuggingFace"):
                saved_model = defaults.get("embed_model_hf", "sentence-transformers/all-MiniLM-L6-v2")
                embed_model = render_hf_embedding_selector(saved_model)
                embed_url = ""
                embed_key = ""
                
                # å¤„ç†é»˜è®¤ä¿å­˜
                if st.session_state.get('save_embed_model'):
                    from src.config import ConfigLoader
                    import time
                    config = ConfigLoader.load()
                    config["embed_model_hf"] = st.session_state.save_embed_model
                    ConfigLoader.save(config)
                    st.toast(f"âœ… é»˜è®¤åµŒå…¥æ¨¡å‹å·²æ›´æ–°")
                    del st.session_state.save_embed_model
                    time.sleep(1)
                    st.rerun()
            elif embed_provider.startswith("OpenAI"):
                embed_model = st.text_input("æ¨¡å‹å", defaults.get("embed_model_openai", "text-embedding-3-small"))
                embed_url = st.text_input("Base URL", defaults.get("embed_url_openai", "https://api.openai.com/v1"))
                embed_key = st.text_input("API Key", defaults.get("embed_key", ""), type="password")
            else:  # Ollama
                embed_model = st.text_input("æ¨¡å‹å", defaults.get("embed_model_ollama", "nomic-embed-text"))
                embed_url = st.text_input("URL", defaults.get("embed_url_ollama", "http://localhost:11434"))
                embed_key = ""
                
    return embed_provider, embed_model, embed_url, embed_key


def render_basic_config(defaults: dict) -> dict:
    """
    æ¸²æŸ“å®Œæ•´çš„åŸºç¡€é…ç½®åŒºåŸŸ
    
    Args:
        defaults: é»˜è®¤é…ç½®å­—å…¸
        
    Returns:
        dict: é…ç½®å­—å…¸
    """
    # LLM é…ç½® (æ¥æ”¶ 5 ä¸ªè¿”å›å€¼)
    llm_provider, llm_url, llm_model, llm_key, extra_params = render_llm_config(defaults)
    
    # Embedding é…ç½®
    embed_provider, embed_model, embed_url, embed_key = render_embedding_config(defaults)
    
    # åˆå¹¶ extra_params åˆ°è¿”å›ç»“æœ
    result = {
        'llm_provider': llm_provider,
        'llm_url': llm_url,
        'llm_model': llm_model,
        'llm_key': llm_key,
        'embed_provider': embed_provider,
        'embed_model': embed_model,
        'embed_url': embed_url,
        'embed_key': embed_key
    }
    result.update(extra_params)
    return result
