"""
é…ç½®è¡¨å•ç»„ä»¶
Stage 3.2.2 - ä¸­é£é™©é‡æ„
æå–è‡ª apppro.py
"""

import os
import streamlit as st
from typing import Tuple, Dict
from .model_selectors import (
    render_ollama_model_selector,
    render_openai_model_selector,
    render_hf_embedding_selector
)


def render_llm_config(defaults: dict) -> Tuple[str, str, str, str]:
    """
    æ¸²æŸ“ LLM é…ç½®è¡¨å•
    
    Args:
        defaults: é»˜è®¤é…ç½®å­—å…¸
        
    Returns:
        Tuple[str, str, str, str]: (provider, url, model, key)
    """
    st.markdown("**LLM å¯¹è¯æ¨¡å‹**")
    
    llm_provider_choice = st.radio(
        "ä¾›åº”å•†",
        ["Ollama (æœ¬åœ°)", "OpenAI-Compatible (äº‘ç«¯)"],
        horizontal=True,
        label_visibility="collapsed",
        key="config_llm_provider_radio"
    )
    
    if llm_provider_choice.startswith("Ollama"):
        llm_provider = "Ollama"
        # Ollama URL å’ŒçŠ¶æ€åŒè¡Œæ˜¾ç¤º
        col_url, col_status = st.columns([3, 1])
        with col_url:
            llm_url = st.text_input("Ollama URL", defaults.get("llm_url_ollama", "http://localhost:11434"), key="config_ollama_url")
        
        # æ£€æµ‹ Ollama çŠ¶æ€
        from src.utils.model_utils import check_ollama_status
        ollama_ok = check_ollama_status(llm_url)
        
        with col_status:
            st.write("")  # å ä½ï¼Œå¯¹é½è¾“å…¥æ¡†
            if ollama_ok:
                st.success("âœ… Ollama å·²è¿æ¥")
            else:
                st.warning("âš ï¸ Ollama æœªè¿è¡Œ")
        
        # ä½¿ç”¨æ¨¡å‹é€‰æ‹©å™¨ç»„ä»¶
        saved_model = defaults.get("llm_model_ollama", "qwen2.5:7b")
        llm_model, save_as_default = render_ollama_model_selector(llm_url, saved_model, ollama_ok)
        
        # å¤„ç†"è®¾ä¸ºé»˜è®¤"
        if save_as_default:
            from src.config import ConfigLoader
            import time
            config = ConfigLoader.load()
            config["llm_model_ollama"] = llm_model
            ConfigLoader.save(config)
            st.success(f"âœ… å·²è®¾ä¸ºé»˜è®¤: {llm_model}")
            time.sleep(1)
            st.rerun()
        
        llm_key = ""
    else:
        llm_provider = "OpenAI-Compatible"
        llm_url = st.text_input("Base URL", defaults.get("llm_url_openai", "https://api.deepseek.com"))
        
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å– Key
        env_key = os.getenv('OPENAI_API_KEY', "")
        default_key = defaults.get("llm_key", "") or env_key
        
        llm_key = st.text_input(
            "API Key",
            value=default_key,
            type="password",
            help="å¯ä»ç¯å¢ƒå˜é‡ OPENAI_API_KEY è‡ªåŠ¨åŠ è½½"
        )
        
        # ä½¿ç”¨æ¨¡å‹é€‰æ‹©å™¨ç»„ä»¶
        saved_model = defaults.get("llm_model_openai", "deepseek-chat")
        llm_model = render_openai_model_selector(llm_url, llm_key, saved_model)
    
    return llm_provider, llm_url, llm_model, llm_key


def render_embedding_config(defaults: dict) -> Tuple[str, str, str, str]:
    """
    æ¸²æŸ“ Embedding é…ç½®è¡¨å•
    
    Args:
        defaults: é»˜è®¤é…ç½®å­—å…¸
        
    Returns:
        Tuple[str, str, str, str]: (provider, model, url, key)
    """
    st.markdown("---")
    st.markdown("**Embedding å‘é‡æ¨¡å‹**")
    st.caption("ğŸ’¡ ç”¨äºç†è§£æ–‡æ¡£è¯­ä¹‰")
    
    embed_idx = defaults.get("embed_provider_idx", 0)
    if embed_idx > 2:
        embed_idx = 0
    
    embed_provider = st.selectbox(
        "ä¾›åº”å•†",
        ["HuggingFace (æœ¬åœ°/æé€Ÿ)", "OpenAI-Compatible", "Ollama"],
        index=embed_idx,
        key="config_embed_provider"
    )
    
    if embed_provider.startswith("HuggingFace"):
        # ä½¿ç”¨ HF åµŒå…¥æ¨¡å‹é€‰æ‹©å™¨ç»„ä»¶
        saved_model = defaults.get("embed_model_hf", "BAAI/bge-small-zh-v1.5")
        embed_model = render_hf_embedding_selector(saved_model)
        
        # å¤„ç†"è®¾ä¸ºé»˜è®¤"ä¿¡å·
        if st.session_state.get('save_embed_model'):
            from src.config import ConfigLoader
            import time
            config = ConfigLoader.load()
            config["embed_model_hf"] = st.session_state.save_embed_model
            ConfigLoader.save(config)
            st.success(f"âœ… å·²è®¾ä¸ºé»˜è®¤")
            time.sleep(1)
            del st.session_state.save_embed_model
            st.rerun()
        
        embed_url = ""
        embed_key = ""
    elif embed_provider.startswith("OpenAI"):
        embed_model = st.text_input("æ¨¡å‹å", defaults.get("embed_model_openai", "text-embedding-3-small"))
        embed_url = st.text_input("Base URL", defaults.get("embed_url_openai", "https://api.openai.com/v1"))
        embed_key = st.text_input("API Key", defaults.get("embed_key", ""), type="password")
    else:  # Ollama
        embed_model = st.text_input("æ¨¡å‹å", defaults.get("embed_model_ollama", "nomic-embed-text"))
        embed_url = st.text_input("URL", defaults.get("embed_url_ollama", "http://localhost:11434"))
        embed_key = ""
    
    return embed_provider, embed_model, embed_url, embed_key


def render_basic_config(defaults: dict) -> dict:
    """
    æ¸²æŸ“å®Œæ•´çš„åŸºç¡€é…ç½®åŒºåŸŸ
    
    Args:
        defaults: é»˜è®¤é…ç½®å­—å…¸
        
    Returns:
        dict: é…ç½®å­—å…¸ {
            'llm_provider': str,
            'llm_url': str,
            'llm_model': str,
            'llm_key': str,
            'embed_provider': str,
            'embed_model': str,
            'embed_url': str,
            'embed_key': str
        }
    """
    with st.expander("âš™ï¸ åŸºç¡€é…ç½®", expanded=True):
        # LLM é…ç½®
        llm_provider, llm_url, llm_model, llm_key = render_llm_config(defaults)
        
        # Embedding é…ç½®
        embed_provider, embed_model, embed_url, embed_key = render_embedding_config(defaults)
    
    return {
        'llm_provider': llm_provider,
        'llm_url': llm_url,
        'llm_model': llm_model,
        'llm_key': llm_key,
        'embed_provider': embed_provider,
        'embed_model': embed_model,
        'embed_url': embed_url,
        'embed_key': embed_key
    }
