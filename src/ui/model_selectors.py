"""
Ê®°ÂûãÈÄâÊã©Âô®ÁªÑ‰ª∂
Stage 3.2.1 - ‰∏≠È£éÈô©ÈáçÊûÑ
ÊèêÂèñËá™ apppro.py
"""

import streamlit as st
import ollama
from typing import List, Optional, Tuple


def render_ollama_model_selector(
    llm_url: str,
    saved_model: str,
    ollama_ok: bool
) -> Tuple[str, bool]:
    """
    Ê∏≤Êüì Ollama Ê®°ÂûãÈÄâÊã©Âô®
    
    Args:
        llm_url: Ollama API URL
        saved_model: ‰øùÂ≠òÁöÑÈªòËÆ§Ê®°Âûã
        ollama_ok: Ollama ËøûÊé•Áä∂ÊÄÅ
        
    Returns:
        Tuple[str, bool]: (ÈÄâ‰∏≠ÁöÑÊ®°Âûã, ÊòØÂê¶ÈúÄË¶Å‰øùÂ≠ò‰∏∫ÈªòËÆ§)
    """
    save_as_default = False
    
    # Âà∑Êñ∞ÊåâÈíÆ
    col_refresh = st.columns([3, 1])[1]
    with col_refresh:
        if st.button("üîÑ", key="refresh_ollama_models", help="Âà∑Êñ∞Ê®°ÂûãÂàóË°®", use_container_width=True):
            if ollama_ok:
                models = _fetch_ollama_models(llm_url)
                if models:
                    st.session_state.ollama_models = models
                    st.toast(f"‚úÖ ÊâæÂà∞ {len(models)} ‰∏™Ê®°Âûã")
                    st.rerun()
                else:
                    st.warning("Êú™ÊâæÂà∞Ê®°Âûã")
                    st.session_state.ollama_models = []
            else:
                st.warning("ËØ∑ÂÖàÂêØÂä® Ollama")
    
    # Ëá™Âä®Âä†ËΩΩÊ®°ÂûãÂàóË°®ÔºàÈ¶ñÊ¨°Ôºâ
    if ollama_ok and ("ollama_models" not in st.session_state or not st.session_state.ollama_models):
        models = _fetch_ollama_models(llm_url)
        st.session_state.ollama_models = models if models else []
    
    if not ollama_ok:
        st.session_state.ollama_models = []
    
    # Ê®°ÂûãÈÄâÊã©/ËæìÂÖ•
    if st.session_state.ollama_models:
        # Â¶ÇÊûúÊúâÊ®°ÂûãÂàóË°®ÔºåÊ∑ªÂä†‰∏Ä‰∏™"ÊâãÂä®ËæìÂÖ•"ÈÄâÈ°π
        options = st.session_state.ollama_models + ["‚úèÔ∏è ÊâãÂä®ËæìÂÖ•..."]
        idx = st.session_state.ollama_models.index(saved_model) if saved_model in st.session_state.ollama_models else 0
        
        col1, col2 = st.columns([4, 1])
        with col1:
            selected = st.selectbox("ÈÄâÊã©/ËæìÂÖ•Ê®°Âûã", options, index=idx, label_visibility="collapsed")
        with col2:
            if st.button("‚≠ê ÈªòËÆ§", key="set_default_llm", use_container_width=True):
                if selected != "‚úèÔ∏è ÊâãÂä®ËæìÂÖ•...":
                    save_as_default = True
                else:
                    st.warning("ËØ∑ÂÖàËæìÂÖ•Ê®°ÂûãÂêç")
        
        if selected == "‚úèÔ∏è ÊâãÂä®ËæìÂÖ•...":
            llm_model = st.text_input("Ê®°ÂûãÂêç", saved_model, label_visibility="collapsed", key="llm_manual_1")
        else:
            llm_model = selected
    else:
        llm_model = st.text_input("ËæìÂÖ•Ê®°ÂûãÂêç", saved_model, key="llm_direct_1")
    
    return llm_model, save_as_default


def render_openai_model_selector(
    llm_url: str,
    llm_key: str,
    saved_model: str
) -> str:
    """
    Ê∏≤Êüì OpenAI ÂÖºÂÆπÊ®°ÂûãÈÄâÊã©Âô®
    
    Args:
        llm_url: API Base URL
        llm_key: API Key
        saved_model: ‰øùÂ≠òÁöÑÈªòËÆ§Ê®°Âûã
        
    Returns:
        str: ÈÄâ‰∏≠ÁöÑÊ®°Âûã
    """
    from src.utils.model_utils import fetch_remote_models
    
    if st.button("üîÑ Âà∑Êñ∞ÂàóË°®", use_container_width=True):
        with st.spinner("Ê≠£Âú®ËøûÊé•Ê®°ÂûãÂàóË°®..."):
            mods, err = fetch_remote_models(llm_url, llm_key)
            if mods:
                st.session_state.model_list = mods
            else:
                st.error(err)
    
    if st.session_state.model_list:
        idx = st.session_state.model_list.index(saved_model) if saved_model in st.session_state.model_list else 0
        llm_model = st.selectbox("ÈÄâÊã©Ê®°Âûã", st.session_state.model_list, index=idx)
    else:
        llm_model = st.text_input("ËæìÂÖ•Ê®°ÂûãÂêç", saved_model, key="llm_openai_1")
    
    return llm_model


def render_hf_embedding_selector(
    saved_model: str,
    preset_models: Optional[List[str]] = None,
    model_descriptions: Optional[dict] = None
) -> str:
    """
    Ê∏≤Êüì HuggingFace ÂµåÂÖ•Ê®°ÂûãÈÄâÊã©Âô®
    
    Args:
        saved_model: ‰øùÂ≠òÁöÑÈªòËÆ§Ê®°Âûã
        preset_models: È¢ÑËÆæÊ®°ÂûãÂàóË°®
        model_descriptions: Ê®°ÂûãÊèèËø∞Â≠óÂÖ∏
        
    Returns:
        str: ÈÄâ‰∏≠ÁöÑÊ®°Âûã
    """
    from src.utils.model_utils import check_hf_model_exists
    
    # ÈªòËÆ§È¢ÑËÆæÊ®°Âûã
    if preset_models is None:
        preset_models = [
            "BAAI/bge-small-zh-v1.5",
            "BAAI/bge-large-zh-v1.5",
            "BAAI/bge-m3",
            "BAAI/bge-base-zh-v1.5",
            "moka-ai/m3e-base",
            "shibing624/text2vec-base-chinese",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "Ëá™ÂÆö‰πâÊ®°Âûã..."
        ]
    
    if model_descriptions is None:
        model_descriptions = {
            "BAAI/bge-small-zh-v1.5": "üöÄ Â∞èÂûãÂø´ÈÄüÁâà | 90MB | ÈÄÇÂêàÂÆûÊó∂Â∫îÁî®„ÄÅËµÑÊ∫êÂèóÈôêÂú∫ÊôØ",
            "BAAI/bge-large-zh-v1.5": "üéØ ‰∏≠ÊñáÊúÄÂº∫Áâà | 1.3GB | ÊúÄÈ´òÂáÜÁ°ÆÂ∫¶ÔºåÊé®ËçêÁî®‰∫éÁ≤æÂáÜÊ£ÄÁ¥¢",
            "BAAI/bge-m3": "üåç Â§öËØ≠Ë®ÄÊúÄÂº∫ | 2GB | ÊîØÊåÅ100+ËØ≠Ë®ÄÔºåË∑®ËØ≠Ë®ÄÊ£ÄÁ¥¢ÊúÄ‰Ω≥",
            "BAAI/bge-base-zh-v1.5": "‚öñÔ∏è Âπ≥Ë°°ÁâàÊú¨ | 400MB | ÈÄüÂ∫¶‰∏éÂáÜÁ°ÆÂ∫¶ÁöÑÂÆåÁæéÂπ≥Ë°°",
            "moka-ai/m3e-base": "üî§ M3E‰∏≠Êñá‰ºòÂåñ | 400MB | ‰∏≠ÊñáËØ≠‰πâÁêÜËß£‰ºòÂåñ",
            "shibing624/text2vec-base-chinese": "üìù Text2Vec‰∏≠Êñá | 400MB | ‰∏≠ÊñáÊñáÊú¨ÂêëÈáèÂåñ‰∏ìÂÆ∂",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "üí° ËΩªÈáèÂ§öËØ≠Ë®Ä | 400MB | ËµÑÊ∫êÂèóÈôêÊó∂ÁöÑÂ§öËØ≠Ë®ÄÊñπÊ°à"
        }
    
    # Á°ÆÂÆöÈªòËÆ§Á¥¢Âºï
    try:
        default_idx = preset_models.index(saved_model) if saved_model in preset_models else 0
    except:
        default_idx = 0
    
    col1, col2 = st.columns([5, 1])
    with col1:
        selected = st.selectbox(
            "HF Ê®°Âûã",
            options=preset_models,
            index=default_idx,
            help=model_descriptions.get(preset_models[default_idx], ""),
            label_visibility="collapsed"
        )
    
    # Â¶ÇÊûúÈÄâÊã©Ëá™ÂÆö‰πâÔºåÊòæÁ§∫ËæìÂÖ•Ê°Ü
    if selected == "Ëá™ÂÆö‰πâÊ®°Âûã...":
        embed_model = st.text_input(
            "ËæìÂÖ•Ê®°ÂûãÂêçÁß∞",
            placeholder="‰æãÂ¶Ç: sentence-transformers/all-MiniLM-L6-v2",
            help="ËæìÂÖ•‰ªªÊÑè HuggingFace Ê®°Âûã ID"
        )
        if not embed_model:
            embed_model = "BAAI/bge-small-zh-v1.5"  # ÈªòËÆ§ÂÄº
    else:
        embed_model = selected
    
    # Ê£ÄÊü•Ê®°ÂûãÊòØÂê¶Â≠òÂú®Âπ∂ÊòæÁ§∫Áä∂ÊÄÅ
    model_exists = check_hf_model_exists(embed_model)
    
    with col2:
        button_label = "‚úÖ ‚≠ê" if model_exists else "‚≠ê"
        if st.button(button_label, key="set_default_embed", use_container_width=True, help="ËÆæ‰∏∫ÈªòËÆ§Ê®°Âûã"):
            # ËøîÂõû‰ø°Âè∑ÔºåËÆ©Ë∞ÉÁî®ËÄÖ‰øùÂ≠òÈÖçÁΩÆ
            st.session_state.save_embed_model = embed_model
    
    # ÊòæÁ§∫Ê®°ÂûãÁä∂ÊÄÅ
    if not model_exists:
        st.warning("‚ö†Ô∏è Ê®°ÂûãÊú™‰∏ãËΩΩ")
        if st.button("üì• ‰∏ãËΩΩÊ®°Âûã", key="download_hf_model", type="primary", use_container_width=True):
            _download_hf_model(embed_model)
    else:
        st.success("‚úÖ Ê®°ÂûãÂ∑≤Â∞±Áª™")
    
    return embed_model


def _fetch_ollama_models(llm_url: str) -> List[str]:
    """
    Ëé∑Âèñ Ollama Ê®°ÂûãÂàóË°®
    
    Args:
        llm_url: Ollama API URL
        
    Returns:
        List[str]: Ê®°ÂûãÂàóË°®
    """
    try:
        from src.utils.model_manager import clean_proxy
        clean_proxy()
        client = ollama.Client(host=llm_url)
        models_resp = client.list()
        
        models = []
        if hasattr(models_resp, 'models'):
            # Êñ∞Áâà ollama ËøîÂõû ListResponse ÂØπË±°
            for m in models_resp.models:
                if hasattr(m, 'model'):
                    models.append(m.model)
                elif isinstance(m, str):
                    models.append(m)
        elif isinstance(models_resp, dict) and 'models' in models_resp:
            # ÊóßÁâàËøîÂõûÂ≠óÂÖ∏
            for m in models_resp['models']:
                if isinstance(m, dict):
                    models.append(m.get('name') or m.get('model', ''))
                else:
                    models.append(str(m))
        
        return [m for m in models if m]
    except Exception as e:
        st.error(f"Ëé∑ÂèñÂ§±Ë¥•: {e}")
        return []


def _download_hf_model(model_name: str) -> None:
    """
    ‰∏ãËΩΩ HuggingFace Ê®°Âûã
    
    Args:
        model_name: Ê®°ÂûãÂêçÁß∞
    """
    import sys
    import subprocess
    
    with st.spinner(f"Ê≠£Âú®‰∏ãËΩΩ {model_name}..."):
        try:
            download_script = f"""
import os
os.environ['HF_HUB_OFFLINE'] = '0'
os.environ['TRANSFORMERS_OFFLINE'] = '0'
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="{model_name}",
    cache_dir="./hf_cache",
    local_dir="./hf_cache/{model_name.replace('/', '--')}",
    local_dir_use_symlinks=False
)
print("SUCCESS")
"""
            result = subprocess.run(
                [sys.executable, "-c", download_script],
                capture_output=True,
                text=True,
                timeout=600
            )
            
            if result.returncode == 0 and "SUCCESS" in result.stdout:
                st.success(f"‚úÖ ‰∏ãËΩΩÂÆåÊàê: {model_name}")
                import time
                time.sleep(1)
                st.rerun()
            else:
                st.error(f"‰∏ãËΩΩÂ§±Ë¥•: {result.stderr}")
        except Exception as e:
            st.error(f"‰∏ãËΩΩÂ§±Ë¥•: {e}")
