"""
Ê®°ÂûãÈÄâÊã©Âô®ÁªÑ‰ª∂
Stage 3.2.1 - ‰∏≠È£éÈô©ÈáçÊûÑ
ÊèêÂèñËá™ apppro.py
"""

import streamlit as st
import ollama
from typing import List, Optional, Tuple


def render_ollama_model_selector(
    llm_url: str,
    saved_model: str,
    ollama_ok: bool
) -> Tuple[str, bool]:
    """
    Ê∏≤Êüì Ollama Ê®°ÂûãÈÄâÊã©Âô® (‰∏çÂåÖÂê´Âà∑Êñ∞ÊåâÈíÆÔºåÁî±Â§ñÈÉ®ÂÆπÂô®Êèê‰æõ)
    """
    save_as_default = False
    
    if not saved_model:
        saved_model = "gpt-oss:20b"
    
    # Ëá™Âä®Âä†ËΩΩÊ®°ÂûãÂàóË°® (v2.9.5 ‰ºòÂåñ)
    # Â¶ÇÊûú URL ÊîπÂèò‰∫ÜÔºå‰πüÈáçÊñ∞Âä†ËΩΩ
    current_url_key = f"last_ollama_url"
    if ollama_ok:
        url_changed = st.session_state.get(current_url_key) != llm_url
        if url_changed or "ollama_models" not in st.session_state or not st.session_state.ollama_models:
            models = _fetch_ollama_models(llm_url)
            st.session_state.ollama_models = models if models else []
            st.session_state[current_url_key] = llm_url
    
    if not ollama_ok:
        st.session_state.ollama_models = []
    
    # Ê®°ÂûãÈÄâÊã©/ËæìÂÖ•
    if st.session_state.get("ollama_models"):
        # Â¶ÇÊûúÊúâÊ®°ÂûãÂàóË°®ÔºåÊ∑ªÂä†‰∏Ä‰∏™"ÊâãÂä®ËæìÂÖ•"ÈÄâÈ°π
        options = st.session_state.ollama_models + ["‚úèÔ∏è ÊâãÂä®ËæìÂÖ•..."]
        idx = st.session_state.ollama_models.index(saved_model) if saved_model in st.session_state.ollama_models else 0
        
        selected = st.selectbox("ÈÄâÊã©Ê®°Âûã", options, index=idx, label_visibility="collapsed", key="config_model_selectbox")
        
        if selected == "‚úèÔ∏è ÊâãÂä®ËæìÂÖ•...":
            llm_model = st.text_input("Ê®°ÂûãÂêç", saved_model, label_visibility="collapsed", key="llm_manual_1")
        else:
            llm_model = selected
    else:
        llm_model = st.text_input("ËæìÂÖ•Ê®°ÂûãÂêç", saved_model, key="llm_direct_1", label_visibility="collapsed")
    
    return llm_model, save_as_default


def render_openai_model_selector(
    llm_url: str,
    llm_key: str,
    saved_model: str
) -> str:
    """
    Ê∏≤Êüì OpenAI ÂÖºÂÆπÊ®°ÂûãÈÄâÊã©Âô® (‰∏çÂåÖÂê´Âà∑Êñ∞ÊåâÈíÆÔºåÁî±Â§ñÈÉ®ÂÆπÂô®Êèê‰æõ)
    """
    from src.utils.model_utils import fetch_remote_models
    
    # ‰ΩøÁî®ÁºìÂ≠òÈîÆ
    cache_key = f"model_list_{hash(llm_url + llm_key)}"
    
    # Ëá™Âä®Âä†ËΩΩ
    if llm_url and llm_key and cache_key not in st.session_state:
        mods, err = fetch_remote_models(llm_url, llm_key)
        if mods:
            st.session_state[cache_key] = mods
    
    model_list = st.session_state.get(cache_key, [])
    
    if model_list:
        if saved_model and saved_model not in model_list:
            model_list.insert(0, saved_model)
        idx = model_list.index(saved_model) if saved_model in model_list else 0
        llm_model = st.selectbox("ÈÄâÊã©Ê®°Âûã", model_list, index=idx, label_visibility="collapsed", key="openai_model_selectbox")
    else:
        llm_model = st.text_input("ËæìÂÖ•Ê®°ÂûãÂêç", saved_model, key="llm_openai_1", label_visibility="collapsed")
    
    return llm_model


def render_hf_embedding_selector(
    saved_model: str,
    preset_models: Optional[List[str]] = None,
    model_descriptions: Optional[dict] = None
) -> str:
    """
    Ê∏≤Êüì HuggingFace ÂµåÂÖ•Ê®°ÂûãÈÄâÊã©Âô®
    
    Args:
        saved_model: ‰øùÂ≠òÁöÑÈªòËÆ§Ê®°Âûã
        preset_models: È¢ÑËÆæÊ®°ÂûãÂàóË°®
        model_descriptions: Ê®°ÂûãÊèèËø∞Â≠óÂÖ∏
        
    Returns:
        str: ÈÄâ‰∏≠ÁöÑÊ®°Âûã
    """
    from src.utils.model_utils import check_hf_model_exists
    
    # ÈªòËÆ§È¢ÑËÆæÊ®°Âûã
    if preset_models is None:
        preset_models = [
            "sentence-transformers/all-MiniLM-L6-v2",
            "BAAI/bge-large-zh-v1.5",
            "BAAI/bge-m3",
            "BAAI/bge-base-zh-v1.5",
            "moka-ai/m3e-base",
            "shibing624/text2vec-base-chinese",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "Ëá™ÂÆö‰πâÊ®°Âûã..."
        ]
    
    if model_descriptions is None:
        model_descriptions = {
            "sentence-transformers/all-MiniLM-L6-v2": "üöÄ Â∞èÂûãÂø´ÈÄüÁâà | 90MB | ÈÄÇÂêàÂÆûÊó∂Â∫îÁî®„ÄÅËµÑÊ∫êÂèóÈôêÂú∫ÊôØ",
            "BAAI/bge-large-zh-v1.5": "üéØ ‰∏≠ÊñáÊúÄÂº∫Áâà | 1.3GB | ÊúÄÈ´òÂáÜÁ°ÆÂ∫¶ÔºåÊé®ËçêÁî®‰∫éÁ≤æÂáÜÊ£ÄÁ¥¢",
            "BAAI/bge-m3": "üåç Â§öËØ≠Ë®ÄÊúÄÂº∫ | 2GB | ÊîØÊåÅ100+ËØ≠Ë®ÄÔºåË∑®ËØ≠Ë®ÄÊ£ÄÁ¥¢ÊúÄ‰Ω≥",
            "BAAI/bge-base-zh-v1.5": "‚öñÔ∏è Âπ≥Ë°°ÁâàÊú¨ | 400MB | ÈÄüÂ∫¶‰∏éÂáÜÁ°ÆÂ∫¶ÁöÑÂÆåÁæéÂπ≥Ë°°",
            "moka-ai/m3e-base": "üî§ M3E‰∏≠Êñá‰ºòÂåñ | 400MB | ‰∏≠ÊñáËØ≠‰πâÁêÜËß£‰ºòÂåñ",
            "shibing624/text2vec-base-chinese": "üìù Text2Vec‰∏≠Êñá | 400MB | ‰∏≠ÊñáÊñáÊú¨ÂêëÈáèÂåñ‰∏ìÂÆ∂",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "üí° ËΩªÈáèÂ§öËØ≠Ë®Ä | 400MB | ËµÑÊ∫êÂèóÈôêÊó∂ÁöÑÂ§öËØ≠Ë®ÄÊñπÊ°à"
        }
    
    # Á°ÆÂÆöÈªòËÆ§Á¥¢Âºï
    try:
        default_idx = preset_models.index(saved_model) if saved_model in preset_models else 0
    except:
        default_idx = 0
    
    col1, col2 = st.columns([5, 1])
    with col1:
        selected = st.selectbox(
            "HF Ê®°Âûã",
            options=preset_models,
            index=default_idx,
            help=model_descriptions.get(preset_models[default_idx], ""),
            key="config_hf_selectbox",
            label_visibility="collapsed"
        )
    
    # Â¶ÇÊûúÈÄâÊã©Ëá™ÂÆö‰πâÔºåÊòæÁ§∫ËæìÂÖ•Ê°Ü
    if selected == "Ëá™ÂÆö‰πâÊ®°Âûã...":
        embed_model = st.text_input(
            "ËæìÂÖ•Ê®°ÂûãÂêçÁß∞",
            placeholder="‰æãÂ¶Ç: sentence-transformers/all-MiniLM-L6-v2",
            help="ËæìÂÖ•‰ªªÊÑè HuggingFace Ê®°Âûã ID"
        )
        if not embed_model:
            embed_model = "sentence-transformers/all-MiniLM-L6-v2"  # ÈªòËÆ§ÂÄº
    else:
        embed_model = selected
    
    # Ê£ÄÊü•Ê®°ÂûãÊòØÂê¶Â≠òÂú®Âπ∂ÊòæÁ§∫Áä∂ÊÄÅ
    model_exists = check_hf_model_exists(embed_model)
    
    with col2:
        if model_exists:
            if st.button("‚úÖ ‚≠ê", key="config_set_default_embed", use_container_width=True, help="Ê®°ÂûãÂ∑≤Â∞±Áª™ÔºåÁÇπÂáªËÆæ‰∏∫ÈªòËÆ§"):
                # ËøîÂõû‰ø°Âè∑ÔºåËÆ©Ë∞ÉÁî®ËÄÖ‰øùÂ≠òÈÖçÁΩÆ
                st.session_state.save_embed_model = embed_model
            st.caption("Â∑≤Â∞±Áª™")
        else:
            if st.button("üì•", key="download_hf_model", type="primary", use_container_width=True, help="Á´ãÂç≥‰∏ãËΩΩÊ®°Âûã"):
                _download_hf_model(embed_model)
            st.caption("Êú™‰∏ãËΩΩ")
    
    return embed_model


def _fetch_ollama_models(llm_url: str) -> List[str]:
    """
    Ëé∑Âèñ Ollama Ê®°ÂûãÂàóË°®
    
    Args:
        llm_url: Ollama API URL
        
    Returns:
        List[str]: Ê®°ÂûãÂàóË°®
    """
    try:
        from src.utils.model_manager import clean_proxy
        clean_proxy()
        client = ollama.Client(host=llm_url)
        models_resp = client.list()
        
        models = []
        if hasattr(models_resp, 'models'):
            # Êñ∞Áâà ollama ËøîÂõû ListResponse ÂØπË±°
            for m in models_resp.models:
                if hasattr(m, 'model'):
                    models.append(m.model)
                elif isinstance(m, str):
                    models.append(m)
        elif isinstance(models_resp, dict) and 'models' in models_resp:
            # ÊóßÁâàËøîÂõûÂ≠óÂÖ∏
            for m in models_resp['models']:
                if isinstance(m, dict):
                    models.append(m.get('name') or m.get('model', ''))
                else:
                    models.append(str(m))
        
        return [m for m in models if m]
    except Exception as e:
        st.error(f"Ëé∑ÂèñÂ§±Ë¥•: {e}")
        return []


def _download_hf_model(model_name: str) -> None:
    """
    ‰∏ãËΩΩ HuggingFace Ê®°Âûã
    
    Args:
        model_name: Ê®°ÂûãÂêçÁß∞
    """
    import sys
    import subprocess
    
    with st.spinner(f"Ê≠£Âú®‰∏ãËΩΩ {model_name}..."):
        try:
            download_script = f"""
import os
os.environ['HF_HUB_OFFLINE'] = '0'
os.environ['TRANSFORMERS_OFFLINE'] = '0'
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="{model_name}",
    cache_dir="./hf_cache",
    local_dir="./hf_cache/{model_name.replace('/', '--')}",
    local_dir_use_symlinks=False
)
print("SUCCESS")
"""
            result = subprocess.run(
                [sys.executable, "-c", download_script],
                capture_output=True,
                text=True,
                timeout=600
            )
            
            if result.returncode == 0 and "SUCCESS" in result.stdout:
                st.success(f"‚úÖ ‰∏ãËΩΩÂÆåÊàê: {model_name}")
                import time
                time.sleep(1)
                st.rerun()
            else:
                st.error(f"‰∏ãËΩΩÂ§±Ë¥•: {result.stderr}")
        except Exception as e:
            st.error(f"‰∏ãËΩΩÂ§±Ë¥•: {e}")
