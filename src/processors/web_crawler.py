import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import hashlib
import re
import fnmatch
from typing import List, Optional, Callable

class WebCrawler:
    def __init__(self, output_dir="temp_uploads/web_crawl"):
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        self.visited_urls = set()
        self.failed_urls = set()
        self.retry_counts = {}
        
        # åˆ›å»ºä¼šè¯ï¼Œå¢å¼ºåçˆ¬å¤„ç†
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0"
        })
        
        # è®¾ç½®é‡è¯•ç­–ç•¥
        try:
            from urllib3.util.retry import Retry
            from requests.adapters import HTTPAdapter
            
            retry_strategy = Retry(
                total=3,
                backoff_factor=1,
                status_forcelist=[429, 500, 502, 503, 504],
            )
            
            adapter = HTTPAdapter(max_retries=retry_strategy)
            self.session.mount("http://", adapter)
            self.session.mount("https://", adapter)
        except ImportError:
            pass  # å¦‚æœæ²¡æœ‰urllib3ï¼Œä½¿ç”¨åŸºæœ¬é‡è¯•
        
        # åçˆ¬å¤„ç†é…ç½®
        self.anti_bot_config = {
            'min_delay': 0.5,      # æœ€å°å»¶è¿Ÿ
            'max_delay': 2.0,      # æœ€å¤§å»¶è¿Ÿ
            'retry_delay': 5.0,    # é‡è¯•å»¶è¿Ÿ
            'max_retries': 3,      # æœ€å¤§é‡è¯•æ¬¡æ•°
            'timeout': 15,         # è¯·æ±‚è¶…æ—¶
        }

    def _is_valid_url(self, url):
        parsed = urlparse(url)
        return bool(parsed.netloc) and bool(parsed.scheme)
    
    
    def _smart_request(self, url: str, status_callback=None) -> requests.Response:
        """æ™ºèƒ½è¯·æ±‚ï¼Œå¤„ç†åçˆ¬æœºåˆ¶"""
        import random
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤±è´¥è¿‡å¤šæ¬¡
        if url in self.failed_urls:
            raise Exception(f"URLå·²è¢«æ ‡è®°ä¸ºå¤±è´¥: {url}")
        
        retry_count = self.retry_counts.get(url, 0)
        if retry_count >= self.anti_bot_config['max_retries']:
            self.failed_urls.add(url)
            raise Exception(f"é‡è¯•æ¬¡æ•°è¶…é™: {url}")
        
        try:
            # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
            delay = random.uniform(
                self.anti_bot_config['min_delay'], 
                self.anti_bot_config['max_delay']
            )
            time.sleep(delay)
            
            # éšæœºåŒ–User-Agent
            user_agents = [
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15"
            ]
            
            headers = self.session.headers.copy()
            headers['User-Agent'] = random.choice(user_agents)
            
            # æ·»åŠ Refererï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            parsed_url = urlparse(url)
            if parsed_url.netloc:
                headers['Referer'] = f"{parsed_url.scheme}://{parsed_url.netloc}/"
            
            response = self.session.get(
                url, 
                headers=headers,
                timeout=self.anti_bot_config['timeout'],
                allow_redirects=True
            )
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code == 403:
                raise Exception(f"è®¿é—®è¢«æ‹’ç» (403): {url}")
            elif response.status_code == 429:
                # é€Ÿç‡é™åˆ¶ï¼Œå¢åŠ å»¶è¿Ÿåé‡è¯•
                if status_callback:
                    status_callback(f"é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {self.anti_bot_config['retry_delay']} ç§’åé‡è¯•")
                time.sleep(self.anti_bot_config['retry_delay'])
                raise Exception(f"é€Ÿç‡é™åˆ¶ (429): {url}")
            elif response.status_code >= 400:
                raise Exception(f"HTTPé”™è¯¯ ({response.status_code}): {url}")
            
            # é‡ç½®é‡è¯•è®¡æ•°
            if url in self.retry_counts:
                del self.retry_counts[url]
            
            return response
            
        except Exception as e:
            # å¢åŠ é‡è¯•è®¡æ•°
            self.retry_counts[url] = retry_count + 1
            
            # å¦‚æœæ˜¯å¯é‡è¯•çš„é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†
            if "429" in str(e) or "timeout" in str(e).lower():
                if retry_count < self.anti_bot_config['max_retries'] - 1:
                    if status_callback:
                        status_callback(f"è¯·æ±‚å¤±è´¥ï¼Œå‡†å¤‡é‡è¯• ({retry_count + 1}/{self.anti_bot_config['max_retries']}): {e}")
                    time.sleep(self.anti_bot_config['retry_delay'])
            
            raise e

    def _fix_url(self, url):
        """è‡ªåŠ¨ä¿®å¤URLæ ¼å¼ï¼Œæ·»åŠ åè®®å‰ç¼€"""
        if not url:
            return ""
        
        url = url.strip()
        
        # å¦‚æœå·²ç»æœ‰åè®®ï¼Œç›´æ¥è¿”å›
        if url.startswith(('http://', 'https://')):
            return url
        
        # å¦‚æœæ˜¯å¸¸è§åŸŸåæ ¼å¼ï¼Œè‡ªåŠ¨æ·»åŠ https://
        import re
        if re.match(r'^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', url):
            return f"https://{url}"
        
        return url

    def _should_exclude_url(self, url: str, exclude_patterns: List[str]) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦åº”è¯¥è¢«æ’é™¤"""
        if not exclude_patterns:
            return False
        
        for pattern in exclude_patterns:
            if fnmatch.fnmatch(url, pattern) or fnmatch.fnmatch(url.lower(), pattern.lower()):
                return True
        return False

    def _extract_links(self, soup, base_url: str, exclude_patterns: List[str] = None) -> List[str]:
        """æå–é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥"""
        links = []
        base_domain = urlparse(base_url).netloc
        
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if not href:
                continue
            
            # æ„å»ºå®Œæ•´URL
            full_url = urljoin(base_url, href)
            parsed_url = urlparse(full_url)
            
            # åˆ¤æ–­æ˜¯å¦å…è®¸å¤–éƒ¨é“¾æ¥
            is_search_engine = any(se in base_domain for se in [
                'google.com', 'bing.com', 'baidu.com', 'yahoo.com', 
                'duckduckgo.com', 'sogou.com', 'so.com', 'zhihu.com'
            ])
            
            # å¦‚æœä¸æ˜¯æœç´¢å¼•æ“ï¼Œåˆ™åªå¤„ç†åŒåŸŸåé“¾æ¥
            if not is_search_engine and parsed_url.netloc != base_domain:
                continue
            
            # ç§»é™¤fragment
            clean_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}"
            if parsed_url.query:
                clean_url += f"?{parsed_url.query}"
            
            # æ£€æŸ¥æ’é™¤æ¨¡å¼
            if self._should_exclude_url(clean_url, exclude_patterns or []):
                continue
            
            # è¿‡æ»¤å¸¸è§çš„éå†…å®¹é“¾æ¥
            skip_patterns = [
                r'\.(?:jpg|jpeg|png|gif|pdf|zip|exe|dmg)$',
                r'#',
                r'javascript:',
                r'mailto:',
                r'/search\?',
                r'/login',
                r'/register',
                r'/logout'
            ]
            
            should_skip = False
            for pattern in skip_patterns:
                if re.search(pattern, clean_url, re.IGNORECASE):
                    should_skip = True
                    break
            
            if not should_skip and clean_url not in links:
                links.append(clean_url)
        
        return links

    def _clean_text(self, text):
        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _save_content(self, url, title, content):
        if not content or len(content.strip()) < 50:  # å†…å®¹å¤ªå°‘åˆ™è·³è¿‡
            return None
            
        # ç”Ÿæˆæ–‡ä»¶å
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        safe_title = re.sub(r'[^\w\u4e00-\u9fff]+', '_', title)[:50]
        filename = f"{safe_title}_{url_hash}.txt"
        filepath = os.path.join(self.output_dir, filename)
        
        # æ·»åŠ å…ƒæ•°æ®å¤´
        file_content = f"URL: {url}\nTitle: {title}\nCrawl Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n{content}"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(file_content)
        
        return filepath

    def crawl_advanced(self, 
                      start_url: str, 
                      max_depth: int = 1, 
                      max_pages: int = 10,
                      exclude_patterns: List[str] = None,
                      parser_type: str = "default",
                      status_callback: Optional[Callable] = None) -> List[str]:
        """
        é«˜çº§é€’å½’çˆ¬å–ç½‘é¡µ
        
        Args:
            start_url: èµ·å§‹URL
            max_depth: æœ€å¤§æ·±åº¦ (1-5)
            max_pages: æ¯å±‚æœ€å¤§é¡µé¢æ•°é‡
            exclude_patterns: æ’é™¤é“¾æ¥æ¨¡å¼åˆ—è¡¨ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰
            parser_type: é¡µé¢è§£æå™¨ç±»å‹ ("default", "article", "documentation")
            status_callback: çŠ¶æ€å›è°ƒå‡½æ•° func(msg)
        
        Returns:
            list: å·²ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # ğŸ›‘ å®‰å…¨ç†”æ–­ï¼šå…¨å±€æœ€å¤§é¡µé¢é™åˆ¶
        GLOBAL_MAX_PAGES = 50000
        total_estimated = max_pages ** max_depth
        if total_estimated > GLOBAL_MAX_PAGES:
            if status_callback:
                status_callback(f"âš ï¸ å®‰å…¨ç†”æ–­ï¼šé¢„ä¼°é¡µé¢æ•° {total_estimated} è¶…è¿‡é™åˆ¶ {GLOBAL_MAX_PAGES}")
            max_pages = min(max_pages, int(GLOBAL_MAX_PAGES ** (1/max_depth)))
        
        # è‡ªåŠ¨ä¿®å¤URLæ ¼å¼
        start_url = self._fix_url(start_url)
        
        if not self._is_valid_url(start_url):
            raise ValueError(f"Invalid URL '{start_url}': No scheme supplied. Perhaps you meant https://{start_url.replace('https://', '').replace('http://', '')}?")
        
        self.visited_urls = set()
        # æŒ‰å±‚çº§ç»„ç»‡é˜Ÿåˆ—: {depth: [urls]}
        current_level = [start_url]
        saved_files = []
        total_count = 0
        
        base_domain = urlparse(start_url).netloc
        
        if status_callback:
            status_callback(f"å¼€å§‹çˆ¬å–: {start_url} (æœ€å¤§æ·±åº¦: {max_depth}, æ¯å±‚æœ€å¤§é¡µæ•°: {max_pages})")
        
        for depth in range(1, max_depth + 1):
            if not current_level:
                break
                
            next_level = []
            level_count = 0
            
            # é™åˆ¶å½“å‰å±‚çš„é¡µé¢æ•°
            current_level = current_level[:max_pages]
            
            if status_callback:
                status_callback(f"ğŸ“‚ ç¬¬{depth}å±‚å¼€å§‹: å‡†å¤‡å¤„ç† {len(current_level)} ä¸ªé“¾æ¥")
            
            for url in current_level:
                if url in self.visited_urls or level_count >= max_pages:
                    continue
                
                self.visited_urls.add(url)
                
                try:
                    if status_callback:
                        status_callback(f"æ­£åœ¨æŠ“å– ({total_count+1}) ç¬¬{depth}å±‚ ({level_count+1}/{max_pages}): {url}")
                    
                    # ä½¿ç”¨æ™ºèƒ½è¯·æ±‚æ–¹æ³•
                    response = self._smart_request(url, status_callback)
                    response.encoding = response.apparent_encoding
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æ ¹æ®è§£æå™¨ç±»å‹æå–å†…å®¹
                    content = self._extract_content_by_parser(soup, parser_type)
                    
                    title = soup.title.string if soup.title else "No Title"
                    title = self._clean_text(title)
                    
                    # ä¿å­˜å†…å®¹
                    filepath = self._save_content(url, title, content)
                    if filepath:
                        saved_files.append(filepath)
                        level_count += 1
                        total_count += 1
                        if status_callback:
                            status_callback(f"âœ… å·²ä¿å­˜: {title} ({len(content)} å­—ç¬¦)")
                    
                    # å¦‚æœè¿˜æ²¡è¾¾åˆ°æœ€å¤§æ·±åº¦ï¼Œæå–ä¸‹ä¸€çº§é“¾æ¥
                    if depth < max_depth:
                        links = self._extract_links(soup, url, exclude_patterns)
                        next_level.extend(links)
                        
                        if status_callback and links:
                            status_callback(f"å‘ç° {len(links)} ä¸ªæ–°é“¾æ¥ï¼Œæ·»åŠ åˆ°ç¬¬{depth+1}å±‚é˜Ÿåˆ—")
                    
                    time.sleep(0.5)  # ç¤¼è²Œçˆ¬å–
                    
                except Exception as e:
                    if status_callback:
                        status_callback(f"æŠ“å–å¤±è´¥ {url}: {e}")
                    continue
            
            # å‡†å¤‡ä¸‹ä¸€å±‚
            current_level = list(set(next_level))  # å»é‡
            
            if status_callback:
                status_callback(f"ğŸ¯ ç¬¬{depth}å±‚å®Œæˆ: æˆåŠŸæŠ“å– {level_count} é¡µï¼Œå‘ç° {len(current_level)} ä¸ªä¸‹çº§é“¾æ¥")
        
        if status_callback:
            status_callback(f"ğŸ‰ çˆ¬å–å®Œæˆï¼æ€»å…±è·å– {len(saved_files)} ä¸ªé¡µé¢ (å…±{max_depth}å±‚)")
                
        return saved_files

    def _extract_content_by_parser(self, soup, parser_type: str) -> str:
        """æ ¹æ®è§£æå™¨ç±»å‹æå–å†…å®¹"""
        
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
            script.decompose()
        
        if parser_type == "article":
            # æ–‡ç« æ¨¡å¼ï¼šä¼˜å…ˆæå–articleã€mainã€contentç­‰æ ‡ç­¾
            content_selectors = [
                'article', 'main', '[role="main"]', 
                '.content', '.post-content', '.article-content',
                '.entry-content', '.post-body'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    text = elements[0].get_text()
                    clean_text = "\n".join([line.strip() for line in text.splitlines() if line.strip()])
                    if len(clean_text) > 100:
                        return clean_text
        
        elif parser_type == "documentation":
            # æ–‡æ¡£æ¨¡å¼ï¼šæå–æ–‡æ¡£ç‰¹å®šçš„å†…å®¹åŒºåŸŸ
            doc_selectors = [
                '.documentation', '.docs-content', '.doc-content',
                '.markdown-body', '.rst-content', '.wiki-content',
                '#content', '.main-content'
            ]
            
            for selector in doc_selectors:
                elements = soup.select(selector)
                if elements:
                    text = elements[0].get_text()
                    clean_text = "\n".join([line.strip() for line in text.splitlines() if line.strip()])
                    if len(clean_text) > 100:
                        return clean_text
        
        # é»˜è®¤æ¨¡å¼ï¼šæå–æ‰€æœ‰æ–‡æœ¬
        text = soup.get_text()
        clean_text = "\n".join([line.strip() for line in text.splitlines() if line.strip()])
        return clean_text

    def crawl(self, start_url, max_depth=1, max_pages=10, status_callback=None):
        """ä¿æŒå‘åå…¼å®¹çš„ç®€å•æ¥å£"""
        return self.crawl_advanced(
            start_url=start_url,
            max_depth=max_depth,
            max_pages=max_pages,
            status_callback=status_callback
        )
