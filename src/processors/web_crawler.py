import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import hashlib
import re
import fnmatch
from typing import List, Optional, Callable

class WebCrawler:
    def __init__(self, output_dir="temp_uploads/web_crawl"):
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        self.visited_urls = set()
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Ch-Ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            "Sec-Ch-Ua-Mobile": "?0",
            "Sec-Ch-Ua-Platform": '"macOS"'
        })

    def _is_valid_url(self, url):
        parsed = urlparse(url)
        return bool(parsed.netloc) and bool(parsed.scheme)
    
    def _fix_url(self, url):
        """è‡ªåŠ¨ä¿®å¤URLæ ¼å¼ï¼Œæ·»åŠ åè®®å‰ç¼€"""
        if not url:
            return ""
        
        url = url.strip()
        
        # å¦‚æœå·²ç»æœ‰åè®®ï¼Œç›´æ¥è¿”å›
        if url.startswith(('http://', 'https://')):
            return url
        
        # å¦‚æœæ˜¯å¸¸è§åŸŸåæ ¼å¼ï¼Œè‡ªåŠ¨æ·»åŠ https://
        import re
        if re.match(r'^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', url):
            return f"https://{url}"
        
        return url

    def _should_exclude_url(self, url: str, exclude_patterns: List[str]) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦åº”è¯¥è¢«æ’é™¤"""
        if not exclude_patterns:
            return False
        
        for pattern in exclude_patterns:
            if fnmatch.fnmatch(url, pattern) or fnmatch.fnmatch(url.lower(), pattern.lower()):
                return True
        return False

    def _extract_links(self, soup, base_url: str, exclude_patterns: List[str] = None) -> List[str]:
        """æå–é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥"""
        links = []
        base_domain = urlparse(base_url).netloc
        
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if not href:
                continue
            
            # æ„å»ºå®Œæ•´URL
            full_url = urljoin(base_url, href)
            parsed_url = urlparse(full_url)
            
            # åˆ¤æ–­æ˜¯å¦å…è®¸å¤–éƒ¨é“¾æ¥
            is_search_engine = any(se in base_domain for se in [
                'google.com', 'bing.com', 'baidu.com', 'yahoo.com', 
                'duckduckgo.com', 'sogou.com', 'so.com', 'zhihu.com'
            ])
            
            # å¦‚æœä¸æ˜¯æœç´¢å¼•æ“ï¼Œåˆ™åªå¤„ç†åŒåŸŸåé“¾æ¥
            if not is_search_engine and parsed_url.netloc != base_domain:
                continue
            
            # ç§»é™¤fragment
            clean_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}"
            if parsed_url.query:
                clean_url += f"?{parsed_url.query}"
            
            # æ£€æŸ¥æ’é™¤æ¨¡å¼
            if self._should_exclude_url(clean_url, exclude_patterns or []):
                continue
            
            # è¿‡æ»¤å¸¸è§çš„éå†…å®¹é“¾æ¥
            skip_patterns = [
                r'\.(?:jpg|jpeg|png|gif|pdf|zip|exe|dmg)$',
                r'#',
                r'javascript:',
                r'mailto:',
                r'/search\?',
                r'/login',
                r'/register',
                r'/logout'
            ]
            
            should_skip = False
            for pattern in skip_patterns:
                if re.search(pattern, clean_url, re.IGNORECASE):
                    should_skip = True
                    break
            
            if not should_skip and clean_url not in links:
                links.append(clean_url)
        
        return links

    def _clean_text(self, text):
        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _save_content(self, url, title, content):
        if not content or len(content.strip()) < 50:  # å†…å®¹å¤ªå°‘åˆ™è·³è¿‡
            return None
            
        # ç”Ÿæˆæ–‡ä»¶å
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        safe_title = re.sub(r'[^\w\u4e00-\u9fff]+', '_', title)[:50]
        filename = f"{safe_title}_{url_hash}.txt"
        filepath = os.path.join(self.output_dir, filename)
        
        # æ·»åŠ å…ƒæ•°æ®å¤´
        file_content = f"URL: {url}\nTitle: {title}\nCrawl Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n{content}"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(file_content)
        
        return filepath

    def crawl_advanced(self, 
                      start_url: str, 
                      max_depth: int = 1, 
                      max_pages: int = 10,
                      exclude_patterns: List[str] = None,
                      parser_type: str = "default",
                      status_callback: Optional[Callable] = None) -> List[str]:
        """
        é«˜çº§é€’å½’çˆ¬å–ç½‘é¡µ
        
        Args:
            start_url: èµ·å§‹URL
            max_depth: æœ€å¤§æ·±åº¦ (1-5)
            max_pages: æœ€å¤§é¡µé¢æ•°é‡é™åˆ¶
            exclude_patterns: æ’é™¤é“¾æ¥æ¨¡å¼åˆ—è¡¨ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰
            parser_type: é¡µé¢è§£æå™¨ç±»å‹ ("default", "article", "documentation")
            status_callback: çŠ¶æ€å›è°ƒå‡½æ•° func(msg)
        
        Returns:
            list: å·²ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # è‡ªåŠ¨ä¿®å¤URLæ ¼å¼
        start_url = self._fix_url(start_url)
        
        if not self._is_valid_url(start_url):
            raise ValueError(f"Invalid URL '{start_url}': No scheme supplied. Perhaps you meant https://{start_url.replace('https://', '').replace('http://', '')}?")
        
        self.visited_urls = set()
        # ä½¿ç”¨é˜Ÿåˆ—å­˜å‚¨ (url, depth, parent_url)
        queue = [(start_url, 1, None)]
        saved_files = []
        count = 0
        
        base_domain = urlparse(start_url).netloc
        
        if status_callback:
            status_callback(f"å¼€å§‹çˆ¬å–: {start_url} (æœ€å¤§æ·±åº¦: {max_depth}, æœ€å¤§é¡µæ•°: {max_pages})")
        
        while queue and count < max_pages:
            url, depth, parent_url = queue.pop(0)
            
            if url in self.visited_urls:
                continue
            
            self.visited_urls.add(url)
            
            try:
                if status_callback:
                    status_callback(f"æ­£åœ¨æŠ“å– ({count+1}/{max_pages}) æ·±åº¦{depth}: {url}")
                
                response = self.session.get(url, timeout=15)
                response.encoding = response.apparent_encoding
                
                if response.status_code != 200:
                    if status_callback:
                        status_callback(f"è·³è¿‡ {url} (çŠ¶æ€ç : {response.status_code})")
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æ ¹æ®è§£æå™¨ç±»å‹æå–å†…å®¹
                content = self._extract_content_by_parser(soup, parser_type)
                
                title = soup.title.string if soup.title else "No Title"
                title = self._clean_text(title)
                
                # ä¿å­˜å†…å®¹
                filepath = self._save_content(url, title, content)
                if filepath:
                    saved_files.append(filepath)
                    count += 1
                    if status_callback:
                        status_callback(f"âœ… å·²ä¿å­˜: {title} ({len(content)} å­—ç¬¦)")
                
                # å¦‚æœè¿˜æ²¡è¾¾åˆ°æœ€å¤§æ·±åº¦ï¼Œæå–ä¸‹ä¸€çº§é“¾æ¥
                if depth < max_depth:
                    links = self._extract_links(soup, url, exclude_patterns)
                    
                    # é™åˆ¶æ¯é¡µæå–çš„é“¾æ¥æ•°é‡ï¼Œé¿å…çˆ†ç‚¸å¼å¢é•¿
                    max_links_per_page = min(20, max_pages - count)
                    links = links[:max_links_per_page]
                    
                    for link in links:
                        if link not in self.visited_urls and (link, depth + 1, url) not in queue:
                            queue.append((link, depth + 1, url))
                    
                    if status_callback and links:
                        status_callback(f"å‘ç° {len(links)} ä¸ªæ–°é“¾æ¥ï¼Œæ·»åŠ åˆ°é˜Ÿåˆ—")
                
                time.sleep(0.5)  # ç¤¼è²Œçˆ¬å–
                
            except Exception as e:
                if status_callback:
                    status_callback(f"æŠ“å–å¤±è´¥ {url}: {e}")
                continue
        
        if status_callback:
            status_callback(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(saved_files)} ä¸ªé¡µé¢")
                
        return saved_files

    def _extract_content_by_parser(self, soup, parser_type: str) -> str:
        """æ ¹æ®è§£æå™¨ç±»å‹æå–å†…å®¹"""
        
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
            script.decompose()
        
        if parser_type == "article":
            # æ–‡ç« æ¨¡å¼ï¼šä¼˜å…ˆæå–articleã€mainã€contentç­‰æ ‡ç­¾
            content_selectors = [
                'article', 'main', '[role="main"]', 
                '.content', '.post-content', '.article-content',
                '.entry-content', '.post-body'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    text = elements[0].get_text()
                    clean_text = "\n".join([line.strip() for line in text.splitlines() if line.strip()])
                    if len(clean_text) > 100:
                        return clean_text
        
        elif parser_type == "documentation":
            # æ–‡æ¡£æ¨¡å¼ï¼šæå–æ–‡æ¡£ç‰¹å®šçš„å†…å®¹åŒºåŸŸ
            doc_selectors = [
                '.documentation', '.docs-content', '.doc-content',
                '.markdown-body', '.rst-content', '.wiki-content',
                '#content', '.main-content'
            ]
            
            for selector in doc_selectors:
                elements = soup.select(selector)
                if elements:
                    text = elements[0].get_text()
                    clean_text = "\n".join([line.strip() for line in text.splitlines() if line.strip()])
                    if len(clean_text) > 100:
                        return clean_text
        
        # é»˜è®¤æ¨¡å¼ï¼šæå–æ‰€æœ‰æ–‡æœ¬
        text = soup.get_text()
        clean_text = "\n".join([line.strip() for line in text.splitlines() if line.strip()])
        return clean_text

    def crawl(self, start_url, max_depth=1, max_pages=10, status_callback=None):
        """ä¿æŒå‘åå…¼å®¹çš„ç®€å•æ¥å£"""
        return self.crawl_advanced(
            start_url=start_url,
            max_depth=max_depth,
            max_pages=max_pages,
            status_callback=status_callback
        )
