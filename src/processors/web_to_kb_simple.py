"""ç®€åŒ–ç‰ˆç½‘é¡µæŠ“å–åˆ°çŸ¥è¯†åº“æ„å»ºæµç¨‹"""

import os
import streamlit as st
from typing import Optional, Callable, List, Dict
from datetime import datetime
from urllib.parse import urlparse
from .web_crawler import WebCrawler


def generate_kb_name_from_web(url: str, files_count: int = 0) -> str:
    """æ ¹æ®URLç”Ÿæˆæ™ºèƒ½çŸ¥è¯†åº“åç§°"""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        
        # ç‰¹æ®Šç½‘ç«™å¤„ç†
        if 'wikipedia.org' in domain:
            path_parts = [p for p in parsed.path.split('/') if p and len(p) > 2]
            if path_parts:
                return f"ç™¾ç§‘_{path_parts[-1][:10]}"
            return "ç»´åŸºç™¾ç§‘"
        elif 'baidu.com' in domain:
            return "ç™¾åº¦ç™¾ç§‘"
        elif 'zhihu.com' in domain:
            return "çŸ¥ä¹é—®ç­”"
        elif 'csdn.net' in domain:
            return "CSDNæŠ€æœ¯"
        elif 'github.com' in domain:
            path_parts = parsed.path.split('/')
            if len(path_parts) >= 3:
                return f"é¡¹ç›®_{path_parts[2][:10]}"
            return "GitHubé¡¹ç›®"
        elif 'stackoverflow.com' in domain:
            return "ç¼–ç¨‹é—®ç­”"
        
        # é€šç”¨å¤„ç†
        domain_name = domain.split('.')[0]
        if files_count > 1:
            return f"{domain_name}_{files_count}é¡µ"
        else:
            return f"{domain_name}_{datetime.now().strftime('%m%d')}"
            
    except:
        return f"ç½‘é¡µ_{datetime.now().strftime('%m%d%H%M')}"


def get_preset_search_sites() -> Dict[str, str]:
    """è·å–é¢„è®¾æœç´¢ç½‘ç«™"""
    return {
        "ç»´åŸºç™¾ç§‘": "https://zh.wikipedia.org/wiki/Special:Search?search={keyword}",
        "ç™¾åº¦ç™¾ç§‘": "https://baike.baidu.com/search?word={keyword}",
        "çŸ¥ä¹": "https://www.zhihu.com/search?type=content&q={keyword}",
        "CSDN": "https://so.csdn.net/so/search?q={keyword}",
        "Stack Overflow": "https://stackoverflow.com/search?q={keyword}",
        "GitHub": "https://github.com/search?q={keyword}&type=repositories"
    }


def crawl_and_create_kb(url: str = None, 
                       keyword: str = None,
                       sites: List[str] = None,
                       max_depth: int = 1,
                       max_pages: int = 10,
                       kb_name: str = None,
                       status_callback: Optional[Callable] = None) -> Dict:
    """
    ç½‘é¡µæŠ“å–å¹¶è‡ªåŠ¨åˆ›å»ºçŸ¥è¯†åº“
    
    Args:
        url: ç›´æ¥æŠ“å–çš„URL
        keyword: æœç´¢å…³é”®è¯ï¼ˆå½“urlä¸ºç©ºæ—¶ä½¿ç”¨ï¼‰
        sites: è¦æœç´¢çš„ç½‘ç«™åˆ—è¡¨
        max_depth: æŠ“å–æ·±åº¦
        max_pages: æœ€å¤§é¡µé¢æ•°
        kb_name: æŒ‡å®šçŸ¥è¯†åº“åç§°
        status_callback: çŠ¶æ€å›è°ƒå‡½æ•°
    
    Returns:
        dict: å¤„ç†ç»“æœ
    """
    try:
        # ä½¿ç”¨å”¯ä¸€çš„æ—¶é—´æˆ³ç›®å½•ï¼Œç¡®ä¿æ¯æ¬¡æŠ“å–éš”ç¦»
        timestamp_dir = datetime.now().strftime('%Y%m%d_%H%M%S')
        unique_output_dir = os.path.join("temp_uploads", f"web_crawl_{timestamp_dir}")
        
        crawler = WebCrawler(output_dir=unique_output_dir)
        crawled_files = []
        
        if url:
            # ç›´æ¥æŠ“å–URL
            if status_callback:
                status_callback(f"ğŸŒ å¼€å§‹æŠ“å–ç½‘é¡µ: {url}")
            
            crawled_files = crawler.crawl_advanced(
                start_url=url,
                max_depth=max_depth,
                max_pages=max_pages,
                status_callback=status_callback
            )
            
            # ç”ŸæˆçŸ¥è¯†åº“åç§°
            if not kb_name:
                kb_name = generate_kb_name_from_web(url, len(crawled_files))
                
        elif keyword:
            # å…³é”®è¯æœç´¢æ¨¡å¼
            if not sites:
                sites = ["ç»´åŸºç™¾ç§‘", "ç™¾åº¦ç™¾ç§‘"]
            
            if status_callback:
                status_callback(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
            
            preset_sites = get_preset_search_sites()
            
            for site_name in sites[:2]:  # é™åˆ¶æœç´¢ç½‘ç«™æ•°é‡
                if site_name in preset_sites:
                    search_url = preset_sites[site_name].format(keyword=keyword)
                    
                    if status_callback:
                        status_callback(f"ğŸŒ æœç´¢ {site_name}: {search_url}")
                    
                    try:
                        files = crawler.crawl_advanced(
                            start_url=search_url,
                            max_depth=1,
                            max_pages=max_pages // len(sites),
                            status_callback=status_callback
                        )
                        crawled_files.extend(files)
                    except Exception as e:
                        if status_callback:
                            status_callback(f"âŒ {site_name} æœç´¢å¤±è´¥: {e}")
                        continue
            
            # ç”ŸæˆçŸ¥è¯†åº“åç§°
            if not kb_name:
                kb_name = f"æœç´¢_{keyword}_{datetime.now().strftime('%m%d')}"
        
        else:
            return {"success": False, "message": "å¿…é¡»æä¾›URLæˆ–å…³é”®è¯"}
        
        if not crawled_files:
            return {"success": False, "message": "æ²¡æœ‰æˆåŠŸæŠ“å–åˆ°ä»»ä½•å†…å®¹"}
        
        # ä½¿ç”¨ç»Ÿä¸€çš„å‘½åé€»è¾‘ç¡®ä¿å”¯ä¸€æ€§
        from src.utils.kb_name_optimizer import KBNameOptimizer
        from src.core.app_config import output_base
        
        # ç¡®ä¿åç§°å”¯ä¸€
        kb_name = KBNameOptimizer.generate_unique_name(kb_name, output_base)
        
        if status_callback:
            status_callback(f"ğŸ“š åˆ›å»ºçŸ¥è¯†åº“: {kb_name}")
        
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å·²å­˜åœ¨ (KBNameOptimizer å·²ç»å¤„ç†äº†åç§°å†²çªï¼Œè¿™é‡Œåªéœ€è¦æ„å»ºè·¯å¾„)
        kb_path = os.path.join(output_base, kb_name)
        
        # åˆ›å»ºçŸ¥è¯†åº“ç›®å½•
        os.makedirs(kb_path, exist_ok=True)
        
        if status_callback:
            status_callback(f"âœ… çŸ¥è¯†åº“åˆ›å»ºå®Œæˆï¼Œå‡†å¤‡æ„å»ºç´¢å¼•...")
        
        # è®¾ç½®session stateï¼Œè®©ä¸»åº”ç”¨çŸ¥é“æœ‰æ–°çš„æ–‡ä»¶éœ€è¦å¤„ç†
        st.session_state.uploaded_path = os.path.abspath(crawler.output_dir)
        st.session_state.upload_auto_name = kb_name
        st.session_state.auto_create_kb = True  # æ ‡è®°è‡ªåŠ¨åˆ›å»ºçŸ¥è¯†åº“
        st.session_state.selected_kb = kb_name  # è‡ªåŠ¨é€‰æ‹©æ–°çŸ¥è¯†åº“
        
        return {
            "success": True,
            "kb_name": kb_name,
            "files_count": len(crawled_files),
            "files": crawled_files,
            "crawler_output_dir": crawler.output_dir,
            "message": f"âœ… ç½‘é¡µæŠ“å–å®Œæˆï¼Œå·²å‡†å¤‡åˆ›å»ºçŸ¥è¯†åº“ '{kb_name}'"
        }
        
    except Exception as e:
        return {"success": False, "message": f"å¤„ç†å¤±è´¥: {e}"}


def render_enhanced_web_crawl():
    """æ¸²æŸ“å¢å¼ºç‰ˆç½‘é¡µæŠ“å–ç•Œé¢ï¼ˆæ›¿æ¢åŸæœ‰çš„ç½‘é¡µæŠ“å–æ ‡ç­¾é¡µï¼‰"""
    
    # åˆ›å»ºå­æ ‡ç­¾é¡µ
    web_tab1, web_tab2 = st.tabs(["ğŸ”— ç›´æ¥æŠ“å–", "ğŸ” å…³é”®è¯æœç´¢"])
    
    with web_tab1:
        st.write("**ç›´æ¥ç½‘å€æŠ“å–å¹¶åˆ›å»ºçŸ¥è¯†åº“**")
        
        # URLè¾“å…¥
        crawl_url = st.text_input(
            "ğŸ”— ç½‘å€", 
            placeholder="ä¾‹å¦‚: python.org æˆ– https://docs.python.org",
            help="æ”¯æŒè‡ªåŠ¨æ·»åŠ https://å‰ç¼€"
        )
        
        # å‚æ•°è®¾ç½®
        col1, col2, col3 = st.columns(3)
        with col1:
            crawl_depth = st.selectbox("ğŸ” æ·±åº¦", [1, 2, 3, 4, 5], index=0, help="æŠ“å–å±‚çº§")
        with col2:
            max_pages = st.selectbox("ğŸ“„ é¡µæ•°", [5, 10, 20, 50], index=1, help="æœ€å¤§é¡µé¢æ•°")
        with col3:
            kb_name = st.text_input("ğŸ“š çŸ¥è¯†åº“å", placeholder="ç•™ç©ºè‡ªåŠ¨ç”Ÿæˆ", help="å¯é€‰")
        
        # é«˜çº§é€‰é¡¹
        with st.expander("âš™ï¸ é«˜çº§é€‰é¡¹"):
            parser_type = st.selectbox("è§£ææ¨¡å¼", ["default", "article", "documentation"])
            exclude_text = st.text_area("æ’é™¤é“¾æ¥æ¨¡å¼", placeholder="*/admin/*\n*.pdf", height=60)
        
        # æŠ“å–æŒ‰é’®
        if st.button("ğŸš€ æŠ“å–å¹¶åˆ›å»ºçŸ¥è¯†åº“", type="primary", disabled=not crawl_url):
            exclude_patterns = [line.strip() for line in exclude_text.split('\n') if line.strip()] if exclude_text else []
            
            status_container = st.empty()
            progress_bar = st.progress(0)
            
            def status_callback(message):
                status_container.info(message)
                if "å·²ä¿å­˜" in message:
                    progress_bar.progress(min(progress_bar._value + 0.1, 0.9))
            
            try:
                result = crawl_and_create_kb(
                    url=crawl_url,
                    max_depth=crawl_depth,
                    max_pages=max_pages,
                    kb_name=kb_name if kb_name else None,
                    status_callback=status_callback
                )
                
                progress_bar.progress(1.0)
                
                if result["success"]:
                    st.success(result["message"])
                    st.info("ğŸ‰ ç°åœ¨å¯ä»¥åœ¨å·¦ä¾§é€‰æ‹©è¯¥çŸ¥è¯†åº“å¹¶å¼€å§‹å¯¹è¯ï¼")
                    
                    # æ˜¾ç¤ºè¯¦æƒ…
                    with st.expander("ğŸ“Š æŠ“å–è¯¦æƒ…"):
                        st.write(f"**çŸ¥è¯†åº“åç§°**: {result['kb_name']}")
                        st.write(f"**æŠ“å–é¡µé¢æ•°**: {result['files_count']}")
                        st.write(f"**æ–‡ä»¶ä½ç½®**: {result['crawler_output_dir']}")
                    
                    # è§¦å‘é‡æ–°è¿è¡Œä»¥æ›´æ–°ç•Œé¢
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error(result["message"])
                    
            except Exception as e:
                st.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
            finally:
                status_container.empty()
                progress_bar.empty()
    
    with web_tab2:
        st.write("**å…³é”®è¯æœç´¢å¹¶åˆ›å»ºçŸ¥è¯†åº“**")
        
        # å…³é”®è¯è¾“å…¥
        keyword = st.text_input(
            "ğŸ” æœç´¢å…³é”®è¯",
            placeholder="ä¾‹å¦‚: äººå·¥æ™ºèƒ½ã€Pythonç¼–ç¨‹ã€æœºå™¨å­¦ä¹ ",
            help="è¾“å…¥è¦æœç´¢çš„å…³é”®è¯"
        )
        
        # ç½‘ç«™é€‰æ‹©
        st.write("é€‰æ‹©æœç´¢ç½‘ç«™:")
        preset_sites = get_preset_search_sites()
        
        selected_sites = []
        cols = st.columns(3)
        for i, site_name in enumerate(preset_sites.keys()):
            with cols[i % 3]:
                default_checked = site_name in ["ç»´åŸºç™¾ç§‘", "ç™¾åº¦ç™¾ç§‘"]
                if st.checkbox(site_name, value=default_checked, key=f"search_site_{site_name}"):
                    selected_sites.append(site_name)
        
        # å‚æ•°è®¾ç½®
        col1, col2 = st.columns(2)
        with col1:
            search_pages = st.selectbox("æ€»é¡µé¢æ•°", [10, 20, 30, 50], index=1)
        with col2:
            search_kb_name = st.text_input("çŸ¥è¯†åº“å", placeholder="ç•™ç©ºè‡ªåŠ¨ç”Ÿæˆ")
        
        # æœç´¢æŒ‰é’®
        if st.button("ğŸ” æœç´¢å¹¶åˆ›å»ºçŸ¥è¯†åº“", type="primary", disabled=not keyword or not selected_sites):
            status_container = st.empty()
            progress_bar = st.progress(0)
            
            def status_callback(message):
                status_container.info(message)
                if "æœç´¢" in message or "æŠ“å–" in message:
                    progress_bar.progress(min(progress_bar._value + 0.2, 0.9))
            
            try:
                result = crawl_and_create_kb(
                    keyword=keyword,
                    sites=selected_sites,
                    max_pages=search_pages,
                    kb_name=search_kb_name if search_kb_name else None,
                    status_callback=status_callback
                )
                
                progress_bar.progress(1.0)
                
                if result["success"]:
                    st.success(result["message"])
                    st.info("ğŸ‰ ç°åœ¨å¯ä»¥åœ¨å·¦ä¾§é€‰æ‹©è¯¥çŸ¥è¯†åº“å¹¶å¼€å§‹å¯¹è¯ï¼")
                    
                    # æ˜¾ç¤ºè¯¦æƒ…
                    with st.expander("ğŸ“Š æœç´¢è¯¦æƒ…"):
                        st.write(f"**æœç´¢å…³é”®è¯**: {keyword}")
                        st.write(f"**æœç´¢ç½‘ç«™**: {', '.join(selected_sites)}")
                        st.write(f"**çŸ¥è¯†åº“åç§°**: {result['kb_name']}")
                        st.write(f"**æŠ“å–é¡µé¢æ•°**: {result['files_count']}")
                    
                    # è§¦å‘é‡æ–°è¿è¡Œä»¥æ›´æ–°ç•Œé¢
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error(result["message"])
                    
            except Exception as e:
                st.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
            finally:
                status_container.empty()
                progress_bar.empty()
