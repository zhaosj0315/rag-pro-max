"""
ç´¢å¼•æ„å»ºå™¨
Stage 4.1 - æå–è‡ª apppro.py
Stage 6 - ä½¿ç”¨ç»Ÿä¸€çš„å¹¶è¡Œæ‰§è¡Œå™¨
"""

import os
import json
import shutil
import time
from dataclasses import dataclass
from typing import List, Dict, Optional

from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, Settings
from llama_index.core.node_parser import SentenceSplitter

from src.metadata_manager import MetadataManager
from src.file_processor import scan_directory_safe
from src.utils.document_processor import get_file_info
from src.utils.parallel_executor import ParallelExecutor
from src.utils.parallel_tasks import extract_metadata_task
from src.utils.concurrency_manager import ConcurrencyManager
from src.utils.vectorization_wrapper import VectorizationWrapper
from src.utils.dynamic_batch import DynamicBatchOptimizer
from src.app_logging.progress_logger import ProgressLogger


@dataclass
class BuildResult:
    """æ„å»ºç»“æœ"""
    success: bool
    index: Optional[VectorStoreIndex]
    file_count: int
    doc_count: int
    duration: float
    error: Optional[str] = None


class IndexBuilder:
    """ç´¢å¼•æ„å»ºå™¨"""
    
    def __init__(self, kb_name: str, persist_dir: str, 
                 embed_model, embed_model_name: str = "Unknown",
                 use_ocr: bool = False,
                 extract_metadata: bool = True,
                 generate_summary: bool = False,
                 logger=None):
        self.kb_name = kb_name
        self.persist_dir = persist_dir
        self.embed_model = embed_model
        self.embed_model_name = embed_model_name
        self.use_ocr = use_ocr  # OCRé€‰é¡¹
        self.extract_metadata = extract_metadata  # æ˜¯å¦æå–å…ƒæ•°æ®
        self.generate_summary = generate_summary  # æ˜¯å¦ç”Ÿæˆæ‘˜è¦
        self.logger = logger
        self.metadata_mgr = MetadataManager(persist_dir)
        
        # åˆå§‹åŒ–å¹¶å‘ä¼˜åŒ–ç»„ä»¶
        self.concurrency_mgr = ConcurrencyManager()
        self.batch_optimizer = DynamicBatchOptimizer()
        self.vectorization_wrapper = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    def build(self, source_path: str, force_reindex: bool = False, 
              action_mode: str = "NEW", status_callback=None) -> BuildResult:
        """æ„å»ºç´¢å¼•"""
        import streamlit as st
        
        start_time = time.time()
        # åˆå§‹åŒ–è¯¦ç»†è¿›åº¦è®°å½•å™¨
        progress = ProgressLogger(total_steps=6, logger=self.logger)
        
        # å‰ç«¯çŠ¶æ€æ˜¾ç¤º
        status_placeholder = st.empty()
        progress_bar = st.progress(0, text="â³ å‡†å¤‡æ„å»ºç´¢å¼•...")
        
        try:
            # è®¾ç½®åµŒå…¥æ¨¡å‹
            Settings.embed_model = self.embed_model
            
            # æ­¥éª¤1: æ£€æŸ¥ç°æœ‰ç´¢å¼•
            progress.start_step(1, "æ£€æŸ¥ç°æœ‰ç´¢å¼•")
            status_placeholder.info("ğŸ” **æ£€æŸ¥ç°æœ‰ç´¢å¼•**: æ­£åœ¨éªŒè¯ç´¢å¼•çŠ¶æ€...")
            progress_bar.progress(0.17, text="ğŸ” æ£€æŸ¥ç°æœ‰ç´¢å¼•...")
            index = self._load_existing_index(force_reindex, action_mode, status_callback)
            progress.end_step("ç´¢å¼•æ£€æŸ¥å®Œæˆ")
            
            # æ­¥éª¤2: æ‰«ææ–‡ä»¶
            progress.start_step(2, f"æ‰«ææ–‡ä»¶å¤¹: {os.path.basename(source_path)}")
            status_placeholder.info(f"ğŸ“ **æ‰«ææ–‡ä»¶**: æ­£åœ¨æ‰«æ {os.path.basename(source_path)}...")
            progress_bar.progress(0.33, text="ğŸ“ æ‰«ææ–‡ä»¶...")
            total_files = self._scan_files(source_path, status_callback)
            progress.end_step(f"å‘ç° {total_files} ä¸ªæ–‡ä»¶")
            
            # æ­¥éª¤3: è¯»å–æ–‡æ¡£
            progress.start_step(3, f"è¯»å–æ–‡æ¡£å†…å®¹ (å…± {total_files} ä¸ªæ–‡ä»¶)")
            status_placeholder.info(f"ğŸ“„ **è¯»å–æ–‡æ¡£**: æ­£åœ¨å¤„ç† {total_files} ä¸ªæ–‡ä»¶...")
            progress_bar.progress(0.50, text=f"ğŸ“„ è¯»å–æ–‡æ¡£ (0/{total_files})...")
            docs, summary = self._read_documents(source_path, total_files, status_callback)
            progress.end_step(f"æˆåŠŸè¯»å– {summary['success']} ä¸ªæ–‡ä»¶")
            
            # æ­¥éª¤4: æ„å»ºæ¸…å•
            progress.start_step(4, "æ„å»ºæ–‡ä»¶æ¸…å•")
            status_placeholder.info("ğŸ“‹ **æ„å»ºæ¸…å•**: æ­£åœ¨ç”Ÿæˆæ–‡ä»¶ç´¢å¼•...")
            progress_bar.progress(0.67, text="ğŸ“‹ æ„å»ºæ–‡ä»¶æ¸…å•...")
            file_map = self._build_manifest(source_path, status_callback)

            # --- åˆå¹¶æ¸…å•é€»è¾‘ (APPEND æ¨¡å¼) ---
            if action_mode == "APPEND":
                try:
                    from src.config import ManifestManager
                    existing = ManifestManager.load(self.persist_dir)
                    existing_files = existing.get('files', [])
                    
                    # å°†ç°æœ‰æ–‡ä»¶è½¬æ¢ä¸º map æ ¼å¼
                    existing_map = {}
                    for f in existing_files:
                        if isinstance(f, dict):
                            fname = f.get('name')
                            if fname:
                                existing_map[fname] = f
                    
                    # è®°å½•ç»Ÿè®¡
                    old_count = len(existing_map)
                    new_count = len(file_map)
                    
                    # åˆå¹¶ï¼šä¿ç•™ç°æœ‰æ–‡ä»¶ï¼Œç”¨æ–°æ–‡ä»¶è¦†ç›–åŒåæ–‡ä»¶
                    # æ³¨æ„ï¼šfile_map ä¸­æ˜¯æ–°æ‰«æçš„æ–‡ä»¶ï¼Œåº”ä¼˜å…ˆä¿ç•™ (è¦†ç›–æ—§çš„åŒåæ–‡ä»¶)
                    # å°†ä¸åœ¨æ–° batch ä¸­çš„æ—§æ–‡ä»¶åŠ å›æ¥
                    for fname, info in existing_map.items():
                        if fname not in file_map:
                            file_map[fname] = info
                            
                    if status_callback:
                        status_callback("info", f"â• è¿½åŠ æ¨¡å¼: åŸæœ‰ {old_count} + æ–°å¢ {new_count} = æ€»è®¡ {len(file_map)} ä¸ªæ–‡ä»¶")
                    if self.logger:
                        self.logger.info(f"æ¸…å•åˆå¹¶: åŸæœ‰ {old_count} + æ–°å¢ {new_count} -> {len(file_map)}")
                        
                except Exception as e:
                    if self.logger: self.logger.warning(f"åˆå¹¶æ¸…å•å¤±è´¥: {e}")
            # -------------------------------

            progress.end_step(f"ç™»è®° {len(file_map)} ä¸ªæ–‡ä»¶")
            
            # æ­¥éª¤5: è§£æç‰‡æ®µ
            progress.start_step(5, f"è§£ææ–‡æ¡£ç‰‡æ®µ (å…± {len(docs)} ä¸ª)")
            valid_docs = self._parse_documents(docs, file_map, source_path, status_callback)
            progress.end_step(f"ç”Ÿæˆ {len(valid_docs)} ä¸ªæœ‰æ•ˆç‰‡æ®µ")
            
            # æ­¥éª¤6: æ„å»ºç´¢å¼•
            progress.start_step(6, "å‘é‡åŒ–å’Œç´¢å¼•æ„å»º")
            index = self._build_index(index, valid_docs, action_mode, status_callback, file_map)
            progress.end_step("ç´¢å¼•æ„å»ºå®Œæˆ")
            
            # ä¿å­˜ manifest
            self._save_manifest(file_map)
            
            progress.finish_all(success=True)
            
            duration = time.time() - start_time
            return BuildResult(
                success=True,
                index=index,
                file_count=len(file_map),
                doc_count=len(valid_docs),
                duration=duration
            )
            
        except Exception as e:
            progress.finish_all(success=False)
            duration = time.time() - start_time
            return BuildResult(
                success=False,
                index=None,
                file_count=0,
                doc_count=0,
                duration=duration,
                error=str(e)
            )
    
    def _load_existing_index(self, force_reindex, action_mode, callback):
        """åŠ è½½ç°æœ‰ç´¢å¼•"""
        if callback:
            callback("step", 1, "æ£€æŸ¥ç°æœ‰ç´¢å¼•")
        
        if force_reindex or action_mode == "NEW":
            return None
        
        if not os.path.exists(self.persist_dir):
            return None
        
        try:
            storage_context = StorageContext.from_defaults(persist_dir=self.persist_dir)
            index = load_index_from_storage(storage_context)
            if callback:
                callback("info", "ç°æœ‰ç´¢å¼•åŠ è½½æˆåŠŸ")
            return index
        except Exception as e:
            if "shapes" in str(e) and "not aligned" in str(e):
                if callback:
                    callback("warning", "å‘é‡ç»´åº¦ä¸åŒ¹é…ï¼Œæ¸…ç†æ—§ç´¢å¼•")
            else:
                if callback:
                    callback("warning", "ç´¢å¼•æŸåï¼Œè½¬ä¸ºæ–°å»ºæ¨¡å¼")
            shutil.rmtree(self.persist_dir, ignore_errors=True)
            return None
    
    def _scan_files(self, source_path, callback) -> int:
        """æ‰«ææ–‡ä»¶"""
        all_files = []
        for root, _, filenames in os.walk(source_path):
            for f in filenames:
                if not f.startswith('.'):
                    all_files.append(os.path.join(root, f))
        
        total = len(all_files)
        if callback:
            callback("info", f"å‘ç° {total} ä¸ªæ–‡ä»¶")
        
        return total
    
    def _read_documents(self, source_path, total_files, callback):
        """è¯»å–æ–‡æ¡£"""
        docs, process_result = scan_directory_safe(source_path, use_ocr=self.use_ocr)
        summary = process_result.get_summary()
        
        if summary['success'] == 0:
            raise ValueError(f"æ²¡æœ‰æˆåŠŸè¯»å–çš„æ–‡ä»¶ã€‚{process_result.get_report()}")
        
        total = summary['success'] + summary['failed'] + summary['skipped']
        success_rate = (summary['success'] / total * 100) if total > 0 else 0
        
        if callback:
            callback("info", f"è¯»å–å®Œæˆ: {summary['success']}/{total} ä¸ªæ–‡ä»¶ ({success_rate:.1f}%)")
        
        return docs, summary
    
    def _build_manifest(self, source_path, callback) -> Dict:
        """æ„å»ºæ–‡ä»¶æ¸…å•"""
        file_map = {}
        for root, _, filenames in os.walk(source_path):
            for f in filenames:
                if not f.startswith('.'):
                    fp = os.path.join(root, f)
                    info = get_file_info(fp, self.metadata_mgr)
                    info['doc_ids'] = []
                    file_map[f] = info
        
        if callback:
            callback("info", f"æ¸…å•å®Œæˆ: {len(file_map)} ä¸ªæ–‡ä»¶å·²ç™»è®°")
        
        return file_map
    
    def _parse_documents(self, docs, file_map, source_path, callback):
        """è§£ææ–‡æ¡£ç‰‡æ®µ"""
        # æ˜ å°„æ–‡æ¡£ID
        file_text_samples = {}
        for d in docs:
            fname = d.metadata.get('file_name')
            if fname and fname in file_map:
                file_map[fname]['doc_ids'].append(d.doc_id)
                if fname not in file_text_samples and d.text.strip():
                    file_text_samples[fname] = d.text[:1000]
        
        # æå–å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        if self.extract_metadata:
            self._extract_metadata(file_map, file_text_samples, source_path, callback)
        else:
            if callback:
                callback("info", "âš¡ è·³è¿‡å…ƒæ•°æ®æå–ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
        
        # ç”Ÿæˆæ‘˜è¦ï¼ˆå¯é€‰ï¼‰
        if self.generate_summary:
            self._queue_summaries(docs, file_map, callback)
        else:
            if callback:
                callback("info", "âš¡ è·³è¿‡æ‘˜è¦ç”Ÿæˆï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
        
        # è¿‡æ»¤æœ‰æ•ˆæ–‡æ¡£
        valid_docs = [d for d in docs if d.text and d.text.strip()]
        
        if callback:
            callback("info", f"è§£æå®Œæˆ: {len(valid_docs)} ä¸ªæœ‰æ•ˆç‰‡æ®µ")
        
        return valid_docs
    
    def _extract_metadata(self, file_map, text_samples, source_path, callback):
        """æå–å…ƒæ•°æ®"""
        if len(text_samples) == 0:
            return
        
        if callback:
            callback("info", f"æå–å…ƒæ•°æ®: {len(text_samples)} ä¸ªæ–‡ä»¶")
        
        # å‡†å¤‡ä»»åŠ¡
        tasks = []
        for fname, text in text_samples.items():
            if fname in file_map:
                fp = os.path.join(source_path, fname)
                if os.path.exists(fp):
                    doc_ids = file_map[fname]['doc_ids']
                    tasks.append((fp, fname, doc_ids, text, self.persist_dir))
        
        if not tasks:
            return
        
        # ä½¿ç”¨å¹¶è¡Œæ‰§è¡Œå™¨ï¼ˆè‡ªåŠ¨åˆ¤æ–­ä¸²è¡Œ/å¹¶è¡Œï¼‰
        executor = ParallelExecutor()
        results = executor.execute(extract_metadata_task, tasks, chunksize=50, threshold=50)
        
        # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
        for fname, meta in results:
            if fname in file_map:
                file_map[fname].update({
                    'file_hash': meta.get('file_hash', ''),
                    'keywords': meta.get('keywords', []),
                    'language': meta.get('language', 'unknown'),
                    'category': meta.get('category', 'å…¶ä»–æ–‡æ¡£')
                })
    
    def _queue_summaries(self, docs, file_map, callback):
        """ç”Ÿæˆæ‘˜è¦å¹¶æ·»åŠ åˆ°ç´¢å¼•ï¼ˆç«‹å³æ‰§è¡Œï¼‰"""
        # æŒ‰æ–‡ä»¶åå»é‡ï¼Œæ¯ä¸ªæ–‡ä»¶åªç”Ÿæˆä¸€æ¬¡æ‘˜è¦
        file_texts = {}
        for d in docs:
            fname = d.metadata.get('file_name')
            if fname and fname in file_map and d.text.strip() and not file_map[fname].get('summary'):
                if fname not in file_texts:
                    file_texts[fname] = d.text[:2000]  # åªå–ç¬¬ä¸€ä¸ªç‰‡æ®µçš„å‰2000å­—ç¬¦
        
        summary_tasks = list(file_texts.items())
        
        if not summary_tasks:
            return
        
        if callback:
            callback("info", f"æ­£åœ¨ç”Ÿæˆæ‘˜è¦: {len(summary_tasks)} ä¸ªæ–‡ä»¶")
        
        # ç«‹å³ç”Ÿæˆæ‘˜è¦ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
        try:
            # å¯¼å…¥æ‘˜è¦ç”Ÿæˆå‡½æ•°
            from src.common.business import generate_doc_summary
            from concurrent.futures import ThreadPoolExecutor, as_completed
            import threading
            
            def generate_single_summary(fname, text):
                """ç”Ÿæˆå•ä¸ªæ–‡ä»¶çš„æ‘˜è¦"""
                try:
                    summary = generate_doc_summary(text, fname)
                    return fname, summary, None
                except Exception as e:
                    return fname, None, str(e)
            
            # å¹¶è¡Œç”Ÿæˆæ‘˜è¦
            max_workers = min(4, len(summary_tasks))  # æœ€å¤š4ä¸ªçº¿ç¨‹
            completed_count = 0
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_fname = {
                    executor.submit(generate_single_summary, fname, text): fname 
                    for fname, text in summary_tasks
                }
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in as_completed(future_to_fname):
                    fname, summary, error = future.result()
                    completed_count += 1
                    
                    if callback:
                        callback("progress", completed_count, len(summary_tasks), f"ç”Ÿæˆæ‘˜è¦: {fname}")
                    
                    if summary:
                        # ä¿å­˜åˆ° file_map
                        file_map[fname]['summary'] = summary
                        if callback:
                            callback("info", f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸ: {fname}")
                    elif error:
                        if self.logger:
                            self.logger.warning(f"âš ï¸ æ‘˜è¦ç”Ÿæˆå¤±è´¥ {fname}: {error}")
                        if callback:
                            callback("warning", f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {fname}")
            
            if callback:
                callback("info", f"æ‘˜è¦ç”Ÿæˆå®Œæˆ: {len(summary_tasks)} ä¸ªæ–‡ä»¶ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰")
                
        except ImportError:
            # å¦‚æœæ— æ³•å¯¼å…¥æ‘˜è¦ç”Ÿæˆå‡½æ•°ï¼Œå›é€€åˆ°é˜Ÿåˆ—æ¨¡å¼
            if callback:
                callback("warning", "æ‘˜è¦ç”Ÿæˆå‡½æ•°ä¸å¯ç”¨ï¼Œä½¿ç”¨é˜Ÿåˆ—æ¨¡å¼")
            self._queue_summaries_async(summary_tasks, callback)
    
    def _queue_summaries_async(self, summary_tasks, callback):
        """å¼‚æ­¥é˜Ÿåˆ—æ¨¡å¼ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        if callback:
            callback("info", f"æ‘˜è¦ç”Ÿæˆå·²åŠ å…¥åå°é˜Ÿåˆ— ({len(summary_tasks)} ä¸ªæ–‡ä»¶)")
        
        # å¼‚æ­¥å†™å…¥é˜Ÿåˆ—æ–‡ä»¶
        import threading
        
        def write_queue_async():
            try:
                queue_file = os.path.join(self.persist_dir, "summary_queue.json")
                os.makedirs(self.persist_dir, exist_ok=True)
                
                cleaned_tasks = [(fname, self._clean_text(text)) for fname, text in summary_tasks]
                
                with open(queue_file, 'w', encoding='utf-8', errors='ignore') as f:
                    json.dump({
                        'tasks': cleaned_tasks,
                        'total': len(cleaned_tasks),
                        'completed': 0
                    }, f, ensure_ascii=False)
            except Exception as e:
                if self.logger:
                    self.logger.warning(f"âš ï¸ æ‘˜è¦é˜Ÿåˆ—å†™å…¥å¤±è´¥: {e}")
        
        # å¯åŠ¨åå°çº¿ç¨‹
        thread = threading.Thread(target=write_queue_async, daemon=True)
        thread.start()
    
    @staticmethod
    def _clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        try:
            return text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')
        except:
            return ""
    
    def _build_index(self, index, valid_docs, action_mode, callback, file_map=None):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        if index and action_mode == "APPEND":
            # è¿½åŠ æ¨¡å¼
            if callback:
                callback("info", "è¿½åŠ æ¨¡å¼: æ’å…¥æ–°æ–‡æ¡£")
            for d in valid_docs:
                index.insert(d)
        else:
            # æ–°å»ºæ¨¡å¼
            if callback:
                callback("info", "æ–°å»ºæ¨¡å¼: æ„å»ºå‘é‡ç´¢å¼•ï¼ˆå¼‚æ­¥ä¼˜åŒ–ï¼‰")
            
            if os.path.exists(self.persist_dir):
                shutil.rmtree(self.persist_dir, ignore_errors=True)
            
            # ä½¿ç”¨ä¼˜åŒ–çš„å‘é‡åŒ–åŒ…è£…å™¨
            try:
                if not self.vectorization_wrapper:
                    self.vectorization_wrapper = VectorizationWrapper(
                        embed_model=self.embed_model,
                        batch_optimizer=self.batch_optimizer
                    )
                
                # åŠ¨æ€æ‰¹é‡ä¼˜åŒ–å‘é‡åŒ–
                index = self.vectorization_wrapper.vectorize_documents(valid_docs, show_progress=True)
                if self.logger:
                    self.logger.info("âœ… ä¼˜åŒ–å‘é‡åŒ–å®Œæˆ")
            except Exception as e:
                # é™çº§åˆ°åŒæ­¥æ¨¡å¼
                if self.logger:
                    self.logger.warning(f"ä¼˜åŒ–å‘é‡åŒ–å¤±è´¥ï¼Œé™çº§åˆ°æ ‡å‡†æ¨¡å¼: {e}")
                index = VectorStoreIndex.from_documents(valid_docs, show_progress=True)
        
        # æ·»åŠ æ‘˜è¦æ–‡æ¡£åˆ°ç´¢å¼•
        if self.generate_summary and file_map:
            self._add_summaries_to_index(index, file_map, callback)
            
        index.storage_context.persist(persist_dir=self.persist_dir)
        
        # ä¿å­˜çŸ¥è¯†åº“ä¿¡æ¯
        self._save_kb_info()
        
        return index
    
    def _add_summaries_to_index(self, index, file_map, callback):
        """å°†æ‘˜è¦æ·»åŠ åˆ°å‘é‡ç´¢å¼•"""
        summary_docs = []
        
        for fname, file_info in file_map.items():
            if file_info.get('summary'):
                try:
                    from llama_index.core import Document
                    summary_doc = Document(
                        text=f"æ–‡æ¡£æ‘˜è¦ - {fname}:\n{file_info['summary']}",
                        metadata={
                            "file_name": fname,
                            "file_type": "summary",
                            "source_file": fname
                        }
                    )
                    summary_docs.append(summary_doc)
                except Exception as e:
                    if self.logger:
                        self.logger.warning(f"æ‘˜è¦æ–‡æ¡£åˆ›å»ºå¤±è´¥ {fname}: {e}")
        
        if summary_docs:
            if callback:
                callback("info", f"æ·»åŠ æ‘˜è¦åˆ°ç´¢å¼•: {len(summary_docs)} ä¸ª")
            
            for doc in summary_docs:
                try:
                    index.insert(doc)
                except Exception as e:
                    if self.logger:
                        self.logger.warning(f"æ‘˜è¦æ’å…¥å¤±è´¥: {e}")
            
            if callback:
                callback("info", f"âœ… æ‘˜è¦å·²æ·»åŠ åˆ°ç´¢å¼•: {len(summary_docs)} ä¸ª")
    
    def _save_kb_info(self):
        """ä¿å­˜çŸ¥è¯†åº“ä¿¡æ¯"""
        try:
            # ä½¿ç”¨åˆå§‹åŒ–æ—¶ä¼ å…¥çš„åŸå§‹æ¨¡å‹åç§°
            embed_model_name = self.embed_model_name
            embed_dim = 0
            
            if self.embed_model:
                # å°è¯•è·å–ç»´åº¦
                try:
                    test_embedding = self.embed_model._get_text_embedding("test")
                    embed_dim = len(test_embedding)
                except:
                    # æ ¹æ®æ¨¡å‹åç§°æ¨æ–­ç»´åº¦
                    if "small" in embed_model_name.lower():
                        embed_dim = 512
                    elif "base" in embed_model_name.lower():
                        embed_dim = 768
                    elif "large" in embed_model_name.lower():
                        embed_dim = 1024
                    else:
                        embed_dim = 1024
            
            kb_info = {
                "embedding_model": embed_model_name,
                "embedding_dim": embed_dim,
                "created_at": time.time()
            }
            
            kb_info_file = os.path.join(self.persist_dir, ".kb_info.json")
            with open(kb_info_file, 'w') as f:
                json.dump(kb_info, f, indent=2)
            
            if self.logger:
                self.logger.success(f"âœ… å·²ä¿å­˜çŸ¥è¯†åº“ä¿¡æ¯: {embed_model_name} ({embed_dim}D)")
        except Exception as e:
            if self.logger:
                self.logger.warning(f"âš ï¸ ä¿å­˜çŸ¥è¯†åº“ä¿¡æ¯å¤±è´¥: {e}")


    def _save_manifest(self, file_map):
        """ä¿å­˜ manifest.json"""
        try:
            from src.config import ManifestManager
            
            files_list = list(file_map.values())
            ManifestManager.save(
                self.persist_dir,
                files_list,
                self.embed_model_name
            )
            
            if self.logger:
                self.logger.success(f"âœ… å·²ä¿å­˜æ–‡ä»¶æ¸…å•: {len(files_list)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            if self.logger:
                self.logger.warning(f"âš ï¸ ä¿å­˜æ–‡ä»¶æ¸…å•å¤±è´¥: {e}")


# å¤šè¿›ç¨‹è¾…åŠ©å‡½æ•°
def _extract_metadata_task(task):
    """å…ƒæ•°æ®æå–ä»»åŠ¡ï¼ˆå¤šè¿›ç¨‹å®‰å…¨ï¼‰"""
    fp, fname, doc_ids, text_sample, persist_dir = task
    temp_mgr = MetadataManager(persist_dir)
    return fname, temp_mgr.add_file_metadata(fp, doc_ids, text_sample)
