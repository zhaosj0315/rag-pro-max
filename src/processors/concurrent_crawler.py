"""
å¹¶å‘çˆ¬å–ç®¡ç†å™¨
æ”¯æŒå¤šè¿›ç¨‹å’Œå¤šçº¿ç¨‹æ··åˆæ¨¡å¼ï¼Œçªç ´GILé™åˆ¶
"""

import threading
import time
import random
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from queue import Queue, Empty
from typing import List, Dict, Callable, Optional
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import os

# ğŸ”¥ æ–°å¢ï¼šå¯¼å…¥æ™ºèƒ½ä¼˜åŒ–å™¨
from .crawl_optimizer import CrawlOptimizer

def fetch_url_worker(args):
    """å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°"""
    url, timeout, user_agents, base_delay, max_delay = args
    
    start_time = time.time()
    result = {
        'url': url,
        'success': False,
        'content': None,
        'title': None,
        'links': [],
        'response_time': 0,
        'error': None
    }
    
    try:
        # åˆ›å»ºæ–°çš„sessionï¼ˆè¿›ç¨‹é—´ä¸å…±äº«ï¼‰
        session = requests.Session()
        
        # éšæœºUser-Agent
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = session.get(url, headers=headers, timeout=timeout)
        response_time = time.time() - start_time
        result['response_time'] = response_time
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title_tag = soup.find('title')
            result['title'] = title_tag.get_text().strip() if title_tag else url
            
            # æå–å†…å®¹
            for script in soup(["script", "style"]):
                script.decompose()
            
            content = soup.get_text()
            lines = (line.strip() for line in content.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            content = ' '.join(chunk for chunk in chunks if chunk)
            
            result['content'] = content
            
            # æå–é“¾æ¥
            base_domain = urlparse(url).netloc
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if href:
                    full_url = urljoin(url, href)
                    parsed = urlparse(full_url)
                    if parsed.netloc == base_domain:
                        result['links'].append(full_url)
            
            result['success'] = True
        else:
            result['error'] = f"HTTP {response.status_code}"
            
    except Exception as e:
        result['error'] = str(e)
        result['response_time'] = time.time() - start_time
    
    # æ™ºèƒ½å»¶è¿Ÿ
    if result['response_time']:
        if result['response_time'] < 1.0:
            delay = base_delay * 0.5
        elif result['response_time'] > 3.0:
            delay = base_delay * 2.0
        else:
            delay = base_delay
    else:
        delay = base_delay
    
    delay += random.uniform(0, 0.5)
    delay = min(delay, max_delay)
    time.sleep(delay)
    
    return result

class ConcurrentCrawler:
    """å¹¶å‘çˆ¬å–ç®¡ç†å™¨ - æ”¯æŒå¤šè¿›ç¨‹å’Œå¤šçº¿ç¨‹"""
    
    def __init__(self, max_workers=None, use_processes=True, base_delay=1.0, max_delay=3.0):
        # è‡ªåŠ¨æ£€æµ‹æœ€ä½³workeræ•°é‡
        if max_workers is None:
            cpu_count = os.cpu_count() or 4
            if use_processes:
                # è¿›ç¨‹æ¨¡å¼ï¼šä½¿ç”¨CPUæ ¸å¿ƒæ•°ï¼Œä½†ä¸è¶…è¿‡6ä¸ªï¼ˆé¿å…è¿‡åº¦æ¶ˆè€—èµ„æºï¼‰
                max_workers = min(cpu_count, 6)
            else:
                # çº¿ç¨‹æ¨¡å¼ï¼šå¯ä»¥ä½¿ç”¨æ›´å¤šçº¿ç¨‹ï¼ˆI/Oå¯†é›†å‹ï¼‰
                max_workers = min(cpu_count * 2, 8)
        
        self.max_workers = max_workers
        self.use_processes = use_processes
        self.base_delay = base_delay
        self.max_delay = max_delay
        
        # ğŸ”¥ æ–°å¢ï¼šæ™ºèƒ½ä¼˜åŒ–å™¨
        self.optimizer = CrawlOptimizer()
        
        # çº¿ç¨‹æ¨¡å¼æ‰éœ€è¦session
        if not use_processes:
            self.session = requests.Session()
        
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'start_time': None,
            'response_times': []
        }
        
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0'
        ]
    
    def get_smart_recommendations(self, url: str) -> Dict:
        """ğŸ”¥ æ–°å¢ï¼šè·å–æ™ºèƒ½çˆ¬å–æ¨èå‚æ•°"""
        return self.optimizer.analyze_website(url)

    def crawl_with_smart_params(self, 
                               start_urls: List[str],
                               use_smart_params: bool = True,
                               manual_depth: Optional[int] = None,
                               manual_pages: Optional[int] = None,
                               progress_callback: Optional[Callable] = None) -> List[Dict]:
        """ğŸ”¥ æ–°å¢ï¼šä½¿ç”¨æ™ºèƒ½å‚æ•°æ¨èçš„å¹¶å‘çˆ¬å–æ–¹æ³•"""
        
        if not start_urls:
            return []
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªURLè¿›è¡Œæ™ºèƒ½åˆ†æ
        main_url = start_urls[0]
        
        if use_smart_params:
            # è·å–æ™ºèƒ½æ¨è
            recommendations = self.get_smart_recommendations(main_url)
            
            if progress_callback:
                progress_callback("ğŸ§  æ™ºèƒ½åˆ†æç½‘ç«™...")
                progress_callback(f"ğŸ“Š ç½‘ç«™ç±»å‹: {recommendations['site_type']}")
                progress_callback(f"ğŸ“ æè¿°: {recommendations['description']}")
                progress_callback(f"ğŸ¯ æ¨èæ·±åº¦: {recommendations['recommended_depth']}å±‚")
                progress_callback(f"ğŸ“„ æ¨èé¡µæ•°: {recommendations['recommended_pages']}é¡µ/å±‚")
                progress_callback(f"ğŸ“ˆ é¢„ä¼°æ€»é¡µæ•°: {recommendations['estimated_pages']:,}é¡µ")
                progress_callback(f"ğŸ” ç½®ä¿¡åº¦: {recommendations['confidence']:.1%}")
            
            # ä½¿ç”¨æ¨èå‚æ•°ï¼ˆå¯è¢«æ‰‹åŠ¨å‚æ•°è¦†ç›–ï¼‰
            max_depth = manual_depth or recommendations['recommended_depth']
            max_pages_per_level = manual_pages or recommendations['recommended_pages']
            
            if progress_callback:
                progress_callback(f"âš™ï¸ æœ€ç»ˆå‚æ•°: æ·±åº¦={max_depth}, é¡µæ•°={max_pages_per_level}")
        else:
            # ä½¿ç”¨æ‰‹åŠ¨å‚æ•°æˆ–é»˜è®¤å€¼
            max_depth = manual_depth or 2
            max_pages_per_level = manual_pages or 20
            
            if progress_callback:
                progress_callback(f"ğŸ”§ æ‰‹åŠ¨å‚æ•°: æ·±åº¦={max_depth}, é¡µæ•°={max_pages_per_level}")
        
        # è°ƒç”¨åŸæœ‰çš„çˆ¬å–æ–¹æ³•
        return self.crawl_with_depth(
            start_urls=start_urls,
            max_depth=max_depth,
            max_pages_per_level=max_pages_per_level,
            progress_callback=progress_callback
        )
    
    def _fetch_url_thread(self, url: str, timeout=15) -> Dict:
        """çº¿ç¨‹æ¨¡å¼çš„URLè·å–"""
        start_time = time.time()
        result = {
            'url': url,
            'success': False,
            'content': None,
            'title': None,
            'links': [],
            'response_time': 0,
            'error': None
        }
        
        try:
            self.stats['total_requests'] += 1
            
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = self.session.get(url, headers=headers, timeout=timeout)
            response_time = time.time() - start_time
            result['response_time'] = response_time
            self.stats['response_times'].append(response_time)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æå–æ ‡é¢˜
                title_tag = soup.find('title')
                result['title'] = title_tag.get_text().strip() if title_tag else url
                
                # æå–å†…å®¹
                for script in soup(["script", "style"]):
                    script.decompose()
                
                content = soup.get_text()
                lines = (line.strip() for line in content.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                content = ' '.join(chunk for chunk in chunks if chunk)
                
                result['content'] = content
                
                # æå–é“¾æ¥
                base_domain = urlparse(url).netloc
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    if href:
                        full_url = urljoin(url, href)
                        parsed = urlparse(full_url)
                        if parsed.netloc == base_domain:
                            result['links'].append(full_url)
                
                result['success'] = True
                self.stats['successful_requests'] += 1
            else:
                result['error'] = f"HTTP {response.status_code}"
                self.stats['failed_requests'] += 1
                
        except Exception as e:
            result['error'] = str(e)
            result['response_time'] = time.time() - start_time
            self.stats['failed_requests'] += 1
        
        # æ™ºèƒ½å»¶è¿Ÿ
        self._smart_delay(result['response_time'])
        return result
    
    def _smart_delay(self, response_time=None):
        """æ™ºèƒ½å»¶è¿Ÿç­–ç•¥"""
        if response_time:
            if response_time < 1.0:
                delay = self.base_delay * 0.5
            elif response_time > 3.0:
                delay = self.base_delay * 2.0
            else:
                delay = self.base_delay
        else:
            delay = self.base_delay
            
        delay += random.uniform(0, 0.5)
        delay = min(delay, self.max_delay)
        time.sleep(delay)
    
    def crawl_urls_concurrent(self, urls: List[str], 
                            progress_callback: Optional[Callable] = None,
                            max_pages: int = 50) -> List[Dict]:
        """å¹¶å‘çˆ¬å–URLåˆ—è¡¨"""
        if not urls:
            return []
        
        self.stats['start_time'] = time.time()
        results = []
        processed_urls = set()
        
        urls_to_process = urls[:max_pages]
        
        if self.use_processes:
            # å¤šè¿›ç¨‹æ¨¡å¼
            with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
                # å‡†å¤‡å‚æ•°
                args_list = []
                for url in urls_to_process:
                    if url not in processed_urls:
                        args_list.append((url, 15, self.user_agents, self.base_delay, self.max_delay))
                        processed_urls.add(url)
                
                # æäº¤ä»»åŠ¡
                future_to_url = {executor.submit(fetch_url_worker, args): args[0] for args in args_list}
                
                # æ”¶é›†ç»“æœ
                completed = 0
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        result = future.result()
                        results.append(result)
                        completed += 1
                        
                        # æ›´æ–°ç»Ÿè®¡ï¼ˆè¿›ç¨‹æ¨¡å¼éœ€è¦æ‰‹åŠ¨æ›´æ–°ï¼‰
                        if result['success']:
                            self.stats['successful_requests'] += 1
                        else:
                            self.stats['failed_requests'] += 1
                        self.stats['total_requests'] += 1
                        self.stats['response_times'].append(result['response_time'])
                        
                        if progress_callback:
                            progress = completed / len(future_to_url)
                            progress_callback(f"å·²å®Œæˆ {completed}/{len(future_to_url)} ä¸ªé¡µé¢ (è¿›ç¨‹æ¨¡å¼)", progress)
                            
                    except Exception as e:
                        error_result = {
                            'url': url,
                            'success': False,
                            'content': None,
                            'title': None,
                            'links': [],
                            'response_time': 0,
                            'error': str(e)
                        }
                        results.append(error_result)
                        completed += 1
                        self.stats['failed_requests'] += 1
                        self.stats['total_requests'] += 1
        else:
            # å¤šçº¿ç¨‹æ¨¡å¼
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_url = {}
                for url in urls_to_process:
                    if url not in processed_urls:
                        future = executor.submit(self._fetch_url_thread, url)
                        future_to_url[future] = url
                        processed_urls.add(url)
                
                completed = 0
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        result = future.result()
                        results.append(result)
                        completed += 1
                        
                        if progress_callback:
                            progress = completed / len(future_to_url)
                            progress_callback(f"å·²å®Œæˆ {completed}/{len(future_to_url)} ä¸ªé¡µé¢ (çº¿ç¨‹æ¨¡å¼)", progress)
                            
                    except Exception as e:
                        error_result = {
                            'url': url,
                            'success': False,
                            'content': None,
                            'title': None,
                            'links': [],
                            'response_time': 0,
                            'error': str(e)
                        }
                        results.append(error_result)
                        completed += 1
        
        return results
    
    def crawl_with_depth(self, start_urls: List[str], 
                        max_depth: int = 2,
                        max_pages_per_level: int = 20,
                        progress_callback: Optional[Callable] = None) -> List[Dict]:
        """æŒ‰æ·±åº¦å¹¶å‘çˆ¬å– - ä¿®å¤é€’å½’é€»è¾‘"""
        all_results = []
        current_urls = start_urls
        processed_urls = set()
        
        if progress_callback:
            progress_callback(f"ğŸš€ å¼€å§‹é€’å½’å¹¶å‘çˆ¬å–: æœ€å¤§æ·±åº¦={max_depth}, åŸºç¡€é¡µæ•°={max_pages_per_level}")
            for d in range(1, max_depth + 1):
                expected_pages = max_pages_per_level ** d
                progress_callback(f"   ç¬¬{d}å±‚é¢„è®¡: {expected_pages} é¡µ")
        
        for depth in range(1, max_depth + 1):
            if not current_urls:
                break
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ¯å±‚çš„é¡µé¢æ•°é‡åº”è¯¥æ˜¯ max_pages_per_level^depth
            current_layer_limit = max_pages_per_level ** depth
            
            # é™åˆ¶å½“å‰å±‚å¤„ç†çš„URLæ•°é‡
            current_urls = current_urls[:current_layer_limit]
                
            if progress_callback:
                mode_str = "è¿›ç¨‹" if self.use_processes else "çº¿ç¨‹"
                progress_callback(f"ğŸ“‚ ç¬¬{depth}å±‚å¼€å§‹: å¤„ç† {len(current_urls)} ä¸ªé“¾æ¥ (é™åˆ¶: {current_layer_limit}, {mode_str}æ¨¡å¼)")
            
            level_urls = [url for url in current_urls 
                         if url not in processed_urls]
            
            if not level_urls:
                if progress_callback:
                    progress_callback(f"âš ï¸ ç¬¬{depth}å±‚: æ— æ–°é“¾æ¥å¯å¤„ç†ï¼Œçˆ¬å–ç»“æŸ")
                break
            
            level_results = self.crawl_urls_concurrent(
                level_urls, 
                progress_callback,
                len(level_urls)  # å¤„ç†æ‰€æœ‰å½“å‰å±‚çš„URL
            )
            
            all_results.extend(level_results)
            processed_urls.update(level_urls)
            
            # æ”¶é›†ä¸‹ä¸€å±‚URL - ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ”¶é›†æ‰€æœ‰æœ‰æ•ˆé“¾æ¥ï¼Œä¸é™åˆ¶æ•°é‡
            next_urls = []
            for result in level_results:
                if result['success'] and result['links']:
                    next_urls.extend(result['links'])
            
            current_urls = list(set(next_urls) - processed_urls)
            
            if progress_callback:
                success_count = len([r for r in level_results if r['success']])
                progress_callback(f"ğŸ¯ ç¬¬{depth}å±‚å®Œæˆ: æˆåŠŸ {success_count} é¡µï¼Œå‘ç° {len(current_urls)} ä¸ªä¸‹çº§é“¾æ¥")
                if depth < max_depth and current_urls:
                    next_layer_limit = max_pages_per_level ** (depth + 1)
                    actual_next = min(len(current_urls), next_layer_limit)
                    progress_callback(f"ğŸ“Š é€’å½’ç»Ÿè®¡: ç¬¬{depth+1}å±‚å°†å¤„ç†å‰ {actual_next} ä¸ªé“¾æ¥")
                success_count = sum(1 for r in level_results if r['success'])
                progress_callback(f"ç¬¬{depth+1}å±‚å®Œæˆ: æˆåŠŸ {success_count}/{len(level_results)} é¡µï¼Œå‘ç° {len(current_urls)} ä¸ªæ–°é“¾æ¥")
        
        return all_results
    
    def get_stats(self) -> Dict:
        """è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯"""
        if self.stats['start_time']:
            elapsed_time = time.time() - self.stats['start_time']
        else:
            elapsed_time = 0
            
        avg_response_time = 0
        if self.stats['response_times']:
            avg_response_time = sum(self.stats['response_times']) / len(self.stats['response_times'])
        
        success_rate = 0
        if self.stats['total_requests'] > 0:
            success_rate = self.stats['successful_requests'] / self.stats['total_requests']
        
        pages_per_minute = 0
        if elapsed_time > 0:
            pages_per_minute = self.stats['successful_requests'] / (elapsed_time / 60)
        
        return {
            'total_requests': self.stats['total_requests'],
            'successful_requests': self.stats['successful_requests'],
            'failed_requests': self.stats['failed_requests'],
            'success_rate': success_rate,
            'elapsed_time': elapsed_time,
            'avg_response_time': avg_response_time,
            'pages_per_minute': pages_per_minute,
            'max_workers': self.max_workers,
            'mode': 'process' if self.use_processes else 'thread',
            'cpu_count': os.cpu_count()
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'start_time': None,
            'response_times': []
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•å¤šè¿›ç¨‹ vs å¤šçº¿ç¨‹çˆ¬å–æ€§èƒ½...")
    
    test_urls = [
        "https://www.runoob.com/",
        "https://docs.python.org/zh-cn/3/",
        "https://help.aliyun.com/"
    ]
    
    def progress_callback(message, progress=None):
        if progress:
            print(f"{message} ({progress:.1%})")
        else:
            print(message)
    
    # æµ‹è¯•å¤šè¿›ç¨‹æ¨¡å¼
    print("\nğŸ”„ æµ‹è¯•å¤šè¿›ç¨‹æ¨¡å¼:")
    process_crawler = ConcurrentCrawler(max_workers=3, use_processes=True)
    start_time = time.time()
    process_results = process_crawler.crawl_urls_concurrent(test_urls, progress_callback)
    process_time = time.time() - start_time
    process_stats = process_crawler.get_stats()
    
    print(f"è¿›ç¨‹æ¨¡å¼ç»“æœ: {len(process_results)}ä¸ªé¡µé¢, è€—æ—¶: {process_time:.2f}ç§’")
    print(f"æˆåŠŸç‡: {process_stats['success_rate']:.1%}, é€Ÿåº¦: {process_stats['pages_per_minute']:.1f}é¡µ/åˆ†é’Ÿ")
    
    # æµ‹è¯•å¤šçº¿ç¨‹æ¨¡å¼
    print("\nğŸ§µ æµ‹è¯•å¤šçº¿ç¨‹æ¨¡å¼:")
    thread_crawler = ConcurrentCrawler(max_workers=3, use_processes=False)
    start_time = time.time()
    thread_results = thread_crawler.crawl_urls_concurrent(test_urls, progress_callback)
    thread_time = time.time() - start_time
    thread_stats = thread_crawler.get_stats()
    
    print(f"çº¿ç¨‹æ¨¡å¼ç»“æœ: {len(thread_results)}ä¸ªé¡µé¢, è€—æ—¶: {thread_time:.2f}ç§’")
    print(f"æˆåŠŸç‡: {thread_stats['success_rate']:.1%}, é€Ÿåº¦: {thread_stats['pages_per_minute']:.1f}é¡µ/åˆ†é’Ÿ")
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"è¿›ç¨‹æ¨¡å¼: {process_time:.2f}ç§’, {process_stats['pages_per_minute']:.1f}é¡µ/åˆ†é’Ÿ")
    print(f"çº¿ç¨‹æ¨¡å¼: {thread_time:.2f}ç§’, {thread_stats['pages_per_minute']:.1f}é¡µ/åˆ†é’Ÿ")
    
    if process_time < thread_time:
        improvement = ((thread_time - process_time) / thread_time) * 100
        print(f"ğŸš€ è¿›ç¨‹æ¨¡å¼å¿« {improvement:.1f}%")
    else:
        improvement = ((process_time - thread_time) / process_time) * 100
        print(f"ğŸ§µ çº¿ç¨‹æ¨¡å¼å¿« {improvement:.1f}%")
        
    def _get_random_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    def _smart_delay(self, response_time=None):
        """æ™ºèƒ½å»¶è¿Ÿç­–ç•¥"""
        if response_time:
            # æ ¹æ®å“åº”æ—¶é—´è°ƒæ•´å»¶è¿Ÿ
            if response_time < 1.0:
                delay = self.base_delay * 0.5  # å¿«é€Ÿå“åº”ï¼Œå‡å°‘å»¶è¿Ÿ
            elif response_time > 3.0:
                delay = self.base_delay * 2.0  # æ…¢é€Ÿå“åº”ï¼Œå¢åŠ å»¶è¿Ÿ
            else:
                delay = self.base_delay
        else:
            delay = self.base_delay
            
        # æ·»åŠ éšæœºæ€§é¿å…è¢«æ£€æµ‹
        delay += random.uniform(0, 0.5)
        delay = min(delay, self.max_delay)
        
        time.sleep(delay)
    
    def _fetch_url(self, url: str, timeout=15) -> Dict:
        """è·å–å•ä¸ªURL"""
        start_time = time.time()
        result = {
            'url': url,
            'success': False,
            'content': None,
            'title': None,
            'links': [],
            'response_time': 0,
            'error': None
        }
        
        try:
            self.stats['total_requests'] += 1
            
            headers = self._get_random_headers()
            response = self.session.get(url, headers=headers, timeout=timeout)
            response_time = time.time() - start_time
            result['response_time'] = response_time
            self.stats['response_times'].append(response_time)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æå–æ ‡é¢˜
                title_tag = soup.find('title')
                result['title'] = title_tag.get_text().strip() if title_tag else url
                
                # æå–å†…å®¹
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                for script in soup(["script", "style"]):
                    script.decompose()
                
                # æå–ä¸»è¦å†…å®¹
                content = soup.get_text()
                # æ¸…ç†ç©ºç™½
                lines = (line.strip() for line in content.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                content = ' '.join(chunk for chunk in chunks if chunk)
                
                result['content'] = content
                
                # æå–é“¾æ¥
                base_domain = urlparse(url).netloc
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    if href:
                        full_url = urljoin(url, href)
                        parsed = urlparse(full_url)
                        if parsed.netloc == base_domain:  # åªä¿ç•™åŒåŸŸåé“¾æ¥
                            result['links'].append(full_url)
                
                result['success'] = True
                self.stats['successful_requests'] += 1
                
            else:
                result['error'] = f"HTTP {response.status_code}"
                self.stats['failed_requests'] += 1
                
        except Exception as e:
            result['error'] = str(e)
            result['response_time'] = time.time() - start_time
            self.stats['failed_requests'] += 1
        
        # æ™ºèƒ½å»¶è¿Ÿ
        self._smart_delay(result['response_time'])
        
        return result
    
    def crawl_urls_concurrent(self, urls: List[str], 
                            progress_callback: Optional[Callable] = None,
                            max_pages: int = 50) -> List[Dict]:
        """å¹¶å‘çˆ¬å–URLåˆ—è¡¨"""
        if not urls:
            return []
        
        self.stats['start_time'] = time.time()
        results = []
        processed_urls = set()
        
        # é™åˆ¶çˆ¬å–æ•°é‡
        urls_to_process = urls[:max_pages]
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_url = {}
            for url in urls_to_process:
                if url not in processed_urls:
                    future = executor.submit(self._fetch_url, url)
                    future_to_url[future] = url
                    processed_urls.add(url)
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed += 1
                    
                    if progress_callback:
                        progress = completed / len(future_to_url)
                        progress_callback(f"å·²å®Œæˆ {completed}/{len(future_to_url)} ä¸ªé¡µé¢", progress)
                        
                except Exception as e:
                    # å¤„ç†å¼‚å¸¸
                    error_result = {
                        'url': url,
                        'success': False,
                        'content': None,
                        'title': None,
                        'links': [],
                        'response_time': 0,
                        'error': str(e)
                    }
                    results.append(error_result)
                    completed += 1
        
        return results
    
    def crawl_with_depth(self, start_urls: List[str], 
                        max_depth: int = 2,
                        max_pages_per_level: int = 20,
                        progress_callback: Optional[Callable] = None) -> List[Dict]:
        """æŒ‰æ·±åº¦å¹¶å‘çˆ¬å– - ä¿®å¤é€’å½’é€»è¾‘"""
        all_results = []
        current_urls = start_urls
        processed_urls = set()
        
        if progress_callback:
            progress_callback(f"ğŸš€ å¼€å§‹é€’å½’å¹¶å‘çˆ¬å–: æœ€å¤§æ·±åº¦={max_depth}, åŸºç¡€é¡µæ•°={max_pages_per_level}")
            for d in range(1, max_depth + 1):
                expected_pages = max_pages_per_level ** d
                progress_callback(f"   ç¬¬{d}å±‚é¢„è®¡: {expected_pages} é¡µ")
        
        for depth in range(1, max_depth + 1):
            if not current_urls:
                break
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ¯å±‚çš„é¡µé¢æ•°é‡åº”è¯¥æ˜¯ max_pages_per_level^depth
            current_layer_limit = max_pages_per_level ** depth
            
            # é™åˆ¶å½“å‰å±‚å¤„ç†çš„URLæ•°é‡
            current_urls = current_urls[:current_layer_limit]
                
            if progress_callback:
                progress_callback(f"ğŸ“‚ ç¬¬{depth}å±‚å¼€å§‹: å¤„ç† {len(current_urls)} ä¸ªé“¾æ¥ (é™åˆ¶: {current_layer_limit})")
            
            # é™åˆ¶æ¯å±‚çš„URLæ•°é‡
            level_urls = [url for url in current_urls 
                         if url not in processed_urls]
            
            if not level_urls:
                if progress_callback:
                    progress_callback(f"âš ï¸ ç¬¬{depth}å±‚: æ— æ–°é“¾æ¥å¯å¤„ç†ï¼Œçˆ¬å–ç»“æŸ")
                break
            
            # å¹¶å‘çˆ¬å–å½“å‰å±‚
            level_results = self.crawl_urls_concurrent(
                level_urls, 
                progress_callback,
                len(level_urls)  # å¤„ç†æ‰€æœ‰å½“å‰å±‚çš„URL
            )
            
            all_results.extend(level_results)
            processed_urls.update(level_urls)
            
            # æ”¶é›†ä¸‹ä¸€å±‚çš„URL - ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ”¶é›†æ‰€æœ‰æœ‰æ•ˆé“¾æ¥ï¼Œä¸é™åˆ¶æ•°é‡
            next_urls = []
            for result in level_results:
                if result['success'] and result['links']:
                    next_urls.extend(result['links'])
            
            # å»é‡å¹¶å‡†å¤‡ä¸‹ä¸€å±‚
            current_urls = list(set(next_urls) - processed_urls)
            
            if progress_callback:
                success_count = sum(1 for r in level_results if r['success'])
                progress_callback(f"ğŸ¯ ç¬¬{depth}å±‚å®Œæˆ: æˆåŠŸ {success_count}/{len(level_results)} é¡µï¼Œå‘ç° {len(current_urls)} ä¸ªæ–°é“¾æ¥")
                if depth < max_depth and current_urls:
                    next_layer_limit = max_pages_per_level ** (depth + 1)
                    actual_next = min(len(current_urls), next_layer_limit)
                    progress_callback(f"ğŸ“Š é€’å½’ç»Ÿè®¡: ç¬¬{depth+1}å±‚å°†å¤„ç†å‰ {actual_next} ä¸ªé“¾æ¥")
        
        return all_results
    
    def get_stats(self) -> Dict:
        """è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯"""
        if self.stats['start_time']:
            elapsed_time = time.time() - self.stats['start_time']
        else:
            elapsed_time = 0
            
        avg_response_time = 0
        if self.stats['response_times']:
            avg_response_time = sum(self.stats['response_times']) / len(self.stats['response_times'])
        
        success_rate = 0
        if self.stats['total_requests'] > 0:
            success_rate = self.stats['successful_requests'] / self.stats['total_requests']
        
        pages_per_minute = 0
        if elapsed_time > 0:
            pages_per_minute = self.stats['successful_requests'] / (elapsed_time / 60)
        
        return {
            'total_requests': self.stats['total_requests'],
            'successful_requests': self.stats['successful_requests'],
            'failed_requests': self.stats['failed_requests'],
            'success_rate': success_rate,
            'elapsed_time': elapsed_time,
            'avg_response_time': avg_response_time,
            'pages_per_minute': pages_per_minute,
            'max_workers': self.max_workers
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'start_time': None,
            'response_times': []
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    crawler = ConcurrentCrawler(max_workers=3)
    
    def progress_callback(message, progress=None):
        if progress:
            print(f"{message} ({progress:.1%})")
        else:
            print(message)
    
    # æµ‹è¯•å¹¶å‘çˆ¬å–
    test_urls = [
        "https://www.runoob.com/",
        "https://docs.python.org/zh-cn/3/",
        "https://help.aliyun.com/"
    ]
    
    results = crawler.crawl_with_depth(
        test_urls, 
        max_depth=2, 
        max_pages_per_level=5,
        progress_callback=progress_callback
    )
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = crawler.get_stats()
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"æ€»è¯·æ±‚: {stats['total_requests']}")
    print(f"æˆåŠŸ: {stats['successful_requests']}")
    print(f"å¤±è´¥: {stats['failed_requests']}")
    print(f"æˆåŠŸç‡: {stats['success_rate']:.1%}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}ç§’")
    print(f"çˆ¬å–é€Ÿåº¦: {stats['pages_per_minute']:.1f}é¡µ/åˆ†é’Ÿ")
