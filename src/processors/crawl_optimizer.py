"""
æ™ºèƒ½çˆ¬å–ä¼˜åŒ–å™¨ - v2.4.1
è‡ªåŠ¨åˆ†æç½‘ç«™ç±»å‹å¹¶æ¨èæœ€ä½³çˆ¬å–å‚æ•°
"""

import re
from urllib.parse import urlparse
from typing import Dict, Tuple, List
import requests
from bs4 import BeautifulSoup

class CrawlOptimizer:
    """æ™ºèƒ½çˆ¬å–ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        # ç½‘ç«™ç±»å‹é…ç½® - è°ƒæ•´ä¸ºæ›´ç°å®çš„å‚æ•°
        self.site_configs = {
            "documentation": {
                "depth": 2,
                "pages_per_level": 5,  # é™ä½æ¯å±‚é¡µæ•°
                "description": "æŠ€æœ¯æ–‡æ¡£ç½‘ç«™ï¼Œå†…å®¹å±‚æ¬¡æ·±",
                "examples": ["docs.python.org", "developer.mozilla.org"]
            },
            "news": {
                "depth": 2,
                "pages_per_level": 5,  # é™ä½æ¯å±‚é¡µæ•°
                "description": "æ–°é—»ç½‘ç«™ï¼Œæ–‡ç« æ•°é‡å¤š",
                "examples": ["36kr.com", "techcrunch.com"]
            },
            "ecommerce": {
                "depth": 2,
                "pages_per_level": 5,  # é™ä½æ¯å±‚é¡µæ•°
                "description": "ç”µå•†ç½‘ç«™ï¼Œå•†å“é¡µé¢ä¸°å¯Œ",
                "examples": ["jd.com", "taobao.com"]
            },
            "blog": {
                "depth": 2,
                "pages_per_level": 5,  # é™ä½æ¯å±‚é¡µæ•°
                "description": "åšå®¢ç½‘ç«™ï¼Œæ–‡ç« åˆ†ç±»æ¸…æ™°",
                "examples": ["medium.com", "dev.to"]
            },
            "forum": {
                "depth": 2,
                "pages_per_level": 5,  # é™ä½æ¯å±‚é¡µæ•°
                "description": "è®ºå›ç½‘ç«™ï¼Œè®¨è®ºå±‚æ¬¡æ·±",
                "examples": ["stackoverflow.com", "reddit.com"]
            },
            "corporate": {
                "depth": 2,
                "pages_per_level": 5,  # å¤§å¹…é™ä½ä¼ä¸šå®˜ç½‘é¢„ä¼°
                "description": "ä¼ä¸šå®˜ç½‘ï¼Œç»“æ„ç›¸å¯¹ç®€å•",
                "examples": ["apple.com", "microsoft.com"]
            },
            "wiki": {
                "depth": 3,
                "pages_per_level": 30,  # é™ä½æ¯å±‚é¡µæ•°
                "description": "ç™¾ç§‘ç½‘ç«™ï¼Œå†…å®¹ä¸°å¯Œäº’è”",
                "examples": ["wikipedia.org", "baike.baidu.com"]
            }
        }
        
        # ç½‘ç«™ç±»å‹è¯†åˆ«è§„åˆ™
        self.type_patterns = {
            "documentation": [
                r"docs?\.", r"developer\.", r"api\.", r"reference\.",
                r"guide", r"tutorial", r"manual"
            ],
            "news": [
                r"news", r"tech", r"36kr", r"techcrunch", r"ithome",
                r"cnbeta", r"pingwest"
            ],
            "ecommerce": [
                r"shop", r"store", r"mall", r"buy", r"jd\.com",
                r"taobao", r"tmall", r"amazon"
            ],
            "blog": [
                r"blog", r"medium", r"dev\.to", r"csdn", r"jianshu"
            ],
            "forum": [
                r"forum", r"bbs", r"discuss", r"stackoverflow",
                r"reddit", r"zhihu"
            ],
            "wiki": [
                r"wiki", r"baike", r"encyclopedia"
            ]
        }
    
    def analyze_website(self, url: str) -> Dict:
        """åˆ†æç½‘ç«™å¹¶è¿”å›æ¨èé…ç½®"""
        try:
            # è§£æURL
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            path = parsed.path.lower()
            
            # è¯†åˆ«ç½‘ç«™ç±»å‹
            site_type = self._identify_site_type(domain, path)
            
            # è·å–æ¨èé…ç½®
            config = self.site_configs.get(site_type, self.site_configs["corporate"])
            
            # å°è¯•è·å–ç½‘ç«™ä¿¡æ¯
            site_info = self._get_site_info(url)
            
            # ä¿®æ­£é¢„ä¼°é€»è¾‘ - æ›´ç°å®çš„é¢„ä¼°
            realistic_estimate = self._calculate_realistic_estimate(
                config["depth"], 
                config["pages_per_level"], 
                site_type,
                site_info
            )
            
            return {
                "site_type": site_type,
                "recommended_depth": config["depth"],
                "recommended_pages": config["pages_per_level"],
                "description": config["description"],
                "estimated_pages": realistic_estimate,
                "confidence": site_info.get("confidence", 0.7),
                "site_info": site_info
            }
            
        except Exception as e:
            # é»˜è®¤é…ç½®
            return {
                "site_type": "unknown",
                "recommended_depth": 2,
                "recommended_pages": 20,
                "description": "æœªçŸ¥ç½‘ç«™ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®",
                "estimated_pages": 40,  # æ›´ç°å®çš„é»˜è®¤é¢„ä¼°
                "confidence": 0.5,
                "error": str(e)
            }
    
    def _calculate_realistic_estimate(self, depth: int, pages_per_level: int, 
                                    site_type: str, site_info: Dict) -> int:
        """è®¡ç®—æ›´ç°å®çš„é¡µé¢é¢„ä¼°"""
        
        # åŸºç¡€é¢„ä¼°ï¼šçº¿æ€§å¢é•¿è€ŒéæŒ‡æ•°å¢é•¿
        base_estimate = pages_per_level * depth
        
        # æ ¹æ®ç½‘ç«™ç±»å‹è°ƒæ•´ç³»æ•°
        type_multipliers = {
            "documentation": 2.5,  # æ–‡æ¡£ç½‘ç«™é“¾æ¥è¾ƒå¤š
            "news": 3.0,          # æ–°é—»ç½‘ç«™æ–‡ç« ä¸°å¯Œ
            "ecommerce": 4.0,     # ç”µå•†ç½‘ç«™å•†å“é¡µé¢å¤š
            "blog": 1.8,          # åšå®¢ç›¸å¯¹è¾ƒå°‘
            "forum": 3.5,         # è®ºå›è®¨è®ºå¤š
            "corporate": 1.2,     # ä¼ä¸šå®˜ç½‘é¡µé¢æœ‰é™
            "wiki": 2.8           # ç™¾ç§‘å†…å®¹ä¸°å¯Œ
        }
        
        multiplier = type_multipliers.get(site_type, 1.5)
        
        # æ ¹æ®å®é™…é“¾æ¥æ•°é‡è°ƒæ•´
        total_links = site_info.get("total_links", 50)
        if total_links > 100:
            multiplier *= 1.5
        elif total_links < 20:
            multiplier *= 0.6
        
        # è®¡ç®—ç°å®é¢„ä¼°
        realistic_estimate = int(base_estimate * multiplier)
        
        # è®¾ç½®åˆç†ä¸Šä¸‹é™
        min_estimate = max(depth * 5, 10)  # æœ€å°‘æ¯å±‚5é¡µ
        max_estimate = pages_per_level * depth * 10  # æœ€å¤š10å€
        
        return max(min_estimate, min(realistic_estimate, max_estimate))
    
    def _identify_site_type(self, domain: str, path: str) -> str:
        """è¯†åˆ«ç½‘ç«™ç±»å‹"""
        full_url = domain + path
        
        for site_type, patterns in self.type_patterns.items():
            for pattern in patterns:
                if re.search(pattern, full_url, re.IGNORECASE):
                    return site_type
        
        return "corporate"  # é»˜è®¤ç±»å‹
    
    def _get_site_info(self, url: str, timeout: int = 5) -> Dict:
        """è·å–ç½‘ç«™åŸºæœ¬ä¿¡æ¯"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=timeout)
            if response.status_code != 200:
                return {"confidence": 0.5}
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # åˆ†æé¡µé¢ç»“æ„
            nav_links = len(soup.find_all(['nav', 'menu']))
            total_links = len(soup.find_all('a', href=True))
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = min(0.9, 0.5 + (total_links / 100) * 0.4)
            
            return {
                "title": soup.title.string if soup.title else "Unknown",
                "nav_sections": nav_links,
                "total_links": total_links,
                "confidence": confidence
            }
            
        except Exception:
            return {"confidence": 0.5}
    
    def get_popular_sites(self) -> Dict[str, List[Dict]]:
        """è·å–çƒ­é—¨ç½‘ç«™é¢„è®¾"""
        popular_sites = {
            "æŠ€æœ¯æ–‡æ¡£": [
                {"name": "Pythonå®˜æ–¹æ–‡æ¡£", "url": "https://docs.python.org/", "type": "documentation"},
                {"name": "MDN Webæ–‡æ¡£", "url": "https://developer.mozilla.org/", "type": "documentation"},
                {"name": "Reactæ–‡æ¡£", "url": "https://react.dev/", "type": "documentation"},
            ],
            "æ–°é—»èµ„è®¯": [
                {"name": "36æ°ª", "url": "https://36kr.com/", "type": "news"},
                {"name": "TechCrunch", "url": "https://techcrunch.com/", "type": "news"},
                {"name": "ITä¹‹å®¶", "url": "https://www.ithome.com/", "type": "news"},
            ],
            "æŠ€æœ¯åšå®¢": [
                {"name": "Medium", "url": "https://medium.com/", "type": "blog"},
                {"name": "Dev.to", "url": "https://dev.to/", "type": "blog"},
                {"name": "CSDN", "url": "https://blog.csdn.net/", "type": "blog"},
            ],
            "é—®ç­”è®ºå›": [
                {"name": "Stack Overflow", "url": "https://stackoverflow.com/", "type": "forum"},
                {"name": "çŸ¥ä¹", "url": "https://www.zhihu.com/", "type": "forum"},
                {"name": "Reddit", "url": "https://www.reddit.com/", "type": "forum"},
            ]
        }
        
        return popular_sites
    
    def generate_crawl_report(self, results: Dict) -> str:
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report = f"""
ğŸ“Š **çˆ¬å–åˆ†ææŠ¥å‘Š**

ğŸŒ **ç½‘ç«™ç±»å‹**: {results['site_type']}
ğŸ“ **æè¿°**: {results['description']}
ğŸ¯ **æ¨èæ·±åº¦**: {results['recommended_depth']}å±‚
ğŸ“„ **æ¯å±‚é¡µæ•°**: {results['recommended_pages']}é¡µ
ğŸ“ˆ **é¢„ä¼°æ€»é¡µæ•°**: {results['estimated_pages']:,}é¡µ
ğŸ” **ç½®ä¿¡åº¦**: {results['confidence']:.1%}

ğŸ’¡ **ä¼˜åŒ–å»ºè®®**:
- æ ¹æ®ç½‘ç«™ç»“æ„è‡ªåŠ¨è°ƒæ•´å‚æ•°
- å»ºè®®ä½¿ç”¨æ¨èé…ç½®ä»¥è·å¾—æœ€ä½³æ•ˆæœ
- å¤§å‹ç½‘ç«™å»ºè®®åˆ†æ‰¹çˆ¬å–
        """
        
        return report.strip()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    optimizer = CrawlOptimizer()
    
    # æµ‹è¯•ç½‘ç«™åˆ†æ
    test_urls = [
        "https://docs.python.org/",
        "https://36kr.com/",
        "https://stackoverflow.com/"
    ]
    
    for url in test_urls:
        print(f"\nğŸ” åˆ†æç½‘ç«™: {url}")
        result = optimizer.analyze_website(url)
        print(optimizer.generate_crawl_report(result))
