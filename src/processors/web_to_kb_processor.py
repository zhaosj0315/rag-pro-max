"""ç½‘é¡µæŠ“å–åˆ°çŸ¥è¯†åº“æ„å»ºçš„å®Œæ•´æµç¨‹å¤„ç†å™¨"""

import os
import json
import requests
from typing import List, Dict, Optional, Callable
from datetime import datetime
from .web_crawler import WebCrawler
from ..kb.kb_manager import KBManager
from ..processors.index_builder import IndexBuilder
import streamlit as st


class WebToKBProcessor:
    """ç½‘é¡µæŠ“å–åˆ°çŸ¥è¯†åº“æ„å»ºçš„å®Œæ•´æµç¨‹å¤„ç†å™¨"""
    
    def __init__(self):
        # self.crawler å°†åœ¨æ¯æ¬¡ä»»åŠ¡æ‰§è¡Œæ—¶ç‹¬ç«‹åˆå§‹åŒ–
        self.kb_manager = KBManager()
        self.index_builder = IndexBuilder()
        
        # é¢„è®¾çŸ¥åç½‘ç«™ - æŒ‰ç±»åˆ«åˆ†ç»„
        self.preset_sites = {
            # ç™¾ç§‘ç±»ç½‘ç«™
            "ç»´åŸºç™¾ç§‘": {
                "base_url": "https://zh.wikipedia.org/wiki/",
                "search_url": "https://zh.wikipedia.org/wiki/Special:Search?search={keyword}",
                "description": "ä¸­æ–‡ç»´åŸºç™¾ç§‘ - å…¨çƒæœ€å¤§çš„ä¸­æ–‡ç™¾ç§‘å…¨ä¹¦",
                "category": "ç™¾ç§‘"
            },
            "ç™¾åº¦ç™¾ç§‘": {
                "base_url": "https://baike.baidu.com/item/",
                "search_url": "https://baike.baidu.com/search?word={keyword}",
                "description": "ç™¾åº¦ç™¾ç§‘ - ä¸­æ–‡ç™¾ç§‘çŸ¥è¯†å¹³å°",
                "category": "ç™¾ç§‘"
            },
            
            # åŒ»å­¦ä¸“ä¸šç½‘ç«™
            "ä¸é¦™å›­": {
                "base_url": "https://www.dxy.com/",
                "search_url": "https://www.dxy.com/search?q={keyword}",
                "description": "ä¸é¦™å›­ - ä¸“ä¸šåŒ»å­¦çŸ¥è¯†å¹³å°",
                "category": "åŒ»å­¦"
            },
            "å¥½å¤§å¤«åœ¨çº¿": {
                "base_url": "https://www.haodf.com/",
                "search_url": "https://www.haodf.com/search?kw={keyword}",
                "description": "å¥½å¤§å¤«åœ¨çº¿ - åŒ»ç–—å¥åº·å’¨è¯¢å¹³å°",
                "category": "åŒ»å­¦"
            },
            "æ˜¥é›¨åŒ»ç”Ÿ": {
                "base_url": "https://www.chunyuyisheng.com/",
                "search_url": "https://www.chunyuyisheng.com/search?q={keyword}",
                "description": "æ˜¥é›¨åŒ»ç”Ÿ - åœ¨çº¿åŒ»ç–—å¥åº·æœåŠ¡",
                "category": "åŒ»å­¦"
            },
            
            # é—®ç­”ç±»ç½‘ç«™
            "çŸ¥ä¹": {
                "base_url": "https://www.zhihu.com/",
                "search_url": "https://www.zhihu.com/search?type=content&q={keyword}",
                "description": "çŸ¥ä¹ - ä¸­æ–‡é—®ç­”ç¤¾åŒº",
                "category": "é—®ç­”"
            },
            
            # æŠ€æœ¯ç±»ç½‘ç«™
            "CSDN": {
                "base_url": "https://blog.csdn.net/",
                "search_url": "https://so.csdn.net/so/search?q={keyword}",
                "description": "CSDN - æŠ€æœ¯åšå®¢å¹³å° âš ï¸ ä»…é€‚ç”¨äºæŠ€æœ¯ç±»æœç´¢",
                "category": "æŠ€æœ¯"
            },
            "GitHub": {
                "base_url": "https://github.com/",
                "search_url": "https://github.com/search?q={keyword}&type=repositories",
                "description": "GitHub - ä»£ç æ‰˜ç®¡å¹³å° âš ï¸ ä»…é€‚ç”¨äºæŠ€æœ¯ç±»æœç´¢",
                "category": "æŠ€æœ¯"
            },
            "Stack Overflow": {
                "base_url": "https://stackoverflow.com/",
                "search_url": "https://stackoverflow.com/search?q={keyword}",
                "description": "Stack Overflow - ç¨‹åºå‘˜é—®ç­”ç¤¾åŒº âš ï¸ ä»…é€‚ç”¨äºæŠ€æœ¯ç±»æœç´¢",
                "category": "æŠ€æœ¯"
            }
        }
    
    def generate_kb_name_from_url(self, url: str, content_preview: str = "") -> str:
        """æ ¹æ®URLå’Œå†…å®¹é¢„è§ˆç”Ÿæˆæ™ºèƒ½çŸ¥è¯†åº“åç§°"""
        from urllib.parse import urlparse
        import re
        
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        
        # ä»URLè·¯å¾„æå–å…³é”®è¯
        path_parts = [p for p in parsed.path.split('/') if p and len(p) > 2]
        
        # ä»å†…å®¹é¢„è§ˆæå–å…³é”®è¯
        content_keywords = []
        if content_preview:
            # ç®€å•çš„å…³é”®è¯æå–
            words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', content_preview[:500])
            content_keywords = [w for w in words if len(w) >= 2][:5]
        
        # ç”Ÿæˆåç§°ç­–ç•¥
        if domain in ['zh.wikipedia.org', 'baike.baidu.com']:
            if path_parts:
                return f"ç™¾ç§‘_{path_parts[-1][:10]}"
        elif domain in ['zhihu.com']:
            return f"çŸ¥ä¹_{path_parts[-1][:10]}" if path_parts else "çŸ¥ä¹é—®ç­”"
        elif domain in ['csdn.net', 'blog.csdn.net']:
            return f"æŠ€æœ¯_{path_parts[-1][:10]}" if path_parts else "CSDNæŠ€æœ¯"
        elif domain in ['github.com']:
            if len(path_parts) >= 2:
                return f"é¡¹ç›®_{path_parts[1][:10]}"
        elif domain in ['stackoverflow.com']:
            return "ç¼–ç¨‹é—®ç­”"
        
        # é€šç”¨ç­–ç•¥
        if content_keywords:
            return f"ç½‘é¡µ_{content_keywords[0][:8]}"
        elif path_parts:
            return f"ç½‘é¡µ_{path_parts[-1][:8]}"
        else:
            domain_name = domain.split('.')[0]
            return f"{domain_name}_{datetime.now().strftime('%m%d')}"
    
    def search_preset_sites(self, keyword: str, sites: List[str] = None) -> List[Dict]:
        """åœ¨é¢„è®¾ç½‘ç«™ä¸­æœç´¢å…³é”®è¯ï¼Œè¿”å›æœç´¢ç»“æœURL"""
        if sites is None:
            sites = list(self.preset_sites.keys())
        
        results = []
        for site_name in sites:
            if site_name in self.preset_sites:
                site_info = self.preset_sites[site_name]
                search_url = site_info["search_url"].format(keyword=keyword)
                results.append({
                    "site": site_name,
                    "url": search_url,
                    "description": site_info["description"]
                })
        
        return results
    
    def crawl_and_build_kb(self, 
                          url: str = None,
                          keyword: str = None,
                          sites: List[str] = None,
                          max_depth: int = 1,
                          max_pages: int = 10,
                          kb_name: str = None,
                          auto_switch: bool = True,
                          status_callback: Optional[Callable] = None) -> Dict:
        """
        å®Œæ•´çš„ç½‘é¡µæŠ“å–åˆ°çŸ¥è¯†åº“æ„å»ºæµç¨‹
        
        Args:
            url: ç›´æ¥æŠ“å–çš„URLï¼ˆä¼˜å…ˆçº§é«˜ï¼‰
            keyword: æœç´¢å…³é”®è¯ï¼ˆå½“urlä¸ºç©ºæ—¶ä½¿ç”¨ï¼‰
            sites: è¦æœç´¢çš„ç½‘ç«™åˆ—è¡¨ï¼ˆå½“ä½¿ç”¨keywordæ—¶ï¼‰
            max_depth: æŠ“å–æ·±åº¦
            max_pages: æœ€å¤§é¡µé¢æ•°
            kb_name: æŒ‡å®šçŸ¥è¯†åº“åç§°ï¼ˆå¯é€‰ï¼‰
            auto_switch: æ˜¯å¦è‡ªåŠ¨åˆ‡æ¢åˆ°æ–°çŸ¥è¯†åº“
            status_callback: çŠ¶æ€å›è°ƒå‡½æ•°
        
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        try:
            # æ¯æ¬¡æ‰§è¡Œä½¿ç”¨ç‹¬ç«‹çš„æŠ“å–å™¨å’Œè¾“å‡ºç›®å½•ï¼Œé˜²æ­¢å†…å®¹æ··æ·†
            timestamp_dir = datetime.now().strftime('%Y%m%d_%H%M%S')
            unique_output_dir = os.path.join("temp_uploads", f"web_crawl_proc_{timestamp_dir}")
            crawler = WebCrawler(output_dir=unique_output_dir)
            
            crawled_files = []
            
            if url:
                # ç›´æ¥æŠ“å–URL
                if status_callback:
                    status_callback(f"ğŸŒ å¼€å§‹æŠ“å–ç½‘é¡µ: {url}")
                
                crawled_files = crawler.crawl_advanced(
                    start_url=url,
                    max_depth=max_depth,
                    max_pages=max_pages,
                    status_callback=status_callback
                )
                
                # ç”ŸæˆçŸ¥è¯†åº“åç§°
                if not kb_name:
                    # è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å†…å®¹ä½œä¸ºé¢„è§ˆ
                    content_preview = ""
                    if crawled_files:
                        try:
                            with open(crawled_files[0], 'r', encoding='utf-8') as f:
                                content_preview = f.read()[:1000]
                        except:
                            pass
                    kb_name = self.generate_kb_name_from_url(url, content_preview)
            
            elif keyword:
                # å…³é”®è¯æœç´¢æ¨¡å¼
                if not sites:
                    sites = ["ç»´åŸºç™¾ç§‘", "ç™¾åº¦ç™¾ç§‘"]  # é»˜è®¤æœç´¢ç½‘ç«™
                
                if status_callback:
                    status_callback(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
                
                search_results = self.search_preset_sites(keyword, sites)
                
                # æŠ“å–æœç´¢ç»“æœ
                for result in search_results[:2]:  # é™åˆ¶æœç´¢ç»“æœæ•°é‡
                    if status_callback:
                        status_callback(f"ğŸŒ æŠ“å– {result['site']}: {result['url']}")
                    
                    try:
                        files = crawler.crawl_advanced(
                            start_url=result['url'],
                            max_depth=2,  # æœç´¢ç»“æœéœ€è¦æŠ“å–2å±‚
                            max_pages=max_pages // len(search_results),
                            status_callback=status_callback
                        )
                        crawled_files.extend(files)
                    except Exception as e:
                        if status_callback:
                            status_callback(f"âŒ {result['site']} æŠ“å–å¤±è´¥: {e}")
                        continue
                
                # ç”ŸæˆçŸ¥è¯†åº“åç§°
                if not kb_name:
                    kb_name = f"æœç´¢_{keyword}_{datetime.now().strftime('%m%d')}"
            
            else:
                return {"success": False, "message": "å¿…é¡»æä¾›URLæˆ–å…³é”®è¯"}
            
            if not crawled_files:
                return {"success": False, "message": "æ²¡æœ‰æˆåŠŸæŠ“å–åˆ°ä»»ä½•å†…å®¹"}
            
            if status_callback:
                status_callback(f"ğŸ“š åˆ›å»ºçŸ¥è¯†åº“: {kb_name}")
            
            # åˆ›å»ºçŸ¥è¯†åº“
            success, message = self.kb_manager.create(kb_name)
            if not success:
                # å¦‚æœçŸ¥è¯†åº“å·²å­˜åœ¨ï¼Œç”Ÿæˆæ–°åç§°
                kb_name = f"{kb_name}_{datetime.now().strftime('%H%M')}"
                success, message = self.kb_manager.create(kb_name)
                if not success:
                    return {"success": False, "message": f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {message}"}
            
            if status_callback:
                status_callback(f"ğŸ”¨ æ„å»ºçŸ¥è¯†åº“ç´¢å¼•...")
            
            # æ„å»ºçŸ¥è¯†åº“ç´¢å¼•
            try:
                if status_callback:
                    status_callback("ğŸ”¨ æ­£åœ¨æ„å»ºç´¢å¼•...")
                
                # è·å–çŸ¥è¯†åº“è·¯å¾„
                kb_info = self.kb_manager.get_info(kb_name)
                if not kb_info:
                    return {"success": False, "message": f"æ— æ³•è·å–çŸ¥è¯†åº“ä¿¡æ¯: {kb_name}"}
                
                kb_path = kb_info['path']
                
                # è·å–å½“å‰åµŒå…¥æ¨¡å‹
                from llama_index.core import Settings
                
                # åˆå§‹åŒ–ç´¢å¼•æ„å»ºå™¨
                # æ³¨æ„ï¼šè¿™é‡Œé‡æ–°åˆå§‹åŒ–æ˜¯ä¸ºäº†ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
                self.index_builder = IndexBuilder(
                    kb_name=kb_name,
                    persist_dir=kb_path,
                    embed_model=Settings.embed_model
                )
                
                # æ‰§è¡Œæ„å»º
                build_result = self.index_builder.build(
                    source_path=unique_output_dir,
                    action_mode="NEW",
                    status_callback=lambda _, msg, *args: status_callback(f"ğŸ”¨ {msg}" if isinstance(msg, str) else "å¤„ç†ä¸­...")
                )
                
                if build_result.success:
                    # å¦‚æœåœ¨Streamlitç¯å¢ƒä¸­ï¼Œè‡ªåŠ¨åˆ‡æ¢çŸ¥è¯†åº“
                    if auto_switch and 'st' in globals():
                        st.session_state.selected_kb = kb_name
                        if status_callback:
                            status_callback(f"âœ… å·²è‡ªåŠ¨åˆ‡æ¢åˆ°çŸ¥è¯†åº“: {kb_name}")
                    
                    return {
                        "success": True,
                        "kb_name": kb_name,
                        "files_count": build_result.file_count,
                        "doc_count": build_result.doc_count,
                        "files": crawled_files,
                        "message": f"âœ… æˆåŠŸåˆ›å»ºçŸ¥è¯†åº“ '{kb_name}'ï¼ŒåŒ…å« {build_result.file_count} ä¸ªæ–‡ä»¶ ({build_result.doc_count} ä¸ªç‰‡æ®µ)"
                    }
                else:
                    return {"success": False, "message": f"ç´¢å¼•æ„å»ºå¤±è´¥: {build_result.error}"}
                
            except Exception as e:
                return {"success": False, "message": f"æ„å»ºç´¢å¼•å¤±è´¥: {e}"}
                
        except Exception as e:
            return {"success": False, "message": f"å¤„ç†å¤±è´¥: {e}"}
    
    def get_preset_sites(self) -> Dict:
        """è·å–é¢„è®¾ç½‘ç«™åˆ—è¡¨"""
        return self.preset_sites
    
    def recommend_sites_for_keyword(self, keyword: str) -> List[str]:
        """æ ¹æ®å…³é”®è¯æ™ºèƒ½æ¨èåˆé€‚çš„ç½‘ç«™"""
        from src.services.configurable_industry_service import get_configurable_industry_service
        
        # ä½¿ç”¨æ–°çš„å¯é…ç½®æ¨èç³»ç»Ÿ
        service = get_configurable_industry_service()
        return service.recommend_sites_for_keyword(keyword)
    
    def generate_suggestions_for_crawl(self, kb_name: str, crawl_url: str, saved_files: List[str]) -> List[str]:
        """ä¸ºç½‘é¡µæŠ“å–ç”Ÿæˆæ¨èé—®é¢˜"""
        from src.chat.unified_suggestion_engine import get_unified_suggestion_engine
        
        # è¯»å–æŠ“å–å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
        context = ""
        for file_path in saved_files[:3]:  # åªè¯»å–å‰3ä¸ªæ–‡ä»¶
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    context += content[:1000] + "\n"  # æ¯ä¸ªæ–‡ä»¶å–å‰1000å­—ç¬¦
            except:
                continue
        
        # ä½¿ç”¨ç»Ÿä¸€æ¨èå¼•æ“
        engine = get_unified_suggestion_engine(kb_name)
        return engine.generate_suggestions(
            context=context,
            source_type='web_crawl',
            metadata={'url': crawl_url, 'files': saved_files},
            num_questions=4
        )
    
    def add_preset_site(self, name: str, base_url: str, search_url: str, description: str):
        """æ·»åŠ é¢„è®¾ç½‘ç«™"""
        self.preset_sites[name] = {
            "base_url": base_url,
            "search_url": search_url,
            "description": description
        }
