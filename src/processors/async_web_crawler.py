"""
å¼‚æ­¥å¹¶å‘ç½‘é¡µçˆ¬è™« - æ€§èƒ½æå‡10å€+
æ”¯æŒå¹¶å‘è¯·æ±‚ã€æ™ºèƒ½é™æµã€æ–­ç‚¹ç»­ä¼ 
"""

import asyncio
import aiohttp
import aiofiles
import json
import time
import hashlib
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from typing import List, Dict, Set, Optional, Callable
import urllib.robotparser
from pathlib import Path
import logging

# ğŸ”¥ æ–°å¢ï¼šå¯¼å…¥æ™ºèƒ½ä¼˜åŒ–å™¨
from .crawl_optimizer import CrawlOptimizer
from src.utils.file_system_utils import set_where_from_metadata

class AsyncWebCrawler:
    def __init__(self, max_concurrent=10, delay_range=(0.5, 2.0), ignore_robots=False):
        self.max_concurrent = max_concurrent
        self.delay_range = delay_range
        self.ignore_robots = ignore_robots  # æ˜¯å¦å¿½ç•¥robots.txt
        self.session = None
        self.visited_urls: Set[str] = set()
        self.failed_urls: Set[str] = set()
        self.content_hashes: Set[str] = set()  # å†…å®¹å»é‡
        self.robots_cache: Dict[str, bool] = {}
        
        # ğŸ”¥ æ–°å¢ï¼šæ™ºèƒ½ä¼˜åŒ–å™¨
        self.optimizer = CrawlOptimizer()
        
        # çŠ¶æ€æŒä¹…åŒ– - ä½¿ç”¨å”¯ä¸€æ–‡ä»¶åé¿å…å†²çª
        import time
        import os
        self.state_file = os.path.join("temp_uploads", f"crawler_state_{int(time.time())}.json")
        self.semaphore = None
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        connector = aiohttp.TCPConnector(
            limit=100,
            limit_per_host=20,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
        )
        
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """æ¸…ç†èµ„æº"""
        if self.session:
            await self.session.close()
    
    async def save_state(self):
        """ä¿å­˜çˆ¬å–çŠ¶æ€"""
        state = {
            'visited_urls': list(self.visited_urls),
            'failed_urls': list(self.failed_urls),
            'content_hashes': list(self.content_hashes),
            'timestamp': time.time()
        }
        
        async with aiofiles.open(self.state_file, 'w') as f:
            await f.write(json.dumps(state, indent=2))
    
    async def load_state(self):
        """åŠ è½½çˆ¬å–çŠ¶æ€"""
        try:
            async with aiofiles.open(self.state_file, 'r') as f:
                content = await f.read()
                state = json.loads(content)
                
            self.visited_urls = set(state.get('visited_urls', []))
            self.failed_urls = set(state.get('failed_urls', []))
            self.content_hashes = set(state.get('content_hashes', []))
            
            return True
        except FileNotFoundError:
            return False
    
    async def check_robots_txt(self, url: str) -> bool:
        """æ£€æŸ¥robots.txtåˆè§„æ€§ - å®½æ¾æ¨¡å¼"""
        domain = urlparse(url).netloc
        
        if domain in self.robots_cache:
            return self.robots_cache[domain]
        
        robots_url = f"https://{domain}/robots.txt"
        
        try:
            async with self.session.get(robots_url) as response:
                if response.status == 200:
                    robots_content = await response.text()
                    # æ›´å®½æ¾çš„æ£€æŸ¥ - åªé˜»æ­¢æ˜ç¡®çš„ "Disallow: /"
                    lines = robots_content.lower().split('\n')
                    for line in lines:
                        line = line.strip()
                        if line == 'disallow: /' and 'user-agent: *' in robots_content.lower():
                            self.robots_cache[domain] = False
                            return False
        except:
            pass
        
        # é»˜è®¤å…è®¸è®¿é—®
        self.robots_cache[domain] = True
        return True
    
    def content_fingerprint(self, text: str) -> str:
        """ç”Ÿæˆå†…å®¹æŒ‡çº¹ç”¨äºå»é‡"""
        # æ¸…ç†æ–‡æœ¬å¹¶ç”Ÿæˆå“ˆå¸Œ
        cleaned = ''.join(text.split()).lower()
        return hashlib.md5(cleaned.encode()).hexdigest()
    
    def is_duplicate_content(self, text: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤"""
        fingerprint = self.content_fingerprint(text)
        if fingerprint in self.content_hashes:
            return True
        self.content_hashes.add(fingerprint)
        return False
    
    async def fetch_with_retry(self, url: str, max_retries=3) -> Optional[str]:
        """å¸¦é‡è¯•çš„å¼‚æ­¥è¯·æ±‚"""
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            for attempt in range(max_retries):
                try:
                    # æ™ºèƒ½å»¶è¿Ÿ
                    if attempt > 0:
                        delay = self.delay_range[1] * (2 ** attempt)
                        await asyncio.sleep(min(delay, 10))
                    else:
                        await asyncio.sleep(
                            __import__('random').uniform(*self.delay_range)
                        )
                    
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            content = await response.text()
                            return content
                        elif response.status == 429:  # é™æµ
                            await asyncio.sleep(5)
                            continue
                        else:
                            break
                            
                except asyncio.TimeoutError:
                    continue
                except Exception as e:
                    if attempt == max_retries - 1:
                        self.failed_urls.add(url)
                    continue
            
            return None
    
    def extract_links(self, html_content: str, base_url: str, max_links: int = 8) -> List[str]:
        """æå–é“¾æ¥"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            links = []
            base_domain = urlparse(base_url).netloc
            
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if not href:
                    continue
                
                # æ„å»ºå®Œæ•´URL
                full_url = urljoin(base_url, href)
                parsed = urlparse(full_url)
                
                # åªçˆ¬å–åŒåŸŸå
                if parsed.netloc == base_domain and full_url not in self.visited_urls:
                    links.append(full_url)
                
                if len(links) >= max_links:
                    break
            
            return links
        except:
            return []
    
    def extract_content(self, html_content: str) -> str:
        """æå–é¡µé¢å†…å®¹"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
            for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
                tag.decompose()
            
            # ä¼˜å…ˆæå–ä¸»è¦å†…å®¹
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
            
            if main_content:
                text = main_content.get_text(separator=' ', strip=True)
            else:
                text = soup.get_text(separator=' ', strip=True)
            
            # æ¸…ç†æ–‡æœ¬
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            return '\n'.join(lines)
        except:
            return ""
    
    async def crawl_url(self, url: str, status_callback: Optional[Callable] = None, ignore_robots: bool = False) -> Optional[Dict]:
        """çˆ¬å–å•ä¸ªURL"""
        if status_callback:
            status_callback(f"ğŸ” å¼€å§‹å¤„ç†URL: {url}")
            
        if url in self.visited_urls:
            if status_callback:
                status_callback(f"â­ï¸ URLå·²è®¿é—®ï¼Œè·³è¿‡: {url}")
            return None
            
        if url in self.failed_urls:
            if status_callback:
                status_callback(f"â­ï¸ URLä¹‹å‰å¤±è´¥ï¼Œè·³è¿‡: {url}")
            return None
        
        # æ£€æŸ¥robots.txtï¼ˆå¯é€‰ï¼‰
        if not ignore_robots and not await self.check_robots_txt(url):
            if status_callback:
                status_callback(f"ğŸš« robots.txtç¦æ­¢è®¿é—®: {url}")
            return None
        
        if status_callback:
            status_callback(f"ğŸ”„ å¼‚æ­¥çˆ¬å–: {url}")
        
        html_content = await self.fetch_with_retry(url)
        if not html_content:
            if status_callback:
                status_callback(f"âŒ è·å–HTMLå¤±è´¥: {url}")
            return None
        
        # æå–å†…å®¹
        content = self.extract_content(html_content)
        
        # è°ƒè¯•ä¿¡æ¯
        if status_callback:
            status_callback(f"ğŸ“Š HTMLé•¿åº¦: {len(html_content)}, æå–å†…å®¹é•¿åº¦: {len(content)}")
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–å¤ªçŸ­
        if len(content.strip()) < 100:
            if status_callback:
                status_callback(f"âš ï¸ å†…å®¹å¤ªçŸ­ï¼Œè·³è¿‡: {url} (é•¿åº¦: {len(content)})")
            return None
        
        # å†…å®¹å»é‡æ£€æŸ¥
        if self.is_duplicate_content(content):
            if status_callback:
                status_callback(f"ğŸ”„ è·³è¿‡é‡å¤å†…å®¹: {url}")
            return None
        
        # æå–æ ‡é¢˜
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            title = soup.title.string if soup.title else "No Title"
            title = title.strip()
        except:
            title = "No Title"
        
        self.visited_urls.add(url)
        
        if status_callback:
            status_callback(f"âœ… å·²çˆ¬å–: {title} ({len(content)} å­—ç¬¦)")
        
        return {
            'url': url,
            'title': title,
            'content': content,
            'html': html_content,
            'timestamp': time.time()
        }
    
    def get_smart_recommendations(self, url: str) -> Dict:
        """ğŸ”¥ æ–°å¢ï¼šè·å–æ™ºèƒ½çˆ¬å–æ¨èå‚æ•°"""
        return self.optimizer.analyze_website(url)

    async def crawl_with_smart_params(self, 
                                     start_url: str,
                                     use_smart_params: bool = True,
                                     manual_depth: Optional[int] = None,
                                     manual_pages: Optional[int] = None,
                                     output_dir: str = "crawled_data",
                                     status_callback: Optional[Callable] = None) -> List[str]:
        """ğŸ”¥ æ–°å¢ï¼šä½¿ç”¨æ™ºèƒ½å‚æ•°æ¨èçš„å¼‚æ­¥çˆ¬å–æ–¹æ³•"""
        
        if use_smart_params:
            # è·å–æ™ºèƒ½æ¨è
            recommendations = self.get_smart_recommendations(start_url)
            
            if status_callback:
                status_callback("ğŸ§  æ™ºèƒ½åˆ†æç½‘ç«™...")
                status_callback(f"ğŸ“Š ç½‘ç«™ç±»å‹: {recommendations['site_type']}")
                status_callback(f"ğŸ“ æè¿°: {recommendations['description']}")
                status_callback(f"ğŸ¯ æ¨èæ·±åº¦: {recommendations['recommended_depth']}å±‚")
                status_callback(f"ğŸ“„ æ¨èé¡µæ•°: {recommendations['recommended_pages']}é¡µ/å±‚")
                status_callback(f"ğŸ“ˆ é¢„ä¼°æ€»é¡µæ•°: {recommendations['estimated_pages']:,}é¡µ")
                status_callback(f"ğŸ” ç½®ä¿¡åº¦: {recommendations['confidence']:.1%}")
            
            # ä½¿ç”¨æ¨èå‚æ•°ï¼ˆå¯è¢«æ‰‹åŠ¨å‚æ•°è¦†ç›–ï¼‰
            max_depth = manual_depth or recommendations['recommended_depth']
            max_pages_per_level = manual_pages or recommendations['recommended_pages']
            
            if status_callback:
                status_callback(f"âš™ï¸ æœ€ç»ˆå‚æ•°: æ·±åº¦={max_depth}, é¡µæ•°={max_pages_per_level}")
        else:
            # ä½¿ç”¨æ‰‹åŠ¨å‚æ•°æˆ–é»˜è®¤å€¼
            max_depth = manual_depth or 3
            max_pages_per_level = manual_pages or 8
            
            if status_callback:
                status_callback(f"ğŸ”§ æ‰‹åŠ¨å‚æ•°: æ·±åº¦={max_depth}, é¡µæ•°={max_pages_per_level}")
        
        # è°ƒç”¨åŸæœ‰çš„çˆ¬å–æ–¹æ³•
        return await self.crawl_recursive(
            start_url=start_url,
            max_depth=max_depth,
            max_pages_per_level=max_pages_per_level,
            output_dir=output_dir,
            status_callback=status_callback
        )
    
    async def crawl_recursive(
        self, 
        start_url: str, 
        max_depth: int = 3, 
        max_pages_per_level: int = 8,
        output_dir: str = "crawled_data",
        status_callback: Optional[Callable] = None
    ) -> List[str]:
        """å¼‚æ­¥é€’å½’çˆ¬å– - ä¿®å¤é€’å½’é€»è¾‘"""
        
        # åŠ è½½ä¹‹å‰çš„çŠ¶æ€
        await self.load_state()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        current_level = [start_url]
        saved_files = []
        
        if status_callback:
            status_callback(f"ğŸš€ å¼€å§‹å¼‚æ­¥é€’å½’çˆ¬å–: {start_url}")
            status_callback(f"ğŸ“Š é€’å½’å‚æ•°: æœ€å¤§æ·±åº¦={max_depth}, åŸºç¡€é¡µæ•°={max_pages_per_level}, å¹¶å‘={self.max_concurrent}")
            for d in range(1, max_depth + 1):
                expected_pages = max_pages_per_level ** d
                status_callback(f"   ç¬¬{d}å±‚é¢„è®¡: {expected_pages} é¡µ")
        
        for depth in range(1, max_depth + 1):
            if not current_level:
                break
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ¯å±‚çš„é¡µé¢æ•°é‡åº”è¯¥æ˜¯ max_pages_per_level^depth
            current_layer_limit = max_pages_per_level ** depth
            
            # é™åˆ¶å½“å‰å±‚å¤„ç†çš„URLæ•°é‡
            current_level = current_level[:current_layer_limit]
            
            if status_callback:
                status_callback(f"ğŸ“‚ ç¬¬{depth}å±‚: å¹¶å‘å¤„ç† {len(current_level)} ä¸ªURL (é™åˆ¶: {current_layer_limit})")
            
            # å¹¶å‘çˆ¬å–å½“å‰å±‚çº§çš„æ‰€æœ‰URL
            tasks = [self.crawl_url(url, status_callback, ignore_robots=self.ignore_robots) for url in current_level]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            next_level = []
            level_success = 0
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    if status_callback:
                        status_callback(f"âŒ çˆ¬å–å¤±è´¥: {current_level[i]} - {result}")
                    continue
                
                if result is None:
                    if status_callback:
                        status_callback(f"âš ï¸ çˆ¬å–è¿”å›None: {current_level[i]}")
                    continue
                
                level_success += 1
                
                # ä¿å­˜æ–‡ä»¶
                title = result.get('title', '').strip()
                if title:
                    # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦
                    safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()
                    safe_title = safe_title.replace(' ', '_')[:50]  # é™åˆ¶é•¿åº¦
                    filename = f"{safe_title}_{len(saved_files)+1:03d}.txt"
                else:
                    filename = f"page_{len(saved_files)+1}_{int(time.time())}.txt"
                
                filepath = output_path / filename
                
                async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                    await f.write(f"URL: {result['url']}\n")
                    await f.write(f"Title: {result['title']}\n")
                    await f.write(f"Timestamp: {result['timestamp']}\n")
                    await f.write(f"Content Length: {len(result['content'])}\n")
                    await f.write(f"\n{result['content']}")
                
                # ä¸ºæ–‡ä»¶è®¾ç½® macOS ä¸‹è½½æ¥æºå…ƒæ•°æ®
                set_where_from_metadata(str(filepath), result['url'])
                
                saved_files.append(str(filepath))
                
                # å¦‚æœè¿˜æ²¡åˆ°æœ€å¤§æ·±åº¦ï¼Œæå–ä¸‹ä¸€çº§é“¾æ¥
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæå–æ‰€æœ‰æœ‰æ•ˆé“¾æ¥ï¼Œä¸é™åˆ¶æ•°é‡
                if depth < max_depth:
                    links = self.extract_links(result['html'], result['url'])  # ç§»é™¤max_pages_per_levelå‚æ•°
                    next_level.extend(links)
            
            # å»é‡å¹¶å‡†å¤‡ä¸‹ä¸€å±‚
            current_level = list(set(next_level))
            
            if status_callback:
                status_callback(f"ğŸ¯ ç¬¬{depth}å±‚å®Œæˆ: æˆåŠŸ {level_success} é¡µï¼Œå‘ç° {len(current_level)} ä¸ªä¸‹çº§é“¾æ¥")
                if depth < max_depth and current_level:
                    next_layer_limit = max_pages_per_level ** (depth + 1)
                    actual_next = min(len(current_level), next_layer_limit)
                    status_callback(f"ğŸ“Š é€’å½’ç»Ÿè®¡: ç¬¬{depth+1}å±‚å°†å¤„ç†å‰ {actual_next} ä¸ªé“¾æ¥")
            
            # å®šæœŸä¿å­˜çŠ¶æ€
            await self.save_state()
        
        if status_callback:
            status_callback(f"ğŸ‰ å¼‚æ­¥çˆ¬å–å®Œæˆï¼è·å– {len(saved_files)} ä¸ªé¡µé¢")
            status_callback(f"ğŸ“ˆ ç»Ÿè®¡: è®¿é—® {len(self.visited_urls)} ä¸ªURLï¼Œå¤±è´¥ {len(self.failed_urls)} ä¸ª")
        
        return saved_files

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    async with AsyncWebCrawler(max_concurrent=20) as crawler:
        def progress_callback(message):
            print(f"[{time.strftime('%H:%M:%S')}] {message}")
        
        files = await crawler.crawl_recursive(
            start_url="https://docs.python.org/",
            max_depth=3,
            max_pages_per_level=10,
            output_dir="async_crawled_data",
            status_callback=progress_callback
        )
        
        print(f"çˆ¬å–å®Œæˆï¼Œä¿å­˜äº† {len(files)} ä¸ªæ–‡ä»¶")

if __name__ == "__main__":
    asyncio.run(main())
