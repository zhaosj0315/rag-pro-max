"""
èŠå¤©å¼•æ“Ž
Stage 7.1 - é—®ç­”å¤„ç†æ ¸å¿ƒé€»è¾‘
"""

import time
import re
from typing import Dict, List, Optional, Any
import streamlit as st

from src.logging import LogManager
logger = LogManager()
from src.utils.parallel_executor import ParallelExecutor
from src.utils.parallel_tasks import process_node_worker


class ChatEngine:
    """èŠå¤©å¼•æ“Ž - å¤„ç†é—®ç­”æµç¨‹"""
    
    def __init__(self, query_engine, kb_name: str):
        """
        åˆå§‹åŒ–èŠå¤©å¼•æ“Ž
        
        Args:
            query_engine: LlamaIndex æŸ¥è¯¢å¼•æ“Ž
            kb_name: çŸ¥è¯†åº“åç§°
        """
        self.query_engine = query_engine
        self.kb_name = kb_name
        self.executor = ParallelExecutor()
    
    def process_question(
        self, 
        question: str,
        llm_model: str,
        quoted_text: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            llm_model: LLM æ¨¡åž‹åç§°
            quoted_text: å¼•ç”¨çš„æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            åŒ…å«å›žç­”ã€æ¥æºã€ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        # æž„å»ºæœ€ç»ˆ prompt
        final_prompt = question
        if quoted_text:
            if len(quoted_text) > 2000:
                quoted_text = quoted_text[:2000] + "...(å·²æˆªæ–­)"
            final_prompt = f"åŸºäºŽä»¥ä¸‹å¼•ç”¨å†…å®¹ï¼š\n> {quoted_text}\n\næˆ‘çš„é—®é¢˜æ˜¯ï¼š{question}"
            logger.info("ðŸ“Œ å·²åº”ç”¨å¼•ç”¨å†…å®¹")
        
        # è®°å½•æ—¥å¿—
        logger.separator("çŸ¥è¯†åº“æŸ¥è¯¢")
        logger.start_operation("æŸ¥è¯¢", f"çŸ¥è¯†åº“: {self.kb_name}")
        logger.log_user_question(final_prompt, kb_name=self.kb_name)
        
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # æ˜¾ç¤ºæ£€ç´¢å¢žå¼ºåŠŸèƒ½
        enhancements = []
        if st.session_state.get('enable_bm25', False):
            enhancements.append("BM25æ··åˆæ£€ç´¢")
        if st.session_state.get('enable_rerank', False):
            enhancements.append("Re-rankingé‡æŽ’åº")
        
        if enhancements:
            enhancement_str = " + ".join(enhancements)
            logger.info(f"ðŸŽ¯ æ£€ç´¢å¢žå¼º: {enhancement_str}")
            logger.log("æŸ¥è¯¢å¯¹è¯", "æ£€ç´¢å¢žå¼º", f"å¯ç”¨åŠŸèƒ½: {enhancement_str}")
        
        # æ£€ç´¢å’Œç”Ÿæˆ
        with logger.timer("æ£€ç´¢ç›¸å…³æ–‡æ¡£"):
            logger.log_retrieval_start(kb_name=self.kb_name)
            
            retrieval_start = time.time()
            response = self.query_engine.stream_chat(final_prompt)
            retrieval_time = time.time() - retrieval_start
            
            logger.info(f"ðŸ” æ£€ç´¢è€—æ—¶: {retrieval_time:.2f}s (GPUåŠ é€Ÿ)")
            
            # æµå¼è¾“å‡º
            full_text = ""
            token_count = 0
            
            for token in response.response_gen:
                full_text += token
                token_count += 1
                yield {"type": "token", "content": token}
        
        # æå– token ç»Ÿè®¡
        prompt_tokens = 0
        completion_tokens = 0
        
        if hasattr(response, 'raw') and response.raw:
            usage = response.raw.get('usage', {})
            prompt_tokens = usage.get('prompt_tokens', 0)
            completion_tokens = usage.get('completion_tokens', 0)
        
        # å¦‚æžœæ²¡æœ‰çœŸå®ž Usageï¼Œåˆ™ä¼°ç®—
        if completion_tokens == 0:
            total_chars = len(full_text)
            chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', full_text))
            completion_tokens = int((chinese_chars * 1.5) + ((total_chars - chinese_chars) * 0.3))
            token_count = completion_tokens
        else:
            token_count = completion_tokens
        
        # å¤„ç†æ¥æºèŠ‚ç‚¹
        sources = []
        if response.source_nodes:
            logger.log_retrieval_result(len(response.source_nodes), kb_name=self.kb_name)
            logger.data_summary("æ£€ç´¢ç»“æžœ", {
                "æŸ¥è¯¢": final_prompt[:50] + "..." if len(final_prompt) > 50 else final_prompt,
                "ç›¸å…³æ–‡æ¡£": len(response.source_nodes),
                "çŸ¥è¯†åº“": self.kb_name
            })
            
            # æå–èŠ‚ç‚¹æ•°æ®
            node_data = []
            for node in response.source_nodes:
                text = ''
                try:
                    if hasattr(node, 'get_text'):
                        text = node.get_text()
                    elif hasattr(node, 'text'):
                        text = node.text
                    elif hasattr(node, 'node') and hasattr(node.node, 'text'):
                        text = node.node.text
                    else:
                        text = str(node)[:150]
                except:
                    text = str(node)[:150]
                
                node_data.append({
                    'metadata': getattr(node, 'metadata', {}),
                    'score': getattr(node, 'score', 0.0),
                    'text': text
                })
            
            # å¹¶è¡Œå¤„ç†èŠ‚ç‚¹
            tasks = [(d, self.kb_name) for d in node_data]
            sources = [s for s in self.executor.execute(process_node_worker, tasks, threshold=10) if s]
            
            if len(node_data) >= 10:
                logger.info(f"âš¡ å¹¶è¡Œå¤„ç†: {len(sources)} ä¸ªèŠ‚ç‚¹")
            else:
                logger.info(f"âš¡ ä¸²è¡Œå¤„ç†: {len(sources)} ä¸ªèŠ‚ç‚¹")
        
        # è®°å½•å®Œæˆæ—¥å¿—
        logger.log_answer_complete(
            kb_name=self.kb_name,
            model=llm_model,
            tokens=token_count,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens
        )
        
        # è®¡ç®—æ€»è€—æ—¶
        total_time = time.time() - start_time
        logger.complete_operation(f"æŸ¥è¯¢å®Œæˆ (è€—æ—¶ {total_time:.2f}s)")
        
        # å‡†å¤‡ç»Ÿè®¡ä¿¡æ¯
        tokens_per_sec = token_count / total_time if total_time > 0 else 0
        stats = {
            "time": total_time,
            "tokens": token_count,
            "tokens_per_sec": tokens_per_sec,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens
        }
        
        yield {
            "type": "complete",
            "content": full_text,
            "sources": sources,
            "stats": stats,
            "final_prompt": final_prompt
        }
