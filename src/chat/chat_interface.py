"""
èŠå¤©ç•Œé¢ - è´Ÿè´£èŠå¤©ç›¸å…³çš„æ‰€æœ‰UIé€»è¾‘
"""

import streamlit as st


class ChatInterface:
    """èŠå¤©ç•Œé¢ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–èŠå¤©ç•Œé¢"""
        pass
    
    def render(self, kb_name: str):
        """æ¸²æŸ“èŠå¤©ç•Œé¢"""
        st.title("ğŸ›¡ï¸ RAG Pro Max")
        
        # æ˜¾ç¤ºçŸ¥è¯†åº“ä¿¡æ¯
        self.render_kb_info(kb_name)
        
        # æ¸²æŸ“èŠå¤©å†å²
        self.render_chat_history()
        
        # æ¸²æŸ“è¾“å…¥åŒºåŸŸ
        self.render_input_area()
        
        # æ¸²æŸ“æ¨èé—®é¢˜
        self.render_suggestions()
    
    def render_kb_info(self, kb_name: str):
        """æ¸²æŸ“çŸ¥è¯†åº“ä¿¡æ¯"""
        # è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        try:
            from src.documents.document_manager import DocumentManager
            import os
            
            default_output_path = os.path.join(os.getcwd(), "vector_db_storage")
            db_path = os.path.join(default_output_path, kb_name)
            
            if os.path.exists(db_path):
                doc_manager = DocumentManager(db_path)
                stats = doc_manager.get_kb_statistics()
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                col1, col2, col3 = st.columns(3)
                col1.metric("ğŸ“„ æ–‡æ¡£æ•°", stats.get('file_cnt', 0))
                col2.metric("ğŸ§© ç‰‡æ®µæ•°", stats.get('total_chunks', 0))
                col3.metric("ğŸ’¾ å¤§å°", f"{stats.get('size', 0) / (1024 * 1024):.1f}MB")
                
                # æ–‡æ¡£ç®¡ç†å…¥å£
                with st.expander("ğŸ“Š çŸ¥è¯†åº“è¯¦æƒ…ä¸ç®¡ç†", expanded=False):
                    from src.document.document_manager_ui import DocumentManagerUI
                    doc_ui = DocumentManagerUI()
                    doc_ui.render_document_list(kb_name)
                    
                    st.divider()
                    doc_ui.render_document_operations(kb_name)
                    
        except Exception as e:
            st.caption(f"æ— æ³•åŠ è½½çŸ¥è¯†åº“ä¿¡æ¯: {str(e)}")
    
    def render_document_detail_dialog(self):
        """æ¸²æŸ“æ–‡æ¡£è¯¦æƒ…å¯¹è¯æ¡†"""
        if st.session_state.get('show_doc_detail') and st.session_state.get('show_doc_detail_kb'):
            from src.document.document_manager_ui import DocumentManagerUI
            DocumentManagerUI.show_document_detail_dialog(
                st.session_state.show_doc_detail_kb, 
                st.session_state.show_doc_detail
            )
    
    def render_chat_history(self):
        """æ¸²æŸ“èŠå¤©å†å²"""
        # åˆå§‹åŒ–æ¶ˆæ¯å†å²
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        # æ˜¾ç¤ºèŠå¤©å†å²
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                
                # æ˜¾ç¤ºæ¨èé—®é¢˜
                if message["role"] == "assistant" and message.get("suggestions"):
                    self.render_message_suggestions(message["suggestions"])
    
    def render_input_area(self):
        """æ¸²æŸ“è¾“å…¥åŒºåŸŸ"""
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨å¤„ç†
        if st.session_state.get('is_processing'):
            # æ­£åœ¨å¤„ç†æ—¶æ˜¾ç¤ºåœæ­¢æŒ‰é’®
            col1, col2 = st.columns([4, 1])
            with col1:
                st.chat_input("æ­£åœ¨ç”Ÿæˆå›ç­”ä¸­...", disabled=True)
            with col2:
                if st.button("â¹ åœæ­¢", key="stop_generation"):
                    st.session_state.stop_generation = True
                    st.session_state.is_processing = False
                    st.rerun()
        else:
            # æ­£å¸¸è¾“å…¥çŠ¶æ€
            if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
                self.handle_user_input(prompt)
    
    def render_suggestions(self):
        """æ¸²æŸ“æ¨èé—®é¢˜"""
        # æ˜¾ç¤ºå…¨å±€æ¨èé—®é¢˜
        if hasattr(st.session_state, 'global_suggestions') and st.session_state.global_suggestions:
            st.markdown("### ğŸ’¡ æ¨èé—®é¢˜")
            
            cols = st.columns(min(len(st.session_state.global_suggestions), 3))
            for i, suggestion in enumerate(st.session_state.global_suggestions[:3]):
                with cols[i]:
                    if st.button(suggestion, key=f"global_suggestion_{i}", use_container_width=True):
                        self.handle_user_input(suggestion)
    
    def render_message_suggestions(self, suggestions):
        """æ¸²æŸ“æ¶ˆæ¯ç›¸å…³çš„æ¨èé—®é¢˜"""
        if suggestions:
            st.markdown("**ğŸ’¡ ç›¸å…³é—®é¢˜:**")
            for i, suggestion in enumerate(suggestions[:3]):
                if st.button(suggestion, key=f"msg_suggestion_{i}_{len(st.session_state.messages)}", use_container_width=True):
                    self.handle_user_input(suggestion)
    
    def handle_user_input(self, prompt: str):
        """å¤„ç†ç”¨æˆ·è¾“å…¥"""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ç”ŸæˆåŠ©æ‰‹å›å¤
        with st.chat_message("assistant"):
            self.generate_response(prompt)
    
    def generate_response(self, prompt: str):
        """ç”ŸæˆåŠ©æ‰‹å›å¤"""
        # è®¾ç½®å¤„ç†çŠ¶æ€
        st.session_state.is_processing = True
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰èŠå¤©å¼•æ“
            if not st.session_state.get('chat_engine'):
                # å°è¯•åŠ è½½èŠå¤©å¼•æ“
                self.load_chat_engine()
                
                if not st.session_state.get('chat_engine'):
                    st.error("âŒ çŸ¥è¯†åº“æœªåŠ è½½ï¼Œè¯·å…ˆé€‰æ‹©çŸ¥è¯†åº“")
                    return
            
            # ç”Ÿæˆå›å¤
            response_placeholder = st.empty()
            
            # å®é™…è°ƒç”¨èŠå¤©å¼•æ“
            try:
                chat_engine = st.session_state.chat_engine
                response = chat_engine.stream_chat(prompt)
                
                full_response = ""
                for token in response.response_gen:
                    # æ£€æŸ¥åœæ­¢ä¿¡å·
                    if st.session_state.get('stop_generation'):
                        st.session_state.stop_generation = False
                        full_response += "\n\nâ¹ **ç”Ÿæˆå·²åœæ­¢**"
                        break
                    
                    full_response += token
                    response_placeholder.markdown(full_response + "â–Œ")
                
                # å®Œæˆç”Ÿæˆ
                response_placeholder.markdown(full_response)
                
                # ç”Ÿæˆè¿½é—®é—®é¢˜
                suggestions = self.generate_follow_up_questions(prompt, full_response)
                
                # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": full_response,
                    "suggestions": suggestions
                })
                
                # ä¿å­˜èŠå¤©å†å²
                kb_name = st.session_state.get('current_kb_name')
                if kb_name:
                    from src.chat import HistoryManager
                    HistoryManager.save(kb_name, st.session_state.messages)
                
            except Exception as e:
                st.error(f"âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")
        
        finally:
            # æ¸…é™¤å¤„ç†çŠ¶æ€
            st.session_state.is_processing = False
    
    def load_chat_engine(self):
        """åŠ è½½èŠå¤©å¼•æ“"""
        kb_name = st.session_state.get('current_kb_name')
        if not kb_name:
            return
        
        try:
            from src.kb.kb_loader import KnowledgeBaseLoader
            from src.config import ConfigLoader
            
            # è·å–é…ç½®
            config = ConfigLoader.load()
            embed_provider = config.get('embed_provider', 'HuggingFace (æœ¬åœ°/æé€Ÿ)')
            embed_model = config.get('embed_model_hf', 'sentence-transformers/all-MiniLM-L6-v2')
            embed_key = config.get('embed_key', '')
            embed_url = config.get('embed_url', '')
            
            # åŠ è½½çŸ¥è¯†åº“
            output_base = os.path.join(os.getcwd(), "vector_db_storage")
            kb_loader = KnowledgeBaseLoader(output_base)
            
            chat_engine, error_msg = kb_loader.load_knowledge_base(
                kb_name, embed_provider, embed_model, embed_key, embed_url
            )
            
            if chat_engine:
                st.session_state.chat_engine = chat_engine
                st.session_state.current_kb_id = kb_name
                
                from src.app_logging import LogManager
                logger = LogManager()
                logger.success("é—®ç­”å¼•æ“å·²å¯ç”¨GPUåŠ é€Ÿ")
                logger.log("SUCCESS", f"çŸ¥è¯†åº“åŠ è½½æˆåŠŸ: {kb_name}", stage="çŸ¥è¯†åº“åŠ è½½")
                
                st.toast(f"âœ… çŸ¥è¯†åº“ '{kb_name}' æŒ‚è½½æˆåŠŸï¼")
                
                from src.utils.memory import cleanup_memory
                cleanup_memory()
                
                # åŠ è½½èŠå¤©å†å²
                from src.chat import HistoryManager
                st.session_state.messages = HistoryManager.load(kb_name)
                
            else:
                from src.app_logging import LogManager
                logger = LogManager()
                logger.log("ERROR", f"çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {kb_name} - {error_msg}", stage="çŸ¥è¯†åº“åŠ è½½")
                
                if "ç»´åº¦ä¸åŒ¹é…" in error_msg:
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button("ğŸ”„ é‡å»ºç´¢å¼•", type="primary", use_container_width=True):
                            import shutil
                            db_path = os.path.join(output_base, kb_name)
                            shutil.rmtree(db_path, ignore_errors=True)
                            st.success("âœ… ç´¢å¼•å·²æ¸…ç†ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡æ¡£")
                            time.sleep(2)
                            st.rerun()
                    with col2:
                        if st.button("â†©ï¸ åˆ‡æ¢æ¨¡å‹", use_container_width=True):
                            st.info("è¯·åœ¨ä¾§è¾¹æ é€‰æ‹©åŸæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ bge-small-zh-v1.5ï¼‰")
                else:
                    st.error(f"çŸ¥è¯†åº“æŒ‚è½½å¤±è´¥ï¼š{error_msg}")
                
                st.session_state.chat_engine = None
                
        except Exception as e:
            st.error(f"åŠ è½½èŠå¤©å¼•æ“å¤±è´¥: {str(e)}")
            st.session_state.chat_engine = None
    
    def generate_follow_up_questions(self, prompt: str, response: str):
        """ç”Ÿæˆè¿½é—®é—®é¢˜"""
        try:
            from src.chat_utils_improved import generate_follow_up_questions_safe
            
            # ä½¿ç”¨æ”¹è¿›çš„è¿½é—®ç”Ÿæˆ
            suggestions = generate_follow_up_questions_safe(prompt, response)
            return suggestions[:3] if suggestions else []
            
        except Exception:
            # é™çº§åˆ°ç®€å•çš„è¿½é—®é—®é¢˜
            follow_ups = [
                "èƒ½è¯¦ç»†è§£é‡Šä¸€ä¸‹å—ï¼Ÿ",
                "æœ‰ç›¸å…³çš„ä¾‹å­å—ï¼Ÿ",
                "è¿˜æœ‰å…¶ä»–æ–¹æ³•å—ï¼Ÿ"
            ]
            return follow_ups[:2]
