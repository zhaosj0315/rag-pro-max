"""
å¢å¼ºæŸ¥è¯¢ç¼“å­˜ç³»ç»Ÿ - çº¯å†…å­˜ç‰ˆæœ¬
ç›®æ ‡ï¼šå®ç°ç§’çº§å“åº”ï¼Œæ— éœ€å¤–éƒ¨æ•°æ®åº“
"""

import hashlib
import json
import time
import threading
from typing import Dict, Any, Optional, List
from collections import OrderedDict
from src.logging import LogManager

logger = LogManager()

class EnhancedQueryCache:
    """å¢å¼ºæŸ¥è¯¢ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self.max_size = max_size
        self.ttl = ttl
        self.cache = OrderedDict()
        self.access_times = {}
        self.hit_count = 0
        self.miss_count = 0
        self.lock = threading.RLock()
        
        # å¯åŠ¨æ¸…ç†çº¿ç¨‹
        self.cleanup_thread = threading.Thread(target=self._cleanup_expired, daemon=True)
        self.cleanup_thread.start()
    
    def _generate_key(self, query: str, kb_name: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # æ ‡å‡†åŒ–æŸ¥è¯¢
        normalized_query = query.strip().lower()
        
        # åˆ›å»ºç¼“å­˜é”®
        key_data = {
            "query": normalized_query,
            "kb_name": kb_name,
            **kwargs
        }
        
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, query: str, kb_name: str, **kwargs) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜ç»“æœ"""
        key = self._generate_key(query, kb_name, **kwargs)
        
        with self.lock:
            if key in self.cache:
                entry = self.cache[key]
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if time.time() - entry["timestamp"] < self.ttl:
                    # æ›´æ–°è®¿é—®æ—¶é—´
                    self.access_times[key] = time.time()
                    # ç§»åˆ°æœ«å°¾ï¼ˆLRUï¼‰
                    self.cache.move_to_end(key)
                    
                    self.hit_count += 1
                    logger.info(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {query[:50]}...")
                    return entry["data"]
                else:
                    # è¿‡æœŸåˆ é™¤
                    del self.cache[key]
                    del self.access_times[key]
            
            self.miss_count += 1
            return None
    
    def set(self, query: str, kb_name: str, data: Dict[str, Any], **kwargs):
        """è®¾ç½®ç¼“å­˜"""
        key = self._generate_key(query, kb_name, **kwargs)
        
        with self.lock:
            # æ£€æŸ¥å®¹é‡
            if len(self.cache) >= self.max_size:
                self._evict_lru()
            
            # å­˜å‚¨ç¼“å­˜
            self.cache[key] = {
                "data": data,
                "timestamp": time.time(),
                "query": query[:100],  # å­˜å‚¨æŸ¥è¯¢ç‰‡æ®µç”¨äºè°ƒè¯•
                "kb_name": kb_name
            }
            self.access_times[key] = time.time()
            
            logger.info(f"ğŸ’¾ ç¼“å­˜å­˜å‚¨: {query[:50]}...")
    
    def _evict_lru(self):
        """æ·˜æ±°æœ€å°‘ä½¿ç”¨çš„ç¼“å­˜"""
        if self.cache:
            # æ‰¾åˆ°æœ€å°‘è®¿é—®çš„é”®
            lru_key = min(self.access_times.keys(), 
                         key=lambda k: self.access_times[k])
            
            del self.cache[lru_key]
            del self.access_times[lru_key]
    
    def _cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        while True:
            try:
                current_time = time.time()
                expired_keys = []
                
                with self.lock:
                    for key, entry in self.cache.items():
                        if current_time - entry["timestamp"] >= self.ttl:
                            expired_keys.append(key)
                    
                    for key in expired_keys:
                        del self.cache[key]
                        del self.access_times[key]
                
                if expired_keys:
                    logger.info(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜: {len(expired_keys)}ä¸ª")
                
                time.sleep(300)  # 5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
                time.sleep(60)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = (self.hit_count / total_requests * 100) if total_requests > 0 else 0
        
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "hit_count": self.hit_count,
            "miss_count": self.miss_count,
            "hit_rate": f"{hit_rate:.1f}%",
            "ttl": self.ttl
        }
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.access_times.clear()
            self.hit_count = 0
            self.miss_count = 0
        
        logger.info("ğŸ§¹ ç¼“å­˜å·²æ¸…ç©º")
    
    def preload_common_queries(self, kb_name: str, common_queries: List[str]):
        """é¢„åŠ è½½å¸¸ç”¨æŸ¥è¯¢"""
        logger.info(f"ğŸ”„ é¢„åŠ è½½å¸¸ç”¨æŸ¥è¯¢: {len(common_queries)}ä¸ª")
        
        for query in common_queries:
            # è¿™é‡Œå¯ä»¥é¢„å…ˆè®¡ç®—å¹¶ç¼“å­˜ç»“æœ
            # å®é™…å®ç°æ—¶éœ€è¦è°ƒç”¨çœŸå®çš„æŸ¥è¯¢å‡½æ•°
            pass

# å…¨å±€ç¼“å­˜å®ä¾‹
enhanced_cache = EnhancedQueryCache()

class SmartCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.cache = enhanced_cache
        self.query_patterns = {}  # æŸ¥è¯¢æ¨¡å¼åˆ†æ
    
    def cached_query(self, query_func):
        """ç¼“å­˜è£…é¥°å™¨"""
        def wrapper(query: str, kb_name: str, **kwargs):
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = self.cache.get(query, kb_name, **kwargs)
            if cached_result:
                return cached_result
            
            # æ‰§è¡ŒæŸ¥è¯¢
            start_time = time.time()
            result = query_func(query, kb_name, **kwargs)
            query_time = time.time() - start_time
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            if result and query_time > 0.5:  # åªç¼“å­˜è€—æ—¶æŸ¥è¯¢
                self.cache.set(query, kb_name, result, **kwargs)
            
            return result
        
        return wrapper
    
    def analyze_query_patterns(self, query: str):
        """åˆ†ææŸ¥è¯¢æ¨¡å¼"""
        # ç®€å•çš„æ¨¡å¼åˆ†æ
        words = query.lower().split()
        for word in words:
            if len(word) > 3:
                self.query_patterns[word] = self.query_patterns.get(word, 0) + 1

# å…¨å±€æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨
smart_cache_manager = SmartCacheManager()
