"""
å†…å­˜å’Œæ˜¾å­˜ç®¡ç†æ¨¡å—
"""

def cleanup_memory():
    """æ¸…ç†å†…å­˜å’Œæ˜¾å­˜ç¼“å­˜"""
    import gc
    gc.collect()
    
    try:
        import torch
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å… pickle é”™è¯¯
        try:
            from src.logging import LogManager
            logger = LogManager()
        except:
            logger = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            if logger:
                logger.info("ğŸ§¹ å·²æ¸…ç† CUDA æ˜¾å­˜ç¼“å­˜")
        elif torch.backends.mps.is_available():
            torch.mps.empty_cache()
            if logger:
                logger.info("ğŸ§¹ å·²æ¸…ç† MPS æ˜¾å­˜ç¼“å­˜")
    except Exception as e:
        try:
            from src.logging import LogManager
            logger = LogManager()
            if logger:
                logger.warning(f"æ˜¾å­˜æ¸…ç†å¤±è´¥: {e}")
        except:
            pass


def get_memory_stats():
    """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
    try:
        import psutil
        
        mem = psutil.virtual_memory()
        stats = {
            'total': mem.total / (1024**3),  # GB
            'available': mem.available / (1024**3),
            'used': mem.used / (1024**3),
            'percent': mem.percent
        }
        
        # GPU å†…å­˜
        try:
            import torch
            if torch.cuda.is_available():
                stats['gpu_allocated'] = torch.cuda.memory_allocated() / (1024**3)
                stats['gpu_reserved'] = torch.cuda.memory_reserved() / (1024**3)
            elif torch.backends.mps.is_available():
                stats['gpu_type'] = 'MPS'
        except:
            pass
        
        return stats
    except Exception as e:
        return {'error': str(e)}
