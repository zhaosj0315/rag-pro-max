"""
æœç´¢ç»“æžœè´¨é‡è¯„ä¼°æ¨¡å—
"""

import re
from typing import Dict, List, Tuple
from urllib.parse import urlparse

class SearchQualityAnalyzer:
    """æœç´¢ç»“æžœè´¨é‡åˆ†æžå™¨"""
    
    def __init__(self):
        # æƒå¨åŸŸååˆ—è¡¨ (v2.9.2 æ‰©å……)
        self.authority_domains = {
            # æŠ€æœ¯ä¸Žå¼€å‘
            'github.com', 'stackoverflow.com', 'github.io', 'pypi.org', 'npmjs.com', 
            'mdn.io', 'mozilla.org', 'w3schools.com', 'dev.to', 'medium.com',
            # å®˜æ–¹æ–‡æ¡£
            'docs.python.org', 'docs.microsoft.com', 'developer.apple.com', 'cloud.google.com',
            'aws.amazon.com', 'react.dev', 'vuejs.org', 'kubernetes.io', 'docker.com',
            # ç»¼åˆä¸Žç™¾ç§‘
            'wikipedia.org', 'zhihu.com', 'quora.com', 'arxiv.org', 'researchgate.net',
            # æƒå¨åª’ä½“ä¸Žæ”¿åŠ¡
            'gov.cn', 'edu.cn', 'org.cn', 'gov', 'edu', 'org', 'reuters.com', 'bloomberg.com',
            'news.ycombinator.com', 'techcrunch.com'
        }
        
        # ä¸“ä¸šæœ¯è¯­å…³é”®è¯ (v2.9.2 æ‰©å…… - ä¸­è‹±åŒè¯­)
        self.professional_keywords = [
            'å®šä¹‰', 'æ¦‚å¿µ', 'åŽŸç†', 'æ–¹æ³•', 'æ ‡å‡†', 'è§„èŒƒ', 'æŒ‡æ ‡', 'éƒ¨ç½²', 'æž¶æž„',
            'åˆ†æž', 'ç ”ç©¶', 'æŠ¥å‘Š', 'æ•°æ®', 'ç»Ÿè®¡', 'è°ƒæŸ¥', 'å®žæˆ˜', 'æ•™ç¨‹', 'æŒ‡å—',
            'API', 'SDK', 'ç®—æ³•', 'é€»è¾‘', 'æ–¹æ¡ˆ', 'è§£å†³', 'æ€§èƒ½', 'ä¼˜åŒ–', 'å®‰å…¨',
            'definition', 'concept', 'principle', 'method', 'standard', 'specification',
            'metrics', 'deployment', 'architecture', 'analysis', 'research', 'report',
            'data', 'statistics', 'survey', 'tutorial', 'guide', 'algorithm', 'logic',
            'solution', 'performance', 'optimization', 'security', 'implementation'
        ]
    
    def analyze_result_quality(self, result: Dict, user_query: str = "") -> Dict:
        """
        åˆ†æžå•ä¸ªæœç´¢ç»“æžœçš„è´¨é‡ (v2.9.3 å¢žå¼ºç‰ˆ)
        å¢žåŠ äº†åŸºäºŽç”¨æˆ·æ„å›¾çš„è¯­ä¹‰ç›¸å…³æ€§åˆ†æž
        """
        title = result.get('title', '')
        body = result.get('body', '')
        url = result.get('href', '')
        
        # 1. æ ¸å¿ƒæ”¹è¿›ï¼šè®¡ç®—è¯­ä¹‰ç›¸å…³æ€§è¯„åˆ† (Semantic Relevance)
        relevance_score = self._calculate_relevance_score(title, body, user_query)
        
        # 2. åŸºç¡€è´¨é‡æŒ‡æ ‡
        authority_score = self._calculate_authority_score(url)
        content_score = self._calculate_content_score(title, body)
        professional_score = self._calculate_professional_score(title, body)
        
        # 3. å™ªéŸ³åˆ¤å®š (Noise Filter)
        noise_penalty = self._identify_noise(title, body, user_query)
        
        # ç»¼åˆè´¨é‡è¯„åˆ† (å¤§å¹…å¢žåŠ ç›¸å…³æ€§æƒé‡)
        total_score = (relevance_score * 0.4 + 
                      authority_score * 0.2 + 
                      content_score * 0.2 + 
                      professional_score * 0.2) - noise_penalty
        
        total_score = max(0.0, min(1.0, total_score))
        
        # ç”Ÿæˆè´¨é‡æ ‡ç­¾
        quality_label = self._get_quality_label(total_score)
        
        return {
            'quality_score': round(total_score, 2),
            'quality_label': quality_label,
            'relevance_score': relevance_score,
            'is_noise': noise_penalty > 0.3,
            'summary': self._generate_summary(title, body),
            'key_points': self._extract_key_points(body)
        }

    def _calculate_relevance_score(self, title: str, body: str, query: str) -> float:
        """è®¡ç®—å†…å®¹ä¸Žç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³æ€§"""
        if not query: return 0.5
        
        # æå–æ ¸å¿ƒè¯ (ç®€å•åˆ†è¯)
        text = f"{title} {body}".lower()
        query_words = [w for k in re.split(r'[ \-,ï¼Œ]', query.lower()) if (w := k.strip()) and len(w) > 1]
        
        if not query_words: return 0.5
        
        # ç»Ÿè®¡åŒ¹é…åº¦
        hit_count = 0
        for word in query_words:
            if word in text:
                hit_count += 1
        
        ratio = hit_count / len(query_words)
        return min(ratio * 1.5, 1.0) # åªè¦åŒ¹é…ä¸€åŠä»¥ä¸Šçš„è¯ï¼Œç›¸å…³æ€§å°±å¾ˆé«˜

    def _identify_noise(self, title: str, body: str, query: str) -> float:
        """è¯†åˆ«è¡Œä¸šæ— å…³å™ªéŸ³ (v2.9.3)"""
        text = f"{title} {body}".lower()
        penalty = 0.0
        
        # å¦‚æžœæ˜¯ AI ç›¸å…³é—®é¢˜ï¼Œä½†å‡ºçŽ°äº†æ— å…³ç¡¬ä»¶/æ–‡ä»¶æ ¼å¼è¯æ±‡
        if 'ai' in query.lower() or 'å¤§æ¨¡åž‹' in query:
            # å™ªéŸ³è¯åº“
            noise_words = ['é“æ¿', 'é’¢æ¿', 'é“æ¿', 'è§„æ ¼å°ºå¯¸', 'illustrator', 'photoshop', 'å†›åŠ›æŠ¥å‘Š', 'äº”è§’å¤§æ¥¼']
            for word in noise_words:
                if word in text:
                    penalty += 0.5
        
        return penalty
    
    def _calculate_authority_score(self, url: str) -> float:
        """è®¡ç®—æ¥æºæƒå¨æ€§è¯„åˆ†"""
        if not url:
            return 0.3
            
        domain = urlparse(url).netloc.lower()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæƒå¨åŸŸå
        for auth_domain in self.authority_domains:
            if auth_domain in domain:
                return 0.9
        
        # æ£€æŸ¥åŸŸåç‰¹å¾
        if any(x in domain for x in ['gov', 'edu', 'org']):
            return 0.8
        elif any(x in domain for x in ['baidu', 'zhihu', 'wikipedia']):
            return 0.7
        elif domain.endswith('.com'):
            return 0.6
        else:
            return 0.4
    
    def _calculate_content_score(self, title: str, body: str) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡è¯„åˆ†"""
        if not body:
            return 0.2
        
        score = 0.5  # åŸºç¡€åˆ†
        
        # å†…å®¹é•¿åº¦è¯„åˆ†
        if len(body) > 500:
            score += 0.2
        elif len(body) > 200:
            score += 0.1
        
        # ç»“æž„åŒ–ç¨‹åº¦
        if 'ã€‚' in body or '.' in body:
            score += 0.1
        if any(x in body for x in ['ï¼š', ':', 'ï¼ˆ', '(']):
            score += 0.1
        
        # æ ‡é¢˜ç›¸å…³æ€§
        if title and any(word in body for word in title.split()[:3]):
            score += 0.1
        
        return min(score, 1.0)
    
    def _calculate_completeness_score(self, body: str) -> float:
        """è®¡ç®—å†…å®¹å®Œæ•´æ€§è¯„åˆ†"""
        if not body:
            return 0.2
        
        score = 0.5
        
        # æ£€æŸ¥æ˜¯å¦è¢«æˆªæ–­
        if body.endswith('...') or body.endswith('â€¦'):
            score -= 0.3
        
        # æ£€æŸ¥å†…å®¹ç»“æž„
        sentences = re.split(r'[ã€‚.!ï¼?ï¼Ÿ]', body)
        if len(sentences) >= 3:
            score += 0.3
        elif len(sentences) >= 2:
            score += 0.2
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´æ®µè½
        if '\n' in body or len(body) > 300:
            score += 0.2
        
        return min(score, 1.0)
    
    def _calculate_professional_score(self, title: str, body: str) -> float:
        """è®¡ç®—ä¸“ä¸šæ€§è¯„åˆ†"""
        text = f"{title} {body}".lower()
        
        # ç»Ÿè®¡ä¸“ä¸šæœ¯è¯­å‡ºçŽ°æ¬¡æ•°
        professional_count = sum(1 for keyword in self.professional_keywords 
                               if keyword in text)
        
        # æ£€æŸ¥æ•°å­—å’Œæ•°æ®
        number_count = len(re.findall(r'\d+', text))
        
        score = min(professional_count * 0.1 + number_count * 0.05, 1.0)
        return max(score, 0.2)  # æœ€ä½Ž0.2åˆ†
    
    def _get_quality_label(self, score: float) -> Tuple[str, str]:
        """æ ¹æ®è¯„åˆ†ç”Ÿæˆè´¨é‡æ ‡ç­¾"""
        if score >= 0.8:
            return ("ðŸ†", "é«˜è´¨é‡")
        elif score >= 0.6:
            return ("â­", "ä¸­ç­‰è´¨é‡")
        else:
            return ("âš ï¸", "éœ€éªŒè¯")
    
    def _extract_key_points(self, body: str) -> List[str]:
        """æå–å…³é”®ä¿¡æ¯ç‚¹"""
        if not body:
            return []
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚.!ï¼?ï¼Ÿ]', body)
        
        # ç­›é€‰å…³é”®å¥å­ï¼ˆåŒ…å«é‡è¦è¯æ±‡çš„å¥å­ï¼‰
        key_sentences = []
        for sentence in sentences[:5]:  # åªå–å‰5å¥
            sentence = sentence.strip()
            if len(sentence) > 10 and any(keyword in sentence 
                                        for keyword in self.professional_keywords):
                key_sentences.append(sentence)
        
        return key_sentences[:3]  # æœ€å¤šè¿”å›ž3ä¸ªè¦ç‚¹
    
    def _generate_summary(self, title: str, body: str) -> str:
        """ç”Ÿæˆæ™ºèƒ½æ‘˜è¦"""
        if not body:
            return title or "æ— å†…å®¹æ‘˜è¦"
        
        # å–å‰200å­—ç¬¦ä½œä¸ºæ‘˜è¦
        summary = body[:200].strip()
        
        # å¦‚æžœè¢«æˆªæ–­ï¼Œå°è¯•åœ¨å¥å·å¤„æˆªæ–­
        if len(body) > 200:
            last_period = summary.rfind('ã€‚')
            if last_period > 100:
                summary = summary[:last_period + 1]
            else:
                summary += "..."
        
        return summary

# å…¨å±€å®žä¾‹
search_quality_analyzer = SearchQualityAnalyzer()
