"""
æ¨¡å‹ç®¡ç†æ¨¡å— - ç»Ÿä¸€ç®¡ç†åµŒå…¥æ¨¡å‹å’Œ LLM æ¨¡å‹çš„åŠ è½½
"""
import os
from llama_index.core import Settings
from llama_index.llms.ollama import Ollama
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.embeddings.openai import OpenAIEmbedding
from ..custom_embeddings import create_custom_embedding
from src.app_logging import LogManager
logger = LogManager()


def clean_proxy():
    """æ¸…ç†ä»£ç†è®¾ç½®ï¼Œé¿å…æœ¬åœ°æœåŠ¡è¿æ¥é—®é¢˜"""
    for key in ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY']:
        if key in os.environ:
            del os.environ[key]


def load_embedding_model(provider: str, model_name: str, api_key: str = "", api_url: str = ""):
    """
    åŠ è½½åµŒå…¥æ¨¡å‹
    
    Args:
        provider: ä¾›åº”å•† (HuggingFace/OpenAI/Ollama)
        model_name: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥ï¼ˆOpenAIéœ€è¦ï¼‰
        api_url: APIåœ°å€ï¼ˆOpenAI/Ollamaéœ€è¦ï¼‰
    
    Returns:
        åµŒå…¥æ¨¡å‹å®ä¾‹ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        if provider.startswith("HuggingFace"):
            # HuggingFace æœ¬åœ°æ¨¡å‹
            cache_dir = "./hf_cache"
            local_paths = [
                os.path.join(cache_dir, model_name.replace('/', '--')),
                model_name,
            ]
            
            local_model_path = None
            for path in local_paths:
                if os.path.exists(os.path.join(path, "config.json")):
                    local_model_path = path
                    break
            
            if not local_model_path:
                local_model_path = model_name
            
            logger.info("ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹...")
            
            # æ£€æµ‹GPUæ”¯æŒ
            device = "cpu"
            try:
                import torch
                
                # æ¸…ç†ç¯å¢ƒå˜é‡
                for key in ['PYTORCH_MPS_HIGH_WATERMARK_RATIO', 'PYTORCH_MPS_LOW_WATERMARK_RATIO']:
                    if key in os.environ:
                        del os.environ[key]
                
                if torch.backends.mps.is_available():
                    device = "mps"
                    try:
                        torch.set_num_threads(10)
                        torch.set_num_interop_threads(3)
                    except:
                        pass
                    
                    os.environ['OMP_NUM_THREADS'] = '10'
                    os.environ['MKL_NUM_THREADS'] = '10'
                    logger.success("ğŸš€ Apple M4 Max GPU (MPS) + CPU åŠ é€Ÿå·²å¯ç”¨")
                    
                elif torch.cuda.is_available():
                    device = "cuda"
                    try:
                        torch.set_num_threads(10)
                    except:
                        pass
                    torch.cuda.set_per_process_memory_fraction(0.9)
                    logger.success("âœ… CUDA GPU + å¤šæ ¸CPU åŠ é€Ÿå·²å¯ç”¨ (é™åˆ¶90%)")
                    
                else:
                    try:
                        torch.set_num_threads(10)
                    except:
                        pass
                    logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨ 10æ ¸CPU å¹¶è¡Œ")
                    
            except Exception as e:
                device = "cpu"
                try:
                    import torch
                    torch.set_num_threads(12)
                except:
                    pass
                logger.error(f"âŒ GPUæ£€æµ‹å¼‚å¸¸: {e}ï¼Œä½¿ç”¨ CPU")
            
            # åŠ¨æ€è®¡ç®—batch_size
            import psutil
            available_memory_gb = psutil.virtual_memory().available / (1024**3)
            total_memory_gb = psutil.virtual_memory().total / (1024**3)
            
            if device == "mps":
                usable_memory = available_memory_gb * 0.9
                if usable_memory > 20:
                    batch_size = 4096
                elif usable_memory > 10:
                    batch_size = 2048
                elif usable_memory > 5:
                    batch_size = 1024
                else:
                    batch_size = 512
                logger.info(f"ğŸ”¥ M4 Max GPU: batch_size={batch_size}, å¯ç”¨å†…å­˜={usable_memory:.1f}GB (ç›®æ ‡ GPU <90%)")
            elif device == "cuda":
                batch_size = min(4096, max(1024, int(available_memory_gb * 50)))
            else:
                batch_size = 64
            
            logger.info(f"åŠ¨æ€batch_size: {batch_size} (æ€»å†…å­˜: {total_memory_gb:.1f}GB, å¯ç”¨: {available_memory_gb:.1f}GB)")
            
            import torch
            torch.set_default_device(device)
            
            result = create_custom_embedding(
                model_name=local_model_path,
                cache_folder="./hf_cache",
                batch_size=batch_size,
                device=device
            )
            logger.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return result
            
        elif provider.startswith("Ollama"):
            clean_proxy()
            logger.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return OllamaEmbedding(model_name=model_name, base_url=api_url)
            
        elif provider.startswith("OpenAI"):
            logger.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return OpenAIEmbedding(model=model_name, api_key=api_key, api_base=api_url)
            
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)[:200]}")
        
    return None


def load_llm_model(provider: str, model_name: str, api_key: str = "", api_url: str = "", temperature: float = 0.7, **kwargs):
    """
    åŠ è½½ LLM æ¨¡å‹
    
    Args:
        provider: ä¾›åº”å•† (Ollama/OpenAI/Azure OpenAI/Anthropic/Gemini/Moonshot/Groq)
        model_name: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥
        api_url: APIåœ°å€ (Base URL æˆ– Endpoint)
        temperature: æ¸©åº¦å‚æ•°
        **kwargs: å…¶ä»–å‚æ•° (å¦‚ api_version)
    
    Returns:
        LLM æ¨¡å‹å®ä¾‹ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        # 1. Ollama
        if provider.startswith("Ollama"):
            clean_proxy()
            base_url = api_url.rstrip('/')
            if base_url.endswith('/api'):
                base_url = base_url[:-4]
            
            return Ollama(
                model=model_name,
                base_url=base_url,
                request_timeout=360.0,
                temperature=temperature
            )
            
        # 2. OpenAI / Moonshot / Groq (OpenAI-Compatible)
        elif provider in ["OpenAI", "Moonshot", "Groq"] or provider.startswith("OpenAI-Compatible"):
            # Monkey-patch: æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹ä»¥ç»•è¿‡ LlamaIndex çš„ä¸¥æ ¼éªŒè¯
            try:
                import llama_index.llms.openai.utils as openai_utils
                
                # æ³¨å†Œä¸Šä¸‹æ–‡çª—å£å¤§å° (é»˜è®¤ 128k)
                if hasattr(openai_utils, 'openai_modelname_to_contextsize'):
                    # æ£€æŸ¥æ˜¯å¦ä¸ºå­—å…¸ï¼ŒæŸäº›ç‰ˆæœ¬å¯èƒ½æ˜¯å‡½æ•°
                    if isinstance(openai_utils.openai_modelname_to_contextsize, dict):
                        if model_name not in openai_utils.openai_modelname_to_contextsize:
                            openai_utils.openai_modelname_to_contextsize[model_name] = 128000
                
                # æ³¨å†Œåˆ°å¯ç”¨æ¨¡å‹åˆ—è¡¨
                if hasattr(openai_utils, 'ALL_AVAILABLE_MODELS'):
                    # æ£€æŸ¥æ˜¯å¦ä¸º Setï¼Œå¦‚æœæ˜¯åˆ™ addï¼Œå¦åˆ™ append
                    if isinstance(openai_utils.ALL_AVAILABLE_MODELS, set):
                        openai_utils.ALL_AVAILABLE_MODELS.add(model_name)
                    elif isinstance(openai_utils.ALL_AVAILABLE_MODELS, list):
                        if model_name not in openai_utils.ALL_AVAILABLE_MODELS:
                            openai_utils.ALL_AVAILABLE_MODELS.append(model_name)
                            
                # æ³¨å†Œåˆ°èŠå¤©æ¨¡å‹åˆ—è¡¨ (å…³é”®éªŒè¯ç‚¹)
                if hasattr(openai_utils, 'CHAT_MODELS'):
                    if isinstance(openai_utils.CHAT_MODELS, dict):
                        openai_utils.CHAT_MODELS[model_name] = 128000
                    elif isinstance(openai_utils.CHAT_MODELS, set):
                        openai_utils.CHAT_MODELS.add(model_name)
                    elif isinstance(openai_utils.CHAT_MODELS, list):
                         if model_name not in openai_utils.CHAT_MODELS:
                            openai_utils.CHAT_MODELS.append(model_name)
                            
            except Exception as e:
                logger.warning(f"âš ï¸ æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹å¤±è´¥ (å¯èƒ½å¯¼è‡´éªŒè¯é”™è¯¯): {e}")

            return OpenAI(
                model=model_name,
                api_key=api_key if api_key else "EMPTY",
                api_base=api_url,
                temperature=temperature,
                request_timeout=120.0
            )
            
        # 3. Azure OpenAI
        elif provider == "Azure OpenAI":
            try:
                from llama_index.llms.azure_openai import AzureOpenAI
                return AzureOpenAI(
                    engine=model_name,  # Deployment name
                    model=model_name,
                    api_key=api_key,
                    azure_endpoint=api_url,
                    api_version=kwargs.get("api_version", "2023-05-15"),
                    temperature=temperature
                )
            except ImportError:
                logger.error("âŒ æœªå®‰è£… Azure æ”¯æŒ: pip install llama-index-llms-azure-openai")
                return None
                
        # 4. Anthropic (Claude)
        elif provider == "Anthropic":
            try:
                from llama_index.llms.anthropic import Anthropic
                return Anthropic(
                    model=model_name,
                    api_key=api_key,
                    temperature=temperature
                )
            except ImportError:
                logger.error("âŒ æœªå®‰è£… Anthropic æ”¯æŒ: pip install llama-index-llms-anthropic")
                return None
                
        # 5. Gemini (Google)
        elif provider == "Gemini":
            try:
                from llama_index.llms.gemini import Gemini
                # Gemini é€šå¸¸éœ€è¦ models/ å‰ç¼€
                if not model_name.startswith("models/"):
                    model_name = f"models/{model_name}"
                return Gemini(
                    model=model_name,
                    api_key=api_key,
                    temperature=temperature
                )
            except ImportError:
                logger.error("âŒ æœªå®‰è£… Gemini æ”¯æŒ: pip install llama-index-llms-gemini")
                return None
            
    except Exception as e:
        logger.error(f"LLM åŠ è½½å¤±è´¥ ({provider}): {e}")
        
    return None


def set_global_embedding_model(provider: str, model_name: str, api_key: str = "", api_url: str = ""):
    """
    è®¾ç½®å…¨å±€åµŒå…¥æ¨¡å‹ï¼ˆSettings.embed_modelï¼‰
    
    Returns:
        bool: æ˜¯å¦è®¾ç½®æˆåŠŸ
    """
    embed_model = load_embedding_model(provider, model_name, api_key, api_url)
    if embed_model:
        Settings.embed_model = embed_model
        try:
            dim = len(embed_model._get_text_embedding("test"))
            logger.success(f"âœ… å…¨å±€åµŒå…¥æ¨¡å‹å·²è®¾ç½®: {model_name} ({dim}ç»´)")
        except:
            logger.success(f"âœ… å…¨å±€åµŒå…¥æ¨¡å‹å·²è®¾ç½®: {model_name}")
        return True
    else:
        logger.error(f"âŒ å…¨å±€åµŒå…¥æ¨¡å‹è®¾ç½®å¤±è´¥: {model_name}")
        return False


def set_global_llm_model(provider: str, model_name: str, api_key: str = "", api_url: str = "", temperature: float = 0.7, **kwargs):
    """
    è®¾ç½®å…¨å±€ LLM æ¨¡å‹ï¼ˆSettings.llmï¼‰
    
    Returns:
        bool: æ˜¯å¦è®¾ç½®æˆåŠŸ
    """
    llm_model = load_llm_model(provider, model_name, api_key, api_url, temperature, **kwargs)
    if llm_model:
        Settings.llm = llm_model
        logger.success(f"âœ… å…¨å±€ LLM å·²è®¾ç½®: {model_name} ({provider})")
        return True
    else:
        logger.error(f"âŒ å…¨å±€ LLM è®¾ç½®å¤±è´¥: {model_name}")
        return False
