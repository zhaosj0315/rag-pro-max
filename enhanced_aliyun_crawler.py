#!/usr/bin/env python3
"""
é˜¿é‡Œäº‘æ–‡æ¡£å¢å¼ºçˆ¬è™« - ä¸“é—¨ä¼˜åŒ–
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import os
import hashlib
import re
from typing import List, Set

class EnhancedAliyunCrawler:
    """å¢å¼ºçš„é˜¿é‡Œäº‘æ–‡æ¡£çˆ¬è™«"""
    
    def __init__(self, output_dir="temp_aliyun_crawl"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
        
        self.visited_urls: Set[str] = set()
        self.saved_files: List[str] = []
    
    def extract_aliyun_links(self, soup, base_url: str) -> List[str]:
        """ä¸“é—¨æå–é˜¿é‡Œäº‘æ–‡æ¡£é“¾æ¥"""
        links = set()
        base_domain = urlparse(base_url).netloc
        
        # é˜¿é‡Œäº‘ç‰¹å®šçš„é“¾æ¥é€‰æ‹©å™¨
        selectors = [
            'a[href*="/zh/"]',           # ä¸­æ–‡æ–‡æ¡£é“¾æ¥
            'a[href*="/product/"]',      # äº§å“æ–‡æ¡£é“¾æ¥
            'a[href*="/help/"]',         # å¸®åŠ©æ–‡æ¡£é“¾æ¥
            'a[href*="help.aliyun.com"]', # å¸®åŠ©ä¸­å¿ƒé“¾æ¥
            '.product-item a',           # äº§å“é¡¹ç›®é“¾æ¥
            '.doc-item a',               # æ–‡æ¡£é¡¹ç›®é“¾æ¥
            '.category-item a',          # åˆ†ç±»é¡¹ç›®é“¾æ¥
            'nav a',                     # å¯¼èˆªé“¾æ¥
            '.menu a',                   # èœå•é“¾æ¥
            '.sidebar a',                # ä¾§è¾¹æ é“¾æ¥
            '.content a',                # å†…å®¹åŒºé“¾æ¥
        ]
        
        for selector in selectors:
            try:
                elements = soup.select(selector)
                for element in elements:
                    href = element.get('href')
                    if not href:
                        continue
                    
                    # æ„å»ºå®Œæ•´URL
                    full_url = urljoin(base_url, href)
                    parsed = urlparse(full_url)
                    
                    # åªå¤„ç†é˜¿é‡Œäº‘åŸŸå
                    if parsed.netloc not in ['help.aliyun.com', 'www.aliyun.com']:
                        continue
                    
                    # æ¸…ç†URL
                    clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                    if parsed.query:
                        # ä¿ç•™é‡è¦å‚æ•°
                        if any(param in parsed.query for param in ['id', 'product', 'category']):
                            clean_url += f"?{parsed.query}"
                    
                    # è¿‡æ»¤æœ‰ç”¨çš„é“¾æ¥
                    if self.is_useful_aliyun_link(clean_url, element):
                        links.add(clean_url)
            except Exception as e:
                continue
        
        # é¢å¤–æŸ¥æ‰¾ï¼šé€šè¿‡JavaScriptæˆ–æ•°æ®å±æ€§
        try:
            # æŸ¥æ‰¾data-*å±æ€§ä¸­çš„é“¾æ¥
            for element in soup.find_all(attrs={'data-url': True}):
                url = element.get('data-url')
                if url and 'help.aliyun.com' in url:
                    links.add(url)
            
            # æŸ¥æ‰¾scriptæ ‡ç­¾ä¸­çš„URL
            for script in soup.find_all('script'):
                if script.string:
                    urls = re.findall(r'["\']https://help\.aliyun\.com[^"\']*["\']', script.string)
                    for url in urls:
                        clean_url = url.strip('"\'')
                        if self.is_useful_aliyun_link(clean_url):
                            links.add(clean_url)
        except Exception:
            pass
        
        return list(links)
    
    def is_useful_aliyun_link(self, url: str, element=None) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰ç”¨çš„é˜¿é‡Œäº‘é“¾æ¥"""
        url_lower = url.lower()
        
        # æ’é™¤æ— ç”¨é“¾æ¥
        exclude_patterns = [
            r'\.(?:jpg|jpeg|png|gif|css|js|ico)$',
            r'/(?:login|logout|register|signin|signup)',
            r'/(?:cart|order|payment|billing)',
            r'#$',
            r'javascript:',
            r'mailto:',
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, url_lower):
                return False
        
        # ä¼˜å…ˆåŒ…å«çš„æ¨¡å¼
        include_patterns = [
            r'/zh/',           # ä¸­æ–‡æ–‡æ¡£
            r'/product/',      # äº§å“æ–‡æ¡£
            r'/help/',         # å¸®åŠ©æ–‡æ¡£
            r'\.html$',        # HTMLé¡µé¢
        ]
        
        # å¦‚æœåŒ¹é…åŒ…å«æ¨¡å¼ï¼Œç›´æ¥è¿”å›True
        for pattern in include_patterns:
            if re.search(pattern, url_lower):
                return True
        
        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬
        if element:
            text = element.get_text(strip=True)
            if len(text) > 0 and len(text) < 100:  # åˆç†çš„é“¾æ¥æ–‡æœ¬é•¿åº¦
                return True
        
        return False
    
    def crawl_page(self, url: str) -> bool:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        if url in self.visited_urls:
            return False
        
        try:
            print(f"  æ­£åœ¨æŠ“å–: {url}")
            
            response = self.session.get(url, timeout=15)
            response.encoding = response.apparent_encoding
            
            if response.status_code != 200:
                print(f"    è·³è¿‡ (çŠ¶æ€ç : {response.status_code})")
                return False
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ ‡é¢˜
            title = "æœªçŸ¥é¡µé¢"
            if soup.title:
                title = soup.title.get_text(strip=True)
            
            # æå–å†…å®¹
            content = self.extract_content(soup)
            
            if len(content) < 100:  # å†…å®¹å¤ªå°‘ï¼Œå¯èƒ½ä¸æ˜¯æœ‰æ•ˆé¡µé¢
                print(f"    è·³è¿‡ (å†…å®¹å¤ªå°‘: {len(content)} å­—ç¬¦)")
                return False
            
            # ä¿å­˜æ–‡ä»¶
            filename = self.save_content(url, title, content)
            if filename:
                self.saved_files.append(filename)
                self.visited_urls.add(url)
                print(f"    âœ… å·²ä¿å­˜: {title} ({len(content)} å­—ç¬¦)")
                return True
            
        except Exception as e:
            print(f"    âŒ æŠ“å–å¤±è´¥: {e}")
        
        return False
    
    def extract_content(self, soup) -> str:
        """æå–é¡µé¢ä¸»è¦å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()
        
        # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            '.main-content',
            '.content',
            '.doc-content',
            '.article-content',
            'main',
            '.container',
            'body'
        ]
        
        content = ""
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text(separator='\n', strip=True)
                if len(content) > 200:  # æ‰¾åˆ°è¶³å¤Ÿçš„å†…å®¹å°±åœæ­¢
                    break
        
        if not content:
            content = soup.get_text(separator='\n', strip=True)
        
        # æ¸…ç†å†…å®¹
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        return '\n'.join(lines)
    
    def save_content(self, url: str, title: str, content: str) -> str:
        """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶"""
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            safe_title = re.sub(r'[^\w\s-]', '', title)[:50]
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            filename = f"{safe_title}_{url_hash}.txt"
            filepath = os.path.join(self.output_dir, filename)
            
            # ä¿å­˜å†…å®¹
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"URL: {url}\n")
                f.write(f"æ ‡é¢˜: {title}\n")
                f.write(f"å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦\n")
                f.write("=" * 50 + "\n\n")
                f.write(content)
            
            return filepath
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def crawl_aliyun_docs(self, start_url: str, max_pages: int = 100) -> List[str]:
        """çˆ¬å–é˜¿é‡Œäº‘æ–‡æ¡£"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å–é˜¿é‡Œäº‘æ–‡æ¡£: {start_url}")
        print(f"ğŸ“Š ç›®æ ‡é¡µé¢æ•°: {max_pages}")
        
        # ç¬¬ä¸€å±‚ï¼šçˆ¬å–é¦–é¡µ
        if not self.crawl_page(start_url):
            print("âŒ é¦–é¡µçˆ¬å–å¤±è´¥")
            return []
        
        # è·å–é¦–é¡µé“¾æ¥
        try:
            response = self.session.get(start_url, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            first_level_links = self.extract_aliyun_links(soup, start_url)
            print(f"ğŸ“‚ ç¬¬1å±‚å‘ç° {len(first_level_links)} ä¸ªé“¾æ¥")
        except Exception as e:
            print(f"âŒ è·å–é¦–é¡µé“¾æ¥å¤±è´¥: {e}")
            return self.saved_files
        
        # ç¬¬äºŒå±‚ï¼šçˆ¬å–ä¸»è¦åˆ†ç±»é¡µé¢
        second_level_links = []
        for i, link in enumerate(first_level_links[:20]):  # é™åˆ¶ç¬¬ä¸€å±‚é“¾æ¥æ•°
            if len(self.saved_files) >= max_pages:
                break
            
            print(f"ğŸ“„ ç¬¬1å±‚ ({i+1}/{min(20, len(first_level_links))})")
            if self.crawl_page(link):
                # è·å–è¿™ä¸ªé¡µé¢çš„é“¾æ¥
                try:
                    response = self.session.get(link, timeout=15)
                    soup = BeautifulSoup(response.text, 'html.parser')
                    page_links = self.extract_aliyun_links(soup, link)
                    second_level_links.extend(page_links)
                    print(f"    å‘ç° {len(page_links)} ä¸ªå­é“¾æ¥")
                except Exception:
                    pass
            
            time.sleep(0.5)  # ç¤¼è²Œå»¶è¿Ÿ
        
        print(f"ğŸ“‚ ç¬¬2å±‚å‘ç° {len(second_level_links)} ä¸ªé“¾æ¥")
        
        # ç¬¬ä¸‰å±‚ï¼šçˆ¬å–è¯¦ç»†æ–‡æ¡£é¡µé¢
        unique_links = list(set(second_level_links) - self.visited_urls)
        for i, link in enumerate(unique_links[:max_pages]):
            if len(self.saved_files) >= max_pages:
                break
            
            print(f"ğŸ“„ ç¬¬2å±‚ ({i+1}/{min(max_pages, len(unique_links))})")
            self.crawl_page(link)
            time.sleep(0.3)  # ç¤¼è²Œå»¶è¿Ÿ
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡çˆ¬å–: {len(self.saved_files)} ä¸ªé¡µé¢")
        
        return self.saved_files

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("  é˜¿é‡Œäº‘æ–‡æ¡£å¢å¼ºçˆ¬è™«æµ‹è¯•")
    print("=" * 60)
    
    crawler = EnhancedAliyunCrawler()
    
    # çˆ¬å–é˜¿é‡Œäº‘æ–‡æ¡£
    saved_files = crawler.crawl_aliyun_docs(
        start_url="https://help.aliyun.com/",
        max_pages=50  # å…ˆæµ‹è¯•50é¡µ
    )
    
    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"  æˆåŠŸçˆ¬å–: {len(saved_files)} é¡µ")
    print(f"  ä¿å­˜ç›®å½•: {crawler.output_dir}")
    
    if saved_files:
        print(f"\nğŸ“„ éƒ¨åˆ†æ–‡ä»¶åˆ—è¡¨:")
        for i, file_path in enumerate(saved_files[:10]):
            file_name = os.path.basename(file_path)
            file_size = os.path.getsize(file_path)
            print(f"  {i+1}. {file_name} ({file_size} bytes)")
        
        if len(saved_files) > 10:
            print(f"  ... è¿˜æœ‰ {len(saved_files) - 10} ä¸ªæ–‡ä»¶")

if __name__ == "__main__":
    main()
