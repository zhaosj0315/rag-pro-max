# ç½‘é¡µæŠ“å–åˆ°çŸ¥è¯†åº“åŠŸèƒ½é›†æˆæŒ‡å—

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

è¿™ä¸ªåŠŸèƒ½å®ç°äº†ä»ç½‘é¡µæŠ“å–å†…å®¹åˆ°è‡ªåŠ¨åˆ›å»ºçŸ¥è¯†åº“çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š

1. **ç›´æ¥URLæŠ“å–** - è¾“å…¥ç½‘å€ï¼Œè‡ªåŠ¨æŠ“å–å†…å®¹å¹¶åˆ›å»ºçŸ¥è¯†åº“
2. **å…³é”®è¯æœç´¢** - è¾“å…¥å…³é”®è¯ï¼Œåœ¨é¢„è®¾ç½‘ç«™æœç´¢å¹¶æŠ“å–ç»“æœ
3. **æ™ºèƒ½å‘½å** - æ ¹æ®ç½‘é¡µå†…å®¹è‡ªåŠ¨ç”Ÿæˆåˆé€‚çš„çŸ¥è¯†åº“åç§°
4. **è‡ªåŠ¨åˆ‡æ¢** - åˆ›å»ºå®Œæˆåè‡ªåŠ¨åˆ‡æ¢åˆ°æ–°çŸ¥è¯†åº“

## ğŸ“ æ–°å¢æ–‡ä»¶

```
src/
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ web_to_kb_processor.py      # å®Œæ•´ç‰ˆå¤„ç†å™¨ï¼ˆåŠŸèƒ½ä¸°å¯Œï¼‰
â”‚   â””â”€â”€ web_to_kb_simple.py         # ç®€åŒ–ç‰ˆå¤„ç†å™¨ï¼ˆæ¨èä½¿ç”¨ï¼‰
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ web_to_kb_interface.py      # UIç•Œé¢ç»„ä»¶
â””â”€â”€ web_crawl_integration_patch.py  # é›†æˆè¡¥ä¸

demo_web_to_kb.py                   # æ¼”ç¤ºè„šæœ¬
WEB_TO_KB_INTEGRATION.md            # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿé›†æˆ

### æ–¹æ³•1: æ›¿æ¢ç°æœ‰ç½‘é¡µæŠ“å–åŠŸèƒ½

åœ¨ `src/apppro.py` ä¸­æ‰¾åˆ°ç½‘é¡µæŠ“å–éƒ¨åˆ†ï¼ˆçº¦ç¬¬802è¡Œï¼‰ï¼Œæ›¿æ¢ä¸ºï¼š

```python
with src_tab_web:
    # å¯¼å…¥å¢å¼ºç‰ˆç½‘é¡µæŠ“å–åŠŸèƒ½
    from src.processors.web_to_kb_simple import render_enhanced_web_crawl
    render_enhanced_web_crawl()
```

### æ–¹æ³•2: æ·»åŠ æ–°çš„æ ‡ç­¾é¡µ

åœ¨ä¸»ç•Œé¢æ·»åŠ æ–°çš„æ ‡ç­¾é¡µï¼š

```python
# åœ¨ç°æœ‰æ ‡ç­¾é¡µåŸºç¡€ä¸Šæ·»åŠ 
tab_main, tab_config, tab_monitor, tab_tools, tab_web_kb, tab_help = st.tabs([
    "ğŸ  ä¸»é¡µ", "âš™ï¸ é…ç½®", "ğŸ“Š ç›‘æ§", "ğŸ”§ å·¥å…·", "ğŸŒ ç½‘é¡µâ†’çŸ¥è¯†åº“", "â“ å¸®åŠ©"
])

with tab_web_kb:
    from src.ui.web_to_kb_interface import WebToKBInterface
    web_interface = WebToKBInterface()
    web_interface.render()
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### 1. æ™ºèƒ½å‘½åç®—æ³•

```python
def generate_kb_name_from_web(url: str, files_count: int = 0) -> str:
    """æ ¹æ®URLç”Ÿæˆæ™ºèƒ½çŸ¥è¯†åº“åç§°"""
    # ç‰¹æ®Šç½‘ç«™è¯†åˆ«
    if 'wikipedia.org' in domain:
        return f"ç™¾ç§‘_{path_parts[-1][:10]}"
    elif 'github.com' in domain:
        return f"é¡¹ç›®_{repo_name[:10]}"
    # ... æ›´å¤šè§„åˆ™
```

### 2. é¢„è®¾æœç´¢ç½‘ç«™

```python
preset_sites = {
    "ç»´åŸºç™¾ç§‘": "https://zh.wikipedia.org/wiki/Special:Search?search={keyword}",
    "ç™¾åº¦ç™¾ç§‘": "https://baike.baidu.com/search?word={keyword}",
    "çŸ¥ä¹": "https://www.zhihu.com/search?type=content&q={keyword}",
    "CSDN": "https://so.csdn.net/so/search?q={keyword}",
    "GitHub": "https://github.com/search?q={keyword}&type=repositories",
    "Stack Overflow": "https://stackoverflow.com/search?q={keyword}"
}
```

### 3. å®Œæ•´æµç¨‹

```python
def crawl_and_create_kb(url=None, keyword=None, **kwargs):
    """å®Œæ•´çš„æŠ“å–â†’åˆ›å»ºçŸ¥è¯†åº“æµç¨‹"""
    
    # 1. æŠ“å–ç½‘é¡µå†…å®¹
    crawler = WebCrawler()
    files = crawler.crawl_advanced(...)
    
    # 2. ç”Ÿæˆæ™ºèƒ½åç§°
    kb_name = generate_kb_name_from_web(url, len(files))
    
    # 3. åˆ›å»ºçŸ¥è¯†åº“ç›®å½•
    os.makedirs(f"vector_db_storage/{kb_name}", exist_ok=True)
    
    # 4. è®¾ç½®session stateï¼Œè§¦å‘ä¸»åº”ç”¨å¤„ç†
    st.session_state.uploaded_path = crawler.output_dir
    st.session_state.upload_auto_name = kb_name
    st.session_state.selected_kb = kb_name
    
    return {"success": True, "kb_name": kb_name, ...}
```

## ğŸ¨ UIç•Œé¢ç‰¹æ€§

### ç›´æ¥æŠ“å–ç•Œé¢
- URLè¾“å…¥ï¼ˆæ”¯æŒè‡ªåŠ¨è¡¥å…¨https://ï¼‰
- æŠ“å–æ·±åº¦é€‰æ‹©ï¼ˆ1-5å±‚ï¼‰
- æœ€å¤§é¡µé¢æ•°é™åˆ¶
- é«˜çº§é€‰é¡¹ï¼ˆæ’é™¤æ¨¡å¼ã€è§£æå™¨ç±»å‹ï¼‰
- å®æ—¶çŠ¶æ€æ˜¾ç¤º

### å…³é”®è¯æœç´¢ç•Œé¢
- å…³é”®è¯è¾“å…¥
- å¤šç½‘ç«™é€‰æ‹©ï¼ˆå¤é€‰æ¡†ï¼‰
- æœç´¢ç»“æœé¡µé¢æ•°é™åˆ¶
- è‡ªåŠ¨ç”ŸæˆçŸ¥è¯†åº“åç§°

### çŠ¶æ€åé¦ˆ
- å®æ—¶è¿›åº¦æ¡
- çŠ¶æ€æ¶ˆæ¯æ˜¾ç¤º
- æˆåŠŸ/å¤±è´¥æç¤º
- è¯¦ç»†ç»“æœå±•ç¤º

## ğŸ§ª æµ‹è¯•æ–¹æ³•

### 1. è¿è¡Œæ¼”ç¤ºè„šæœ¬

```bash
python demo_web_to_kb.py
```

### 2. æ‰‹åŠ¨æµ‹è¯•

```python
from src.processors.web_to_kb_simple import crawl_and_create_kb

# æµ‹è¯•ç›´æ¥æŠ“å–
result = crawl_and_create_kb(
    url="https://docs.python.org/3/tutorial/",
    max_depth=1,
    max_pages=3
)

# æµ‹è¯•å…³é”®è¯æœç´¢
result = crawl_and_create_kb(
    keyword="Pythonç¼–ç¨‹",
    sites=["ç»´åŸºç™¾ç§‘", "ç™¾åº¦ç™¾ç§‘"],
    max_pages=5
)
```

## ğŸ“‹ é›†æˆæ£€æŸ¥æ¸…å•

- [ ] ç¡®è®¤ `src/processors/web_crawler.py` å­˜åœ¨ä¸”åŠŸèƒ½æ­£å¸¸
- [ ] ç¡®è®¤ `temp_uploads` å’Œ `vector_db_storage` ç›®å½•å­˜åœ¨
- [ ] æµ‹è¯•ç½‘ç»œè¿æ¥å’Œç½‘é¡µæŠ“å–åŠŸèƒ½
- [ ] éªŒè¯æ™ºèƒ½å‘½åç®—æ³•å·¥ä½œæ­£å¸¸
- [ ] æµ‹è¯•çŸ¥è¯†åº“åˆ›å»ºå’Œåˆ‡æ¢åŠŸèƒ½
- [ ] æ£€æŸ¥UIç•Œé¢æ˜¾ç¤ºæ­£å¸¸
- [ ] éªŒè¯é”™è¯¯å¤„ç†å’Œç”¨æˆ·åé¦ˆ

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ç½‘é¡µæŠ“å–å¤±è´¥**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - ç¡®è®¤URLæ ¼å¼æ­£ç¡®
   - æŸ¥çœ‹æ˜¯å¦è¢«ç½‘ç«™åçˆ¬è™«æœºåˆ¶é˜»æ­¢

2. **çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥**
   - æ£€æŸ¥ç›®å½•æƒé™
   - ç¡®è®¤ç£ç›˜ç©ºé—´å……è¶³
   - éªŒè¯çŸ¥è¯†åº“åç§°åˆæ³•æ€§

3. **UIç•Œé¢å¼‚å¸¸**
   - æ£€æŸ¥Streamlitç‰ˆæœ¬å…¼å®¹æ€§
   - ç¡®è®¤æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…
   - æŸ¥çœ‹æµè§ˆå™¨æ§åˆ¶å°é”™è¯¯ä¿¡æ¯

### è°ƒè¯•æ–¹æ³•

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æµ‹è¯•å•ä¸ªç»„ä»¶
from src.processors.web_crawler import WebCrawler
crawler = WebCrawler()
files = crawler.crawl("https://example.com", max_pages=1)
print(f"æŠ“å–ç»“æœ: {files}")
```

## ğŸ‰ ä½¿ç”¨æ•ˆæœ

ç”¨æˆ·ä½“éªŒæµç¨‹ï¼š
1. ç”¨æˆ·è¾“å…¥ç½‘å€æˆ–å…³é”®è¯
2. ç‚¹å‡»"æŠ“å–å¹¶åˆ›å»ºçŸ¥è¯†åº“"æŒ‰é’®
3. ç³»ç»Ÿæ˜¾ç¤ºå®æ—¶æŠ“å–è¿›åº¦
4. è‡ªåŠ¨ç”Ÿæˆåˆé€‚çš„çŸ¥è¯†åº“åç§°
5. åˆ›å»ºçŸ¥è¯†åº“å¹¶è‡ªåŠ¨åˆ‡æ¢
6. ç”¨æˆ·å¯ä»¥ç«‹å³å¼€å§‹å¯¹è¯

è¿™ä¸ªåŠŸèƒ½å¤§å¤§ç®€åŒ–äº†ä»ç½‘é¡µå†…å®¹åˆ›å»ºçŸ¥è¯†åº“çš„æµç¨‹ï¼Œä»åŸæ¥çš„"æŠ“å–â†’ä¿å­˜â†’ä¸Šä¼ â†’åˆ›å»ºçŸ¥è¯†åº“"å˜æˆäº†ä¸€é”®å®Œæˆçš„ä½“éªŒã€‚
